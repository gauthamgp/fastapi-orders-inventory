WEBVTT

00:05:32.000 --> 00:05:37.000
Okay, so it's, uh, 9.05, guys, and I believe all of us have joined, so shall we start a class?

00:05:37.000 --> 00:05:44.000
Yeah? Everyone? Class, start honeepa, chat, disable Kardina, please. Yes, for sure.

00:05:44.000 --> 00:05:51.000
Uh, good morning, good morning, good morning, everyone. I can see all good morning messages, okay?

00:05:51.000 --> 00:06:10.000
So, does deep learning and LP is required for this class, as I mentioned in the roadmap. So, if you're going ahead with a full-stack data science, and then if you are coming, like, for this generative AI, then obviously deep learning and LP is required. Otherwise, uh, it's not required. So, we have made it, like, independent of each other.

00:06:10.000 --> 00:06:22.000
Uh, good morning, sir. A small question, how Yuri is different from Popexity? Uh, so yeah, Popxlity is also giving you a multiple model access, and Major League Popacility is trying to solve a Google search problem, and here, so we are trying to.

00:06:22.000 --> 00:06:29.000
Uh, give you a multiple, again, LLM access, and we are trying to solve almost a similar kind of a problem.

00:06:29.000 --> 00:06:36.000
Uh, so that you don't have to go anywhere for a chat or for any… your… any of your AI need. Plus, uh.

00:06:36.000 --> 00:06:46.000
Api access, so I believe publicity. Is also giving you a $5 of credit in terms of API access, but here, so you are getting more.

00:06:46.000 --> 00:06:57.000
In terms of offering.

00:06:57.000 --> 00:07:07.000
It's the same thing, something wrong your app prompt registered, and nobody picks up phone from your own. Why to pick a phone from your own every time, for everything?

00:07:07.000 --> 00:07:11.000
I mean, like, you are able to join the class, I think that solves your problem.

00:07:11.000 --> 00:07:23.000
Yeah. Okay, so, uh, now guys, uh, let's start. Uh, so basically today, I'm going to talk about a vector database. So, not just today, so even today and tomorrow. So, I will be talking about, uh.

00:07:23.000 --> 00:07:30.000
Uh, vector databases, plus in my last class, in my previous class, so I have already discussed about APIs.

00:07:30.000 --> 00:07:36.000
And, uh, I have given you, uh, assignments, a kind of a challenge, and I've seen that, that many people have already done that, right?

00:07:36.000 --> 00:07:48.000
So many people were able to solve the assignments, and uh… Uh, yeah, uh, many people were not able to solve the assignment, and maybe you have not tried, maybe you have not attempted. So…

00:07:48.000 --> 00:08:05.000
Go ahead, attempt it, yeah? That is going to help you out a lot. Believe me, that is going to help you out, and you can go and ask to the people who have already solved the assignments. So, they are able to get a real, real kind of a knowledge. So, I'll keep on giving you assignment, a lot of assignment, a lot of challenges.

00:08:05.000 --> 00:08:09.000
And, uh, yes, your responsibility is to solve it, and if you're not solving it.

00:08:09.000 --> 00:08:36.000
Then… You will not be able to fill the gap, simple, yeah? So, only Behook parties remaining challenge was awesome. That's amazing, yeah. So solve it, solve the hook. So Behook is also a kind of an API, so where you are sending a POST request, uh, at some, like, uh, triggers, basically, or whenever you are observing some sort of an event, so it's not very different from the API. In terms of our definitions, so yeah, it's different, but yes, in terms of a.

00:08:36.000 --> 00:08:49.000
Creation, it's not very different. Fine, everyone. So, let's get started. It's working now, okay, so please redirect from here.

00:08:49.000 --> 00:08:58.000
Yeah, so I believe, uh, live class, right? So… I have already updated our live class date and link inside your dashboard. There was some.

00:08:58.000 --> 00:09:04.000
There's some conversation which is going inside, uh. That's, uh, WhatsApp group, I believe?

00:09:04.000 --> 00:09:20.000
Okay, fine, not an issue. Fine, guys, let's start. So, do we have to submit the last assignment anywhere? No, you don't have to submit assignments, so basically, uh, I have asked everyone to discuss your assignment and share your assignment with your teammates, with your batchmates, basically. So…

00:09:20.000 --> 00:09:26.000
It's not our assignment to where you are going to summit, I'm going to evaluate, because we are not in college, basically. We are not.

00:09:26.000 --> 00:09:40.000
Like, I keep on saying these lines, right? That we are not in a college, we are not preparing for the exam. We are trying to build something. We are, like, learning all of these things so that we can build, uh, develop some of the tools, some of the functionalities on our own.

00:09:40.000 --> 00:09:47.000
So, we'll follow the exact same pattern. So that will… progress towards that particular path.

00:09:47.000 --> 00:09:58.000
Uh, now I'm going to, uh, disable a chat, so you will be able to chat with me, right? And after a live class, obviously, I'm going to enable it.

00:09:58.000 --> 00:10:12.000
Yeah, so that we all can chat. Okay, so now, let's get started, guys, and like I said, so today I will be talking about, uh, vector database. Now.

00:10:12.000 --> 00:10:24.000
So, I believe we all are aware about our databases. Uh, we have so many different, different kind of databases in our market, and we have been using our databases in almost all the applications.

00:10:24.000 --> 00:10:34.000
Uh, it doesn't matter which application you are going to build, maybe a BIB application, maybe a mobile application, maybe a AI ML kind of application, you are going to build.

00:10:34.000 --> 00:10:49.000
Obviously, you will end up using some sort of a databases to store a data, to process a data in a step-by-step manner. So everywhere, databases are required, right? And we have been using our databases since age, I would say. So, obviously.

00:10:49.000 --> 00:11:00.000
Uh, the kind of a database that we are using now, the kind of a database that we were using maybe two or three decades back, it was different, obviously, in terms of a scale, in terms of feature and functionalities.

00:11:00.000 --> 00:11:05.000
But we have been using it. So here, uh, the thing is that, that, uh.

00:11:05.000 --> 00:11:15.000
We are going to introduce our new kind of a database in a generative AI, and that database is called as a vector database.

00:11:15.000 --> 00:11:27.000
So, you might have heard about a database called as a SQL database, maybe, right? So, where MySQL or MSSQL or maybe a DB2, maybe a PostgreSQL.

00:11:27.000 --> 00:11:37.000
Or SQLite, so all these things comes in between, right? So you might have heard about us, SQL database, right, everyone? Yeah? I believe we all have heard about our database.

00:11:37.000 --> 00:11:44.000
Sql, at least? Yes? So, SQL database always try to store a data in a…

00:11:44.000 --> 00:11:50.000
Tabular format, you can say, right? So, where you will be having a column names, you will be having technically a schema.

00:11:50.000 --> 00:11:59.000
And then inside that, so you try to store a data, and then it will generally go for a sequential search, right? So whenever you will try to find some sort of a data.

00:11:59.000 --> 00:12:04.000
One by one, one by one, in a sequential search manner, so it will try to find out the data set from your.

00:12:04.000 --> 00:12:12.000
Tables, basically. So wherever we had structured data, right, completely structured data, so structured means.

00:12:12.000 --> 00:12:18.000
A kind of a data where you can define the schema, means column name, right? And then, accordingly, you can try to store it.

00:12:18.000 --> 00:12:27.000
So this is where a SQL kind of a system comes into a picture. Then, in a later stage, NoSQL. Nosql means not only SQL.

00:12:27.000 --> 00:12:39.000
So, NoSQL kind of a DB became very, very popular. So, where it was allowing you to store a data without even creating a schema in a format of a document.

00:12:39.000 --> 00:12:50.000
Or maybe in a form of a graph. Or maybe in a form of a time series. So, for example, we have a MongoDB, we have a Cassandra, we have an InfluxDB, we have Google BigQuery table.

00:12:50.000 --> 00:13:04.000
Uh, we have, uh, like, uh. Hbs, by the way, so key value pair. So, we have a lot of NoSQL kind, or new 4J, right? So, we had a lot of NoSQL kind of databases.

00:13:04.000 --> 00:13:14.000
So, where we can try to store our data in a form of a document, or maybe in a form of a time series, maybe in a form of a graph relationships.

00:13:14.000 --> 00:13:33.000
A different, different, like, for purposes, a different, different kind of a DB is been designed. Ultimate goal was to store our data, right, and make read and write faster. That was the ultimate goal, right? So that I will be able to read a data, and I will be able to write a data as fast as possible. Depends upon the nature of the data.

00:13:33.000 --> 00:13:40.000
Now, so when this generative AI becoming, like, became popular in last, like, 3 years.

00:13:40.000 --> 00:13:47.000
Uh, so people have started thinking about a new kind of a databases, yeah? New kind of databases.

00:13:47.000 --> 00:13:54.000
And, uh, they have named it as a vector DB. Now, what is the meaning of a vector, by the way?

00:13:54.000 --> 00:13:57.000
What kind of a data we will be able to store.

00:13:57.000 --> 00:14:04.000
In those vector DB, what kind of advantages I will be able to get out of that VectorDB.

00:14:04.000 --> 00:14:10.000
Where we are going to use that vector DB in a generative AI application, or maybe in any other application apart from generative AI.

00:14:10.000 --> 00:14:23.000
So, there are so many questions. You all will get, uh, in general, right? So, when you will start studying about this, uh, vector database, and we'll try to answer those questions in my today's class. Yeah?

00:14:23.000 --> 00:14:30.000
Hi, everyone! Yep. Context is clear. What we are going to do.

00:14:30.000 --> 00:14:43.000
Yeah, I was just trying to set the context, by the way, fine? So, there was a SQL DB, there was a NoSQL kind of a DB, and there is a vector DB, so in case of generative AI application.

00:14:43.000 --> 00:15:00.000
In general, so we are going to use a vector database. Now, we'll try to explain you. We'll try to explain you that, uh… how internally these vector database is going to work. What is meaning of a vector, by the way? So, we'll try to understand that as well.

00:15:00.000 --> 00:15:18.000
Uh, we are going to even do an installation, uh, later stage, maybe in a second half of this class, we are going to do an installation, and after doing an installation, so we'll try to convert our data set into an embedding, so where your URI API is very much required. Without that, you will not be able to do it.

00:15:18.000 --> 00:15:26.000
And then we'll try to store those data inside a vector DB, and then we'll try to do some sort of a query.

00:15:26.000 --> 00:15:31.000
Right? We'll try to do some sort of a query, and we'll try to understand that.

00:15:31.000 --> 00:15:37.000
As per my query, what is our response I'm receiving? So, in a step-by-step manner, we'll try to move.

00:15:37.000 --> 00:15:43.000
So, let's get started, guys. So, let me share my scribble ink, SCR.

00:15:43.000 --> 00:16:00.000
Yeah. Okay, so…

00:16:00.000 --> 00:16:12.000
Vector dB, vector database, basically. We are going to discuss. Now. So, what is vector, first of all? Any idea, guys? What is vector?

00:16:12.000 --> 00:16:18.000
Yeah? Any idea?

00:16:18.000 --> 00:16:28.000
What is the meaning of a vector, by the way? So, list of a scalar, collection, why we can't… we use NOE that exists.

00:16:28.000 --> 00:16:39.000
Uh, having both magnitude and direction, okay. Just give me a layman example. I'm not looking for some fancy example, I'm not looking for, like, you know, kind of example which no one can understand.

00:16:39.000 --> 00:16:44.000
I'll name an example, I'm, like, a… looking for.

00:16:44.000 --> 00:16:55.000
To get a similarity search, okay, velocity, given number of values, arrow we use in math, okay? So, we are getting some amazing answer.

00:16:55.000 --> 00:17:04.000
Uh… Okay, okay. Uh, convert into a numerical value, magnitude, uh, a vector is a simple way to store and compare.

00:17:04.000 --> 00:17:28.000
Information using numbers, okay, convert into a numerical value. Mmm, okay. Are they… metrics, direction, and magnitude, okay?

00:17:28.000 --> 00:17:34.000
Okay, so basically, see. So, in mathematics, generally, we try to talk about a.

00:17:34.000 --> 00:17:41.000
A scalar entity, and we talk about, basically, a vector entity, right? So, we talk about a vector entity.

00:17:41.000 --> 00:17:49.000
So, a scalar is nothing but a kind of a data, right? A kind of a data, which will be having only a magnitude. For example.

00:17:49.000 --> 00:18:00.000
Or weightage, you can say. For example, if I'm trying to write 10 over here, right? A simple 10 if I'm going to write over here. So, simple 10 means it's representing a value.

00:18:00.000 --> 00:18:18.000
Maybe are 10 numbers, so 10 units of something, 10 unit of a mobile phone, 10 unit of Apple, orange, mango, guava, whatever, right? Or maybe a 10KG, it is going to represent maybe 10 cointel, it is going to represent maybe a 10 meter, it is going to represent.

00:18:18.000 --> 00:18:23.000
Right? At the end of the day. So, a scalar is… so, basically, this 10 is a scalar, because it is just having a.

00:18:23.000 --> 00:18:31.000
Magnitude attached with this one. So, I will be able to, when someone is saying that, okay, fine, so the weight is 10KG.

00:18:31.000 --> 00:18:40.000
So, I will be able to understand just that magnitude, that, okay, fine, so that's that weightage, right, of that value, I will be able to understand.

00:18:40.000 --> 00:18:53.000
And then, obviously, I will be able to do a comparison. So, maybe someone has, like, a written aid, so I can say that, that, okay, so 10 is greater than 8. And I'm just trying to do the comparison with respect to the magnitude over here.

00:18:53.000 --> 00:19:01.000
And this is technically called as scalar entity, a kind of an entity which is going to have just a magnitude, nothing else, right? Nothing else.

00:19:01.000 --> 00:19:12.000
But, let's suppose, right? So, let's suppose I have a coordinate, right? So, let's suppose I have a coordinate, let's suppose X1 and Y1.

00:19:12.000 --> 00:19:26.000
And if I'm going to represent this coordinate as 1, right, if I'm going to represent this particular coordinate as 1, so can I say that, that here we have a magnitude?

00:19:26.000 --> 00:19:31.000
Right? So, here we have a magnitude, as well as we have a direction.

00:19:31.000 --> 00:19:41.000
Yeah? So, to represent this coordinate, generally we say, right, latitude, longitude, yeah? Latitude, longitude. So, you will be able to get a coordinate.

00:19:41.000 --> 00:19:52.000
And the coordinate looks something like this, let's suppose. So, here, so whenever we are trying to say that we have X1 and Y1, and 1, so 1 for X1, 1 for Y1, we have over here.

00:19:52.000 --> 00:20:02.000
Then you will be able to represent this entity on your 2D plane, right? On your 2D plane. So, let's suppose this is my x-axis, and this is my Y axis.

00:20:02.000 --> 00:20:10.000
This is my origin. So, here we have 1, here we have 2, here we have 3. Again, we have 1, again we have 2, and again, we have 3 over here.

00:20:10.000 --> 00:20:23.000
Now, if we have to represent this 1, right, 1, so what is the magnitude of X? So, technically, it's a 1. So, till this point, right? What is the magnitude of Y? So, basically, it's a… this one.

00:20:23.000 --> 00:20:26.000
This much, right? So, here, if I'm going to combine both.

00:20:26.000 --> 00:20:32.000
So, this point, right, this particular point, is going to represent what? X1, comma Y1.

00:20:32.000 --> 00:20:48.000
Yeah, this particular point. And if I'm going to draw an arrow over here, so this is… like, uh, I can simply say that, that, okay, this X1Y1, so in case of X1Y1, so it's 1 unit far from origin.

00:20:48.000 --> 00:20:58.000
On X direction, and 1 unit far from. Origin in Y direction. So, this is how I can call it out in general, right? How I will be able to call it out.

00:20:58.000 --> 00:21:08.000
And here, so this arrow which I'm trying to draw as of now. So, obviously, it is trying to suggest me some sort of a direction with a magnitude.

00:21:08.000 --> 00:21:15.000
Right? So, technically, this entity will become a vector entity. So, wherever we have a magnitude.

00:21:15.000 --> 00:21:30.000
Right? Wherever we have a magnitude, as well as we have a direction. So, in some direction, we are moving, right? So, when I have said, like, 1, so you can't move in this direction. So, you have to move only in this direction, because this is the direction of 1 comma 1.

00:21:30.000 --> 00:21:39.000
So you have a proper direction given to you. Now, this entity is technically represented as a vector entity.

00:21:39.000 --> 00:21:48.000
Making sense, guys? Yeah? Making sense? So, in this 2D coordinate, so we are able to represent this entity. As simple as that.

00:21:48.000 --> 00:21:54.000
Now, there is a possibility that we have X1, comma Y1, J1.

00:21:54.000 --> 00:22:03.000
Maybe something like this. So, 1 comma, 0, 2. Now, this is going to represent a 3D data.

00:22:03.000 --> 00:22:17.000
Right? Three-dimensional data. So, I need x-axis, I need y-axis. As well as I need a JED axis somewhere. So, I need a JIT axis if I have to represent this data in certain plane, right? In a 3D plane, if I have to represent this data.

00:22:17.000 --> 00:22:27.000
So, obviously, I have to consider height, length, and width, all three together, in general, right? So, whenever, like, you will look into the room where you are sitting, even as of now.

00:22:27.000 --> 00:22:37.000
So, you must be able to find out a height, you will be able to find out a width, you will be able to find out a length. So basically, we all are living into a 3D space. We as a human being.

00:22:37.000 --> 00:22:47.000
Are living into a 3D space. So, any dot in this space, you will be able to represent it by using three coordinates, any dots, right? Any dots.

00:22:47.000 --> 00:22:52.000
And here's where I'm trying to write all of these things. So, this plane is technically a 2D plane, yeah? 2d plane.

00:22:52.000 --> 00:23:02.000
So, where I'm trying to, like, write each and everything. Now, uh, so, if I have maybe a dataset, something like X1, Y1.

00:23:02.000 --> 00:23:08.000
J1 and maybe A1, yeah? So, again, this is basically a 4D dataset.

00:23:08.000 --> 00:23:20.000
Now, to represent this dataset, in this particular manner, I need a 4D plane, which is eventually not possible. So yes, I can imagine, I can virtually create this 4D plane.

00:23:20.000 --> 00:23:28.000
But, practically, it's not possible. But yes, practically, it is possible to represent any kind of a data.

00:23:28.000 --> 00:23:33.000
In four dimensions, right. Practically, it is possible to represent some data into 5 dimension.

00:23:33.000 --> 00:23:40.000
Practically, it is possible to represent a data, I mean, to a thousand dimensions also, right.

00:23:40.000 --> 00:23:51.000
Although you will not be able to visualize it, although you will not be able to see it, but yes, practically, it is possible to represent those kind of a data into that many number of dimensions. Maybe Indura 1000 dimension.

00:23:51.000 --> 00:23:57.000
Which we are going to do it today itself. So, we are going to represent our dataset into a 1536 dimension.

00:23:57.000 --> 00:24:06.000
Bill will show you that. We'll show you that. But… The meaning of… technical meaning of a vector is very simple. So, if we have a magnitude.

00:24:06.000 --> 00:24:11.000
And along with that, if we have a dimension, so that entity is technically called as.

00:24:11.000 --> 00:24:16.000
Vector, right? That entity is technically called as a vector. Now.

00:24:16.000 --> 00:24:21.000
So, why we are even talking about a vector over here?

00:24:21.000 --> 00:24:29.000
That's our very first question, right? We are able to understand that what is vector, but why we are even talking about a vector. So, see, guys.

00:24:29.000 --> 00:24:42.000
Uh, in case of generative AI, right? In case of generative AI, we will end up dealing with lots of unstructured data, or you can say that we will end up dealing with lots of kind of a data.

00:24:42.000 --> 00:24:53.000
So, where you will not be able to find out any kind of a schema set, right? So, you will end up dealing with lots of sentences, lots of phrases.

00:24:53.000 --> 00:25:00.000
Or maybe such kind of a lines, or maybe a prompt, right? So, we try to write a prompt just like a plain English, and the beauty of generative AI is.

00:25:00.000 --> 00:25:07.000
That majority of application. You will be able to use it, or you will be able to get an outcome.

00:25:07.000 --> 00:25:20.000
Just as a English, right? Just as a English, which is technically an unstructured data set. So, maybe you will try to give an input as an structure, and you will receive an output as an unstructured data set.

00:25:20.000 --> 00:25:29.000
Now, so, system, when I talk about this unestablished dataset, for example, if I'm trying to say that my name is.

00:25:29.000 --> 00:25:36.000
My… name is S-U-D-H, right? So, when I say this.

00:25:36.000 --> 00:25:45.000
To a system. System will never be able to understand this English language. Never means, literally, never it will be able to understand this English language.

00:25:45.000 --> 00:25:55.000
System understands only one… two things, 0 and 1, binary. This is how this entire computer system has been built, right? So, system just understand our numeric value.

00:25:55.000 --> 00:26:04.000
And again, on a low label, so it is just going to understand 0 and 1. And whenever we talk about a computer, the computer that we are using as of now.

00:26:04.000 --> 00:26:15.000
At the end of the day, it will be able to do a mathematical calculation, but it will never be able to understand these words, this alphabets. Even though we try to give input as alphabet, but again.

00:26:15.000 --> 00:26:24.000
It will never be able to understand. So, eventually, it try to convert each and everything, maybe into an ASCII, maybe into some different representation that we are going to use it.

00:26:24.000 --> 00:26:31.000
Case by case, and it will be able to do a mathematical calculation. And as an output, you will be able to see the.

00:26:31.000 --> 00:26:46.000
Final result. Now. No. So here, like I said, so in case of a generative AI, majority of time, not majority, I would say 99.9% of the time, you will be dealing with this kind of a sentences, right? You will be dealing with this kind of a.

00:26:46.000 --> 00:27:06.000
Complete unstructured data. Now, so, you will have to do a comparison between two data sets, right? You will have to maybe, like, uh… do a lot of, like, prompting over there based on the prompt, so you will be able to get the answer. Maybe you will end up doing a lot of search operation over there, so based on the similarities, so it should be able to give you the output.

00:27:06.000 --> 00:27:11.000
So, at the end of the day, the only thing is that that we'll be dealing with.

00:27:11.000 --> 00:27:17.000
Kind of a unstructured data. Now, when we are dealing with unstructured data.

00:27:17.000 --> 00:27:22.000
But to where the similarity of the data matters a lot, right? Similarity means.

00:27:22.000 --> 00:27:31.000
The connection between two datasets matters a lot, so that system will be able to give you the better output, or the nearest output system will be able to give you.

00:27:31.000 --> 00:27:47.000
So, at that point of a time. I need a kind of a evaluation, I need a kind of a parameter, so which can help me out to give me a better and better result, based on the similarity between two sentences. For example, so if I have written my name is Dhanshu.

00:27:47.000 --> 00:27:52.000
Is equals to sentence number 1, right? And then, if I have written.

00:27:52.000 --> 00:28:06.000
Soudh is… my name, right? So, both the sentence, I have phrased it in a very, very different way. My name is Sudhanchu, and Sudha is my name. So, both the sentences is almost similar.

00:28:06.000 --> 00:28:12.000
Right? But if you are going to do a word-to-word matching, so maybe it will not look that similar to you.

00:28:12.000 --> 00:28:18.000
Now, so, uh… Apple is red.

00:28:18.000 --> 00:28:26.000
Sentence number 3. Now, there is no similarity between this, this, and the last sentence which I have written. Apple is red.

00:28:26.000 --> 00:28:37.000
So, obviously, I need some evaluation, I need some parameters, so whenever I have to do a comparison between any of these two sentences, any of these two sentences.

00:28:37.000 --> 00:28:44.000
I will be able to do a comparison. I will be able to, like, evaluate that, okay, fine, so which sentence is closer to which one?

00:28:44.000 --> 00:28:50.000
Right? And this is… to do that, right? To do that.

00:28:50.000 --> 00:28:56.000
I have to convert these entire data. Into unnumerical representation.

00:28:56.000 --> 00:29:01.000
Unless and until I'm not going to convert this data into a numerical representation.

00:29:01.000 --> 00:29:12.000
I will not be able to do a math, I will not be able to do a calculation. And here, so in one of the sentences, maybe I just have 4 or, like, 3 or 2 words, but there is a possibility that I will end up getting a sentence.

00:29:12.000 --> 00:29:25.000
Where in just one sentence, I will be having 100, 200 words. And that's not equivalent. So, in different, different sentences, I will end up getting a different, different kind of a word. And that is going to create a kind of a problem for me.

00:29:25.000 --> 00:29:33.000
So, keeping that in a mind. We try to convert this entire data set into a vector.

00:29:33.000 --> 00:29:40.000
Into a vector. Vector means maybe into two dimension, maybe into three dimension, maybe into 100 dimension, maybe into a.

00:29:40.000 --> 00:29:45.000
Thousandth dimension means, so, in this 2D, so we try to represent.

00:29:45.000 --> 00:29:50.000
Let's suppose, different, different, different, different kind of a data set.

00:29:50.000 --> 00:30:00.000
Now, when we are able to represent this dataset over here, so obviously I will be able to do, uh, distance matching, or maybe I will be able to find out that, okay, fine, so which one is far.

00:30:00.000 --> 00:30:08.000
Which one is near? And based on that, I will be able to tell you that, okay, fine, so this data is similar to this one, or that data is similar to that one.

00:30:08.000 --> 00:30:19.000
But yes, the very first roles and responsibility of mine will be to represent this data into a vector space, or into a numerical format.

00:30:19.000 --> 00:30:23.000
Light into a numerical format. Now, once my dataset is available into a vector format.

00:30:23.000 --> 00:30:29.000
Then, go ahead, do the math, and then you will be able to find out the final result. Now.

00:30:29.000 --> 00:30:35.000
On the question. So, when I say that, uh, try to find out a similarity between two data sets.

00:30:35.000 --> 00:30:42.000
Right? Try to find out similarity between two data sets. So, maybe I can try to match.

00:30:42.000 --> 00:30:48.000
Or I can try to find out a distance, so based on a distance, we will be able to evaluate that how far.

00:30:48.000 --> 00:30:52.000
This dataset is from this one, or maybe this dataset is from this one.

00:30:52.000 --> 00:30:59.000
One possibility, right? There is other possibility that you will be able to find out. So, let's suppose I have two data sets.

00:30:59.000 --> 00:31:04.000
Right? So, this is one dataset which I'm trying to represent in 2D coordinate.

00:31:04.000 --> 00:31:10.000
So X1Y1, let's suppose X1. Y1, and this is the another dataset.

00:31:10.000 --> 00:31:18.000
I'm trying to represent X2Y2, and obviously, both of this data set is technically a 2D vector.

00:31:18.000 --> 00:31:22.000
There is a possibility that based on the angle, right, based on the angle between the data.

00:31:22.000 --> 00:31:28.000
I will be able to do a similarity search. I will be able to do a mapping that, okay, so how.

00:31:28.000 --> 00:31:35.000
Far or how near this dataset is from this one. And maybe I have another dataset over here.

00:31:35.000 --> 00:31:50.000
So… again, I can try to… maybe do a mapping between this and this, or maybe this and this, and based on the angle, I will be able to say that, okay, so which one is near to which one?

00:31:50.000 --> 00:31:55.000
So, this is where the similarity search comes into a picture. And, uh, again.

00:31:55.000 --> 00:32:07.000
To find out a similarity between two data sets. I need a vector. I need a numerical. Numerical value. Without a numerical value, I will not be able to do any such kind of a things.

00:32:07.000 --> 00:32:12.000
We'll try to understand that, how similarity such calculation is going to happen. I'll show you the.

00:32:12.000 --> 00:32:22.000
Manual calculation as well, right? But, uh… Is this fundamentals clear to all of us guys, that what is the meaning of vector, the very first fundamental?

00:32:22.000 --> 00:32:30.000
Uh, what we are going to do with the dataset, how we are going to do it? We'll talk about it, but what we are going to do with respect to a dataset, is it clear.

00:32:30.000 --> 00:32:36.000
And, uh, again, so, what is a basic definition of, uh, matching between a data?

00:32:36.000 --> 00:32:40.000
So maybe with the help of angles, maybe with the help of distances.

00:32:40.000 --> 00:32:45.000
So, we are going to find out, basically, a matching between our data. How?

00:32:45.000 --> 00:32:49.000
How calculation is going to happen, we'll show you, yeah? We'll show you. Okay.

00:32:49.000 --> 00:32:59.000
So, uh, Chirag is saying that, but we were doing kind of a similar search in RDBMS and NoSQL databases earlier as well, for example, a regex-like operator, where specifically we.

00:32:59.000 --> 00:33:16.000
Thought of going… Uh, so why specifically we thought of going to the vector DB? See, so we have been doing a similarity search, not just like a now inside our vector database, but even inside our Elasticsearch DB, or maybe in other DBs.

00:33:16.000 --> 00:33:31.000
Now, over there, so we try to do a key-value pair mapping, or based on the hashing, so we were trying to store our data, and whenever we try to do a search, so based on the index mapping, or based on the hashing, that it will be able to find out. It was trying to give me a result.

00:33:31.000 --> 00:33:39.000
Right? And this is what has been, like, we have been doing in case of a SQL, in case of a NoSQL databases.

00:33:39.000 --> 00:33:44.000
But here, so, when I have to do a sentence-to-sentence comparison, right, sentence-to-sentence comparison.

00:33:44.000 --> 00:33:55.000
And not just that, right? Centers-centers comparison means I'm not saying that I'm trying to do a word-to-word kind of a matching. No, that is important. That's completely fine, right?

00:33:55.000 --> 00:34:02.000
But we have to take care of semantic, and we have to take care of a syntactic. Both means.

00:34:02.000 --> 00:34:13.000
So we have to take care of even a grammar, that which one is going to come after, which one is going to come before. We have to even take care of our punctuation, and keeping all of these things in our mind.

00:34:13.000 --> 00:34:19.000
Right? Keeping all of these things in our mind, we have to do a search.

00:34:19.000 --> 00:34:27.000
Then your NoSQL-based similarity search or SQL-based similarity search is going to fail, and then you have to adopt this particular approach.

00:34:27.000 --> 00:34:33.000
Yes? This particular approach. Why? What is the reason? The reason is very simple. See?

00:34:33.000 --> 00:34:40.000
So here we have a sentence, right? My name is Sudhanshu, okay? I have a simple and plain sentence over here.

00:34:40.000 --> 00:34:44.000
Now, when I will try to convert the sentence into a vector representation.

00:34:44.000 --> 00:35:00.000
Right? Into our vector representation. So, it's not like I will be just using, uh, just one-to-one kind of a mapping that, okay, for my just give one for name, just try to give two for is, try to give 3 for sooth, try to give 4, right? No, not in this way.

00:35:00.000 --> 00:35:11.000
What I will be doing over here, so I will be using a transformer. I will be using a pre-trained model, which has been trained on a Hughes Corpus, which has been trained on a.

00:35:11.000 --> 00:35:23.000
Entire world's knowledge, right? So, which understands our grammar, which understands which will come before, which will come after. Technically, attention model I'm talking about. A transformer-based attention model I'm talking about.

00:35:23.000 --> 00:35:34.000
So, I will be using such kind of a model, ChatGPT, right? Chatgpt, let's suppose, a GPT-based model, if I'm going to use. So, we all know that GPT is having a knowledge of entire world.

00:35:34.000 --> 00:35:44.000
So, it understands our grammar. Grammar means, so which one will come after, which one will come before, so where we have to apply a punctuation, so what should be the tone, each and everything, it understands, right?

00:35:44.000 --> 00:35:58.000
So I will be using that kind of a model, so which understands all of these things, and then I will try to convert this entire data into its numerical representation, because at the end of the day, when we say models, right, what is the meaning of model?

00:35:58.000 --> 00:36:04.000
Technically, a numeral calculation, we are. We are trying to do, right? When you try to send.

00:36:04.000 --> 00:36:07.000
Some data into a chat GPT, maybe by writing a prompt.

00:36:07.000 --> 00:36:14.000
So, it converts immediately those data into a numerical representation, and then based on calculation.

00:36:14.000 --> 00:36:26.000
You'll see the output. This is technically happens, right? Because the system never understands your chat GPT even never understands the… English word. So, he tried to convert it. So, here, we will be using.

00:36:26.000 --> 00:36:35.000
Right? We will be using a transformer-based or GPT kind of a model by which I can convert those data into a vector space.

00:36:35.000 --> 00:36:41.000
And then I can go ahead with a similarity search, or maybe I can go ahead with the distance calculation.

00:36:41.000 --> 00:36:52.000
So, here the main agenda was not to do just a similarity search. See, I could have done a similarity search even in case of a MongoDB, even in case of a document, right? That is possible.

00:36:52.000 --> 00:36:59.000
But how we are trying to convert our data into a numerical space, that matters in our first place.

00:36:59.000 --> 00:37:07.000
Making sense, guys? Yeah? Making sense?

00:37:07.000 --> 00:37:25.000
Yes? Yeah. So, this is the important part, because I need someone, so, who can convert a data into a numerical space by understanding a grammar, by understanding a punctuation, by understanding semantic and syntactic meaning of the data, not just maybe some sort of a one-to-one mapping. So this is where.

00:37:25.000 --> 00:37:33.000
The major part comes into a picture with respect to a vector dv, and then if we are trying to do a similarity search, obviously I will be able to get the best result, because.

00:37:33.000 --> 00:37:45.000
In terms of similarity search, we are not doing very much different, right? So, it's a cosine similarity that we are going to find out. Maybe we are going to find out the Euclidean distances, maybe we are going to find out a Manhattan distances.

00:37:45.000 --> 00:37:49.000
So, this is something that we have been doing even in my old approach.

00:37:49.000 --> 00:37:53.000
Right? But the things which was missing at that point of a time was.

00:37:53.000 --> 00:38:02.000
Conversion of the data. From one state to another state, means this, uh, English, plain English state to the numerical.

00:38:02.000 --> 00:38:13.000
Vector state. Okay, so I'll come to that point, right? I'll come to that point, but before that, so let me talk about, basically, a similarity search. So, let's suppose, right?

00:38:13.000 --> 00:38:21.000
Let's suppose we have two vectors. So, let's suppose we have a vector, uh, D1, right? So, vector D1.

00:38:21.000 --> 00:38:27.000
And its value is 101. I'm just considering a numerical directly.

00:38:27.000 --> 00:38:32.000
We have a vector D2, and we have a value 0.

00:38:32.000 --> 00:38:42.000
1, 1, right? We have a vector D3. And we have a value, let's suppose, 1, 1, and 0.

00:38:42.000 --> 00:38:54.000
And then we have, let's suppose, a vector D. And its value is technically 10 0. So, these are the vector which has been given to me, and these are the 3D vector which has been given to me.

00:38:54.000 --> 00:39:02.000
Now, if anyone is going to ask me a question that, uh, how similar or how far.

00:39:02.000 --> 00:39:09.000
D4 is from D1, or D4 is from D2, or D4 is from AD3.

00:39:09.000 --> 00:39:17.000
So, how I will be able to do a calculation? So, here, I can try to use something called as cosine similarity. So, COSINE.

00:39:17.000 --> 00:39:29.000
Cosine similarity. Cosine similarity. Now, what cosine similarity, uh, like us says? So, cosine similarity says that, that if you're trying to find out some similarity between two vectors.

00:39:29.000 --> 00:39:37.000
So, let's suppose a vector A and a vector V. So, dot product of a vector A and vector B divided by a magnitude of a vector A.

00:39:37.000 --> 00:39:42.000
Into magnitude of a vector B. So, this is going to give you, finally.

00:39:42.000 --> 00:39:50.000
Uh, similarity between two of these vectors, as simple as that. This is going to give you similarity in between two vectors.

00:39:50.000 --> 00:39:54.000
So now, this one is going to yield a result, 1.

00:39:54.000 --> 00:39:59.000
Or maybe Agile, or maybe minus 1. So, closer to 1, not exact to 1, I would say.

00:39:59.000 --> 00:40:10.000
So, if your final value, if your result is closer to 1, so you can say that, that, uh, it's highly similar.

00:40:10.000 --> 00:40:19.000
Highly. Similar, if value is closer to 0, so you can say that, that no similarity at all.

00:40:19.000 --> 00:40:28.000
No similarity, and E value is coming closer to 1, so it's simply say that, that completely.

00:40:28.000 --> 00:40:34.000
Completely dissimilar, yeah? Completely dissimilar, or it's, like, in a complete opposite direction.

00:40:34.000 --> 00:40:45.000
So, here, this is the formula for a cosine similarity, and we know that that, uh… cost theta value always fluctuates between minus 1 to 1. We are already aware about it, right?

00:40:45.000 --> 00:40:52.000
So based on that, so we are going to find out our cosine similarity, and then again, if value is coming closer to 1, highly similar, value is coming closer to 0, dissimilar.

00:40:52.000 --> 00:41:04.000
And then values coming closer to minus 1. Simply say that it's a complete, like, going into a different direction. So, it's not at all similar to each other. Means two vectors are not, like, a… equal to each other at all.

00:41:04.000 --> 00:41:12.000
Now, what is this vector? So, in case of generative AI, or in case of my dataset.

00:41:12.000 --> 00:41:17.000
It is nothing but a representation of some of the sentences, right? Some of the sentences, some of the words, as simple as that.

00:41:17.000 --> 00:41:28.000
So how we are going to come to this particular stage? We'll see. Like I said, so we are going to use a transformer model, or we are going to use kind of a model, which is going to give me a final vector, or maybe an embeddings.

00:41:28.000 --> 00:41:33.000
But here, right? So here we have a cosine similarity formula. Now, my question was again same.

00:41:33.000 --> 00:41:41.000
So, let's try to find out a similarity between D4 and D1, yeah? So, let's try to find out a similarity between a D4 and a D1.

00:41:41.000 --> 00:41:46.000
Okay, I will be able to find out similarity between D4 and D1. So here.

00:41:46.000 --> 00:41:58.000
The cosine similarity. D4, and D1, let's suppose. So, dot product of D4 and D1 divided by magnitude of D4.

00:41:58.000 --> 00:42:02.000
And magnitude of… D1, right? This is the formula.

00:42:02.000 --> 00:42:10.000
So here, if we have to find out a dot product between D4 and D1, so simple, 1 into 1.

00:42:10.000 --> 00:42:18.000
0 into 0, 0 into 1. Addition of everything, right? So, 1 into 1 plus 0 into 0, plus…

00:42:18.000 --> 00:42:29.000
0 into 1. So, 0 into 1. Divided by magnitude of D1, so if we have to find out D4 and D1, so magnitude of D4 is what?

00:42:29.000 --> 00:42:32.000
One is square plus 0 is squared plus 0 is square.

00:42:32.000 --> 00:42:42.000
Into magnitude of D1. So, one is square plus 0 is square plus 1 is square. I believe in our childhood, we have done this kind of a mathemat.

00:42:42.000 --> 00:42:47.000
Mathematical calculation a lot, right, everyone? Yep.

00:42:47.000 --> 00:42:59.000
We have done that, right? So many times, yeah? So, the numerator is going to be 1, and denominator is going to be 1 squared, so it is going to be 1 into…

00:42:59.000 --> 00:43:07.000
Right? So, one is square, and then one square plus 1 is square, so 2 is, uh, basically, it'll be like a 2.

00:43:07.000 --> 00:43:16.000
And, uh, root of 2 is 1 point. 414. So, ideally, it is going to be, after calculation.

00:43:16.000 --> 00:43:32.000
0.707. So this is going to give me. The cosine similarity between D4 and D1. So yeah, it's coming closer to 1, so I can say that, that yes, there is some similarity between D4 and D1. So, yeah, there is a similarity, I can say.

00:43:32.000 --> 00:43:42.000
So, 1 and 1 is same, 0 and 0 is same, and the only difference is 1 and 0. So, there is some similarity, right? And even with the help of this result, I'm able to observe that, that there is some similarity between.

00:43:42.000 --> 00:43:55.000
This D4 and D1 vector, these two vectors, right? So, this is how we can try to do the math, this is how we can try to do a cosine similarity check. Now, cosine is nothing but it is just trying to do a calculation, and based on that calculation, it is trying to tell you.

00:43:55.000 --> 00:44:00.000
They look at how similar these two vectors are. Similarly, I can try to do a calculation with respect to.

00:44:00.000 --> 00:44:14.000
D4 and D2, right? D4 and D3. Now we have a three-dimension vector. There is a possibility that I will be having a 30-dimension vector, 1, comma 2, comma 3, comma 4, comma 1, comma 4.

00:44:14.000 --> 00:44:25.000
I will be able to do a calculation. Right? I will be able to do a calculation. So, it doesn't matter what is the length of the vector, right, what is the length of the vector, I will be able to find out a cosine similarity.

00:44:25.000 --> 00:44:33.000
Between two data sets. Yeah? And I will be able to eventually tell you that, okay, fine, so which one is similar to which one?

00:44:33.000 --> 00:44:44.000
This is where cosine similarity comes into a picture. The another one is, basically, you could distances, right? Another one is basically a Euclidean distances.

00:44:44.000 --> 00:44:52.000
So, again, I believe we all able… we all understand that what is the meaning of Euclidean distances, and how do we find out.

00:44:52.000 --> 00:44:57.000
So, equine distances between two vectors are nothing but this particular distance, right? This particular distance is.

00:44:57.000 --> 00:45:05.000
So, here we have, like, our D4 and D1. So, like, an equivalent distance between D4 and D1.

00:45:05.000 --> 00:45:09.000
Euclidean distance between D4 and D1 is nothing but root of.

00:45:09.000 --> 00:45:16.000
1 minus 1. Square, 0 minus 0.

00:45:16.000 --> 00:45:30.000
Square, plus 0 minus 1. 0 minus 1 is square. Now, do the math, 1 minus 1 is square, 0. 0 minus 0 squared is 0. And then 0 minus 1, so, uh, square, it is going to be, technically, 1.

00:45:30.000 --> 00:45:36.000
So, this is going to give you a Euclidean distances at the end of the day. Simple, right?

00:45:36.000 --> 00:45:41.000
So again, this is going to be a parameter where I will be able to measure a similarity between a data.

00:45:41.000 --> 00:45:52.000
But cosine is better, as cosine is going to consider an angle. So even if there will be a smaller deviation, right, even if there will be a small deviation, it will be able to capture it.

00:45:52.000 --> 00:46:02.000
Right? It will be able to capture it. And it is going to give you a better similarity result. So, majority of the vector databases that you will be able to find out.

00:46:02.000 --> 00:46:09.000
You will see that they are using a cosine similarity in a very, very first place, or maybe even a hybrid approach.

00:46:09.000 --> 00:46:20.000
Right? Hybrid approach, combination of both. But yeah, majority of the databases are trying to use a cosine similarity. Okay, so we are, we are now able to understand that how cosine simulatic calculation has been done. And again, from an interview perspective.

00:46:20.000 --> 00:46:32.000
People are going to ask you a question. That, can you please explain me a cosine similarity, right? And how this calculation happens. Although the calculation is very simple, but yeah, so we tend to forget.

00:46:32.000 --> 00:46:36.000
This information. So that's a region, so it becomes a interview question.

00:46:36.000 --> 00:46:44.000
For all of us. Nimble. No. So, this is completely fine, right? This is completely fine. We are able to understand, like, uh…

00:46:44.000 --> 00:46:53.000
What is vector? Then… how we can try to check these two different, different vectors, whether they are similar, whether they are not similar.

00:46:53.000 --> 00:46:58.000
What is a cosine similarity? And plus, we are able to recall our Euclidean distances as well.

00:46:58.000 --> 00:47:05.000
Now it's time to do each and everything in a practical manner, right? It's time to do each and everything in a practical manner.

00:47:05.000 --> 00:47:09.000
And, like I told you, right, and like I told you.

00:47:09.000 --> 00:47:18.000
The major part is not to do this calculation. Not to find out a clearian distances, not to find out a cosine similarity. What is the major part, guys?

00:47:18.000 --> 00:47:25.000
Yeah? What is the major part?

00:47:25.000 --> 00:47:30.000
Find a similarity? No, if we have a vector, I will be able to find the similarity, right?

00:47:30.000 --> 00:47:39.000
The major part is converting your dataset into a vector space, and what model you are using, which model you are using, that matters a lot.

00:47:39.000 --> 00:47:50.000
Right? That is the major part. Because, see, at the end of the day, similarities check means for. We are trying to do some sort of a math, and we are trying to, like, check some sort of a distances, or maybe an angle.

00:47:50.000 --> 00:48:02.000
This is what we are able to understand. Now, this is not a major part. Obviously, if I'm able to represent our data in a correct numerical format, right, correct numerical format, so there will be a similarity.

00:48:02.000 --> 00:48:09.000
Right? There will be a similarity. So, the major part is our data conversion, and which model we are going to use.

00:48:09.000 --> 00:48:17.000
To do that conversion, that matters a lot. So that's the reason, whenever you are trying to convert your data.

00:48:17.000 --> 00:48:32.000
Your sentences into a numerical space, into a vector space. You have to make sure that you are using a model which is having a most of the knowledges, or which has been trained on a huge corpus, on a huge amount of the.

00:48:32.000 --> 00:48:44.000
Database. Data sets, right? So now, uh…

00:48:44.000 --> 00:49:03.000
If you will go to Hugging Face, so Hugging Face is basically a repository, so where you will be able to find out, like, every kind of models, right? So, here, if you are going to search for embeddings, you will be able to find out so many models for embedding, right? So many models for an embedding, you all will be able to find out.

00:49:03.000 --> 00:49:13.000
Not just one, not just two, but maybe thousands of models, so Quinn-based models, 6 billion PowerPoint, 6 billion parameter, quen embedding, 8 billion parameter model, Jinja embedding.

00:49:13.000 --> 00:49:19.000
V4, QUEN3 embeddings, 0.6 billion parameter. So, quen is, again, one of the famous series, right?

00:49:19.000 --> 00:49:24.000
So, you will be able to find out EMBE double D-I-N-G. So, you will be able to find out.

00:49:24.000 --> 00:49:42.000
Uh…

00:49:42.000 --> 00:49:48.000
Where is embedding task, by the way?

00:49:48.000 --> 00:50:03.000
Somewhere you will be able to see it, or maybe go for the search, right? Go for the search. So you will be able to find out not just one or two models, but like I said, so you will be able to find out thousands, thousands of models that people have trained, and they have released its weight. That, okay, go ahead, use those models.

00:50:03.000 --> 00:50:13.000
And then, uh, convert your dataset into an embeddings, means try to pass your English sentences, and from on the other end, it will try to give you the vector representation, right?

00:50:13.000 --> 00:50:18.000
Vector representation. Now. So, here.

00:50:18.000 --> 00:50:25.000
In case of a hugging phase, right, in case of a hugging phase, you will be able to find out a best or best model, there is no doubt about it.

00:50:25.000 --> 00:50:36.000
Right? There's no doubt at all. Recently, they have even released ChatGPT OSS20B and 120 billion parameter for our text generation, right? So, which was, again, one of the.

00:50:36.000 --> 00:50:41.000
Very, very, very, very powerful model that people have released so far. And I have even shown you.

00:50:41.000 --> 00:50:47.000
Inferencing. Have you, have you seen that, guys? Inferencing over a GPU?

00:50:47.000 --> 00:50:51.000
And even if they tradition, I have already, like, shown it to everyone.

00:50:51.000 --> 00:50:57.000
Last week itself.

00:50:57.000 --> 00:51:04.000
Yes? Ramesh Kumar is saying that not getting a calculation part. Okay, well, not an issue, I'll try to repeat it once again.

00:51:04.000 --> 00:51:13.000
Yep. Okay, someone is saying not yet, so please try to go ahead with that. So, like, GPT has open sourced this model, and I have already created a video where I have shown you that.

00:51:13.000 --> 00:51:19.000
How you can host it in a local, plus how you can host it on a cloud GPU. Not just that, how you can create the API.

00:51:19.000 --> 00:51:26.000
Uh, maybe in your free time, so if you have any kind of interest, you can just go ahead with that.

00:51:26.000 --> 00:51:35.000
Now here, so, like I said, you will be able to find out so many embedding models. So, let's try to search one of the embedding models. So, I'm going to search QUEN3 embedding 8 billion parameter model.

00:51:35.000 --> 00:51:40.000
Now, if you will go ahead with this one and follow along.

00:51:40.000 --> 00:51:48.000
So, you have to download the model. From a hugging face, and then, after downloading the model, so you have to basically pass your document.

00:51:48.000 --> 00:52:00.000
It is going to convert your document. Into a vector space, and then you can go ahead with the similarity search. So, everywhere, you will be able to find out the exact same kind of a story.

00:52:00.000 --> 00:52:04.000
I can go ahead and I can try to use this model. There is no issue at all.

00:52:04.000 --> 00:52:11.000
But, uh, if I can get a better option, right? If I can get a better option.

00:52:11.000 --> 00:52:24.000
Maybe I'll end up using that. And keeping that in my mind… and maybe I don't have to, like, go ahead and download the model, right? So that better option we have given you inside a URI.

00:52:24.000 --> 00:52:28.000
Basically. So, let me open up this, uh, yeah, signed in.

00:52:28.000 --> 00:52:33.000
So, if you will go to Yuri API, by the way, so let's suppose if I'm going to URI API.

00:52:33.000 --> 00:52:43.000
And if I'm going to a code example, right? So here, text embedding and similarity. So, we have already given you a model access.

00:52:43.000 --> 00:52:51.000
Which model access? So, we have given you text embedding 3 small model access. Now, this model is coming from whom?

00:52:51.000 --> 00:53:00.000
So, this model is coming from OpenAI itself. And what is the total context length it generates? 1536. So, I'll show you that, I'll show you that.

00:53:00.000 --> 00:53:10.000
Right? But this is one of the best models that you will be able to find out, which is having all the knowledge. And that was the problem, right? That was the problem that we have discussed.

00:53:10.000 --> 00:53:18.000
So, the major problem is not to do a similarity search, right? Similarity search is a very simple mathematics. The major problem is to use a best-of-best model.

00:53:18.000 --> 00:53:29.000
Which understands our data, and based on those data understandings. It converts. It converts my data into a vector or numerical space, right?

00:53:29.000 --> 00:53:37.000
So, here, in case of a URI API, if you are going to consume, so text embedding 3 small model access, we have given to you.

00:53:37.000 --> 00:53:43.000
You don't have to do any kind of installation, you don't have to do any kind of a setup, and directly call the APIs.

00:53:43.000 --> 00:53:50.000
And bingo, right? And bingo. So we'll do it. We'll do it now itself, yeah? So, we are, we are going to, like, do it now.

00:53:50.000 --> 00:54:05.000
So here, let's suppose I'm going to copy this API, and open up your VS Code, guys, VS Code Go Collab, whatever you want, just try to open up.

00:54:05.000 --> 00:54:11.000
Anything? Could I have, like, VS Code or any Jupyter notebook, just try to open up.

00:54:11.000 --> 00:54:41.000
Where you can, like, write the code. So, open folder and D drive, let me…

00:54:41.000 --> 00:54:48.000
So I have just, like, uh… created a folder. Now, inside this, I'm going to create a test.

00:54:48.000 --> 00:54:53.000
Underscore vector. Dot IPYNB. Fine?

00:54:53.000 --> 00:54:58.000
And now here, so let me attach any of the kernel.

00:54:58.000 --> 00:55:06.000
And, uh, here, so I'm going to paste the same code, right? I have not done any kind of a modification. Now, what this code is trying to do.

00:55:06.000 --> 00:55:14.000
So, this code is trying to give you an endpoint. I believe now we all understand what is the meaning of a endpoint, right?

00:55:14.000 --> 00:55:17.000
That's the reason, so in the very first place, I talked about an API, because.

00:55:17.000 --> 00:55:28.000
Again and again, so many times, we will end up using an API, or maybe we will end up building an API. So, here is my endpoint, right? Here is my endpoint. Now, here, authorization-wise.

00:55:28.000 --> 00:55:34.000
So, I can try to pass a token, so I can try to replace this entire static with my token.

00:55:34.000 --> 00:55:40.000
How I will be able to get my token? Again, very simple. So, I can go to the API key.

00:55:40.000 --> 00:55:47.000
Click over here, and now create API key. I can give any names, and then I will end up creating the.

00:55:47.000 --> 00:55:59.000
Tokens, right? I will end up creating the tokens. Last class, we have done that, right? Those who are new, so… just do it once, and let me know if you have any kind of issue. I know some of you have joined.

00:55:59.000 --> 00:56:05.000
Uh, this class as a very first class, that's completely fine. So, you will get familiar with all of these things.

00:56:05.000 --> 00:56:11.000
So here, we have a function, and we have a replaced static with the URI API key.

00:56:11.000 --> 00:56:21.000
Right? Payload-wise, so we are trying to use this particular model, text embedding 3 is small, from our OpenAI itself, we have hosted, so we are going to use this particular model.

00:56:21.000 --> 00:56:25.000
And what is the data we are trying to pass? So, we are trying to pass.

00:56:25.000 --> 00:56:28.000
Our weather is sunny today, so maybe I can try to pass over here.

00:56:28.000 --> 00:56:34.000
My name is… Is?

00:56:34.000 --> 00:56:39.000
Yeah? Anything I can try to pass, simple, right? So anything, any kind of a data I can try to pass.

00:56:39.000 --> 00:56:43.000
Now, if I'm going to hit it, if I'm going to execute it.

00:56:43.000 --> 00:56:50.000
I will be able to see some sort of a output. I've executed it right, I've executed it, now let's try to print the embeddings.

00:56:50.000 --> 00:56:57.000
So here, this is the numerical representation. This is basically a numerical representation. It has returned, yeah?

00:56:57.000 --> 00:57:11.000
Now, what is our length of this particular vector? So, maybe I can try to calculate. So, length is… 1536. It is already showing you as a save, so length is basically 1536, means.

00:57:11.000 --> 00:57:23.000
Whatever data you are going to pass, whatever data, right? Whatever data, doesn't matter whether you have one single word or you have a hundreds of words, maybe you have thousands of words in one single line.

00:57:23.000 --> 00:57:28.000
Right? Whatever data you are going to pass inside this model.

00:57:28.000 --> 00:57:36.000
Eventually, it will try to convert all of those data into a numerical values, right, into a numerical values of length what?

00:57:36.000 --> 00:57:50.000
1536, or in other words, I can say that dimension. Means it is trying to use 1536 different different coordinates, just like X coordinate, Y coordinate, J coordinate. So, 1536 coordinate to represent one point.

00:57:50.000 --> 00:57:59.000
That one point in that space is representing our data. Is it making sense?

00:57:59.000 --> 00:58:11.000
Yes, everyone? Yep. Share the code. Code is already available, by the way, guys. Code is already available here inside a code example. So if you'll click over here.

00:58:11.000 --> 00:58:20.000
And come down, text embedding and similarity, you will be able to find out. So code is already available, you just have to replace your static.

00:58:20.000 --> 00:58:31.000
Basically, our API key, and then change your sentence. So whatever sentence that you would like to pass, you can just pass those sentences, and it is going to give you return as a embedding, a vector, right?

00:58:31.000 --> 00:58:38.000
So here, we are able to do this conversion, and this conversion simply means that I'm trying to represent this data.

00:58:38.000 --> 00:58:47.000
Right? This data, in 1, 5, 3, 6 dimension. So, any model, right, any model that you are going to use in this entire world.

00:58:47.000 --> 00:58:54.000
To convert your data into its numerical representation, it will always be having a fixed dimension.

00:58:54.000 --> 00:58:58.000
It will always try to give you a return into a fixed dimension.

00:58:58.000 --> 00:59:07.000
Yes? So 1536 dataset point here. So, yes, 1536, for example, right? So, for example.

00:59:07.000 --> 00:59:14.000
If I'm trying to say… So, if I'm trying to say.

00:59:14.000 --> 00:59:20.000
That in this coordinate, we have this point. Now, what is the meaning of this point?

00:59:20.000 --> 00:59:24.000
Meaning of this point is, it's… we are trying to represent it in a 2D coordinate, right?

00:59:24.000 --> 00:59:29.000
So, meaning of this point is X1Y1. Simple, two values is representing.

00:59:29.000 --> 00:59:34.000
This point. Similarly, in a 3D space, right, in a 3D space.

00:59:34.000 --> 00:59:40.000
Let's suppose we have an X coordinate, we have basically a Y coordinate, and we have a J coordinate.

00:59:40.000 --> 00:59:44.000
And we have a point somewhere, right? Somewhere over here. What is the meaning of this point?

00:59:44.000 --> 00:59:48.000
What is the meaning of this point? So, I mean, this point is X.

00:59:48.000 --> 00:59:54.000
Y and J, right? So, this one single point is being represented by 3 coordinate, this is the meaning of a 3D axis, right?

00:59:54.000 --> 01:00:06.000
And this is the meaning of a 2D axis. Similarly, so when I say 1536, 1536 number of vector we have, so similar… so we have basically 1,53C is kind of a dimension, which we can't visualize.

01:00:06.000 --> 01:00:15.000
Right? But there is only one point, means one data in that space, so which has been represented by 1536 number of a coordinate. So, more dimension.

01:00:15.000 --> 01:00:19.000
More coordinate, it simply means that your dataset is more explanatory.

01:00:19.000 --> 01:00:24.000
Right? Explanatory. And, uh, that too, you will be able to do, uh.

01:00:24.000 --> 01:00:29.000
Search or similarity, you will be able to find out in a best possible manner.

01:00:29.000 --> 01:00:35.000
Why? So, it means above text is represented in 1536, exactly, yes.

01:00:35.000 --> 01:00:45.000
Yeah? Just like I'm trying to, like, show you 2D and 3D. So, here, in this case, it's 1, 5, 3, 6 dimension.

01:00:45.000 --> 01:00:49.000
And that is altogether representing one single point in that space.

01:00:49.000 --> 01:00:57.000
Are you able to imagine it, guys? That's the reason I've given you these two examples, 2D and 3D.

01:00:57.000 --> 01:01:04.000
Yep, passandra is saying no. So, what is, like, what you are not able to understand, by the way?

01:01:04.000 --> 01:01:12.000
Are you able to understand a 2D? Are you able to understand a 3D? So what is the meaning of a 2D data? What is the meaning of a 3D data?

01:01:12.000 --> 01:01:24.000
Yep. Okay, so people are trying to paste their, like, code, and along with their own API key.

01:01:24.000 --> 01:01:28.000
And, uh, we have given you a limitation with respect to the API key, by the way, guys.

01:01:28.000 --> 01:01:34.000
It's only one lakh token per day, so if you're going to share your API key.

01:01:34.000 --> 01:01:40.000
People will exhaust it. Yes?

01:01:40.000 --> 01:01:45.000
Archit Kiara data. So, just, uh, look into your API key. The last one is static.

01:01:45.000 --> 01:01:49.000
So you have copy and pasted your API key, but it's wrong.

01:01:49.000 --> 01:01:58.000
You forgot to, like, remove the last static.

01:01:58.000 --> 01:02:12.000
Yes? So, this is, like, a how easy it is, and this is what I'm trying to prove over here, that when I say 1536, doesn't matter what kind of a data I'm trying to pass, ultimately, the output which I will be able to get.

01:02:12.000 --> 01:02:18.000
It's all 1536. In case of this particular model. Now, in case of a different, different model.

01:02:18.000 --> 01:02:25.000
Sizes are going to be different, but maybe, like, 700-something, maybe 1000-something, maybe 1500-something.

01:02:25.000 --> 01:02:29.000
Uh, so OpenAI model which we are using inside a URI.

01:02:29.000 --> 01:02:37.000
It is going to give you a return of 1536, and the meaning of 1536 is very simple. 1536 coordinates, right.

01:02:37.000 --> 01:02:42.000
Is it representing this particular data. So, this is one coordinate, this is just one single dot, right?

01:02:42.000 --> 01:02:46.000
This entire data set, which it is trying to give me as an output.

01:02:46.000 --> 01:02:52.000
This is just a single dot. Right? Single dot in.

01:02:52.000 --> 01:02:57.000
These many dimensions. Just like we have a single dot. For this single dot.

01:02:57.000 --> 01:03:10.000
Or this one. And I know, but like, many of us are studying this, you know, geometry, and we are talking about the coordinate, but when it comes to our visualization.

01:03:10.000 --> 01:03:14.000
Most of us are not able to do it. All thanks goes to our schooling system.

01:03:14.000 --> 01:03:25.000
Right? Most of us, I have seen that, like, which is not, like, a… outlier for me. I have seen that in all of my classes, by the way, right?

01:03:25.000 --> 01:03:36.000
So it's a very common phenomena, which I have observed. That people struggles to, you know, visualize a 2D and a 3D and N dimension as well, and it's very difficult for…

01:03:36.000 --> 01:03:46.000
Most of us to understand. Uh, basic difference between a scalar and a vector. So hope that concept is clear now.

01:03:46.000 --> 01:03:54.000
Yes? Sorry, basic route here, SAPE1536, you are referring as a dimension. So, yeah, so if you have a vector.

01:03:54.000 --> 01:04:12.000
Simple. That's the reason I told you, right? So, all thanks to our schooling system, that we are learning something from our childhood, but still we are not able to visualize it, we are not able to relate it. So, see, just don't go after 1536, right? Let's just talk about, uh… two-dimension, yes? Let's just talk about the two-dimension.

01:04:12.000 --> 01:04:18.000
Now, any data set, any dot, let's suppose I have drawn this particular dot. This is one simple dot.

01:04:18.000 --> 01:04:22.000
Right? Now, so what is a value of this dot, by the way?

01:04:22.000 --> 01:04:29.000
In 2DS space, where we have an X coordinate and we have a Y coordinate. So, obviously, if we have to represent this dot.

01:04:29.000 --> 01:04:35.000
I need a value of X, I need a value of Y. Then only in a 2DS space, I will be able to represent.

01:04:35.000 --> 01:04:39.000
Yes? Then in the 2DS space, I will be able to represent.

01:04:39.000 --> 01:04:44.000
Now, uh, let me… I'll share my screen, yeah.

01:04:44.000 --> 01:05:14.000
Now, let's think in this way, by the way. Where is my third pen? Let me find out.

01:05:16.000 --> 01:05:30.000
Not able to find out my third pen, by the way.

01:05:30.000 --> 01:05:36.000
So, just try to think in this manner, yes? So, we have this one.

01:05:36.000 --> 01:05:40.000
We have a 2D space, so we have basically x-axis, and we have, basically, Y-axis.

01:05:40.000 --> 01:05:54.000
Now, anything… this is the plane, right? Plane means? So maybe, like, uh… Plain simply means that… that… Let's suppose this is the copy, right? So this is… this is trying to represent my 2DS space, so any dot over here.

01:05:54.000 --> 01:06:00.000
If I have to represent. So, obviously, I need this value, and I need this value. If I have these two values.

01:06:00.000 --> 01:06:03.000
Then only I will be able to represent something in a 2D space.

01:06:03.000 --> 01:06:16.000
Now, we all are living in a… tedious space, right? And let's suppose… let's suppose I have a small entity over here. For example, this entity, right? This is small, uh, like a…

01:06:16.000 --> 01:06:22.000
Cap we have over here into our 3D space. Now, if I'm going to ask you a question.

01:06:22.000 --> 01:06:27.000
That, uh, with respect to this particular room, right, because this particular room is having our walls.

01:06:27.000 --> 01:06:34.000
And it is having a surface, and it is having a ceiling. So, with respect to this particular room, what is a height.

01:06:34.000 --> 01:06:54.000
And how, like, uh, what is the height of this particular cap? How far it is from, like, that particular wall? So maybe a width, and… what is the length? Maybe from the… another wall. So, obviously, to represent this particular cap in this 3D space, you need all of these three coordinates, and all together, these three coordinates are going to.

01:06:54.000 --> 01:07:02.000
Represent just this particular cap, or this particular data set. Right? So, similarly, this is the meaning of coordinates, by the way, right? Everyone?

01:07:02.000 --> 01:07:12.000
So that's the meaning of our coordinates. So similarly.

01:07:12.000 --> 01:07:17.000
Duty, and then… 3D. So, I need 3 coordinates.

01:07:17.000 --> 01:07:28.000
To represent it, and I need 2 coordinates to represent this one. Now, so here we have… let's suppose four coordinate. Again, so 4 coordinate, you can't visualize.

01:07:28.000 --> 01:07:33.000
Not even in a programming languages, you will not be able to visualize it. Max to max till 3D, you will be able to visualize it.

01:07:33.000 --> 01:07:41.000
But yeah, there is a possibility that 4th coordinate exists. I can use, maybe, X, Y, G, and A.

01:07:41.000 --> 01:07:51.000
To represent a 4 coordinate. Maybe B. To represent a 5 coordinate, maybe C, XYZ, XYZ, A, B, C. So, to represent something into a 6 coordinates.

01:07:51.000 --> 01:07:57.000
Now, this is what I'm trying to do over here. So, I'm trying to just to represent this data into.

01:07:57.000 --> 01:08:08.000
1, 5, 3, 6. Coordinate. This is what it means. So, this is one of the vector, and what is the dimension of this vector, by the way? So, dimension of this vector is 1, 5.

01:08:08.000 --> 01:08:17.000
3, 6. Like, we have 2D over here, so we have 3D over here, we have 6D over here, we have 1, 5, 3, 6D over here.

01:08:17.000 --> 01:08:24.000
And eventually, it is representing in that space, because even in this 2D space, in this 2D space.

01:08:24.000 --> 01:08:31.000
These two coordinates are representing only one point. In 3D space, there's 3 quarters are representing only one point, only one dot, right?

01:08:31.000 --> 01:08:37.000
In 6-dimensional space, the 6 values are going to represent only one dot.

01:08:37.000 --> 01:08:45.000
In 1536 dimensional space. This entire one is going to represent only one dot, only one.

01:08:45.000 --> 01:08:52.000
Yes?

01:08:52.000 --> 01:08:56.000
So, SAPE is one of these how many coordinates we have to choose in case what is the reason of choosing 1536.

01:08:56.000 --> 01:09:03.000
See, 1536 is a model output dimension, so whenever we talk about a generative AI.

01:09:03.000 --> 01:09:14.000
So, we always talk about a context length. Now, what is the meaning of context length, by the way? The kind of an output, the length of the output, it will be able to give me, or maybe a length of the input it will be able to take.

01:09:14.000 --> 01:09:22.000
Yes? Because whatever model you are going to create, eventually, it's going to be a neural network.

01:09:22.000 --> 01:09:28.000
Right? It's on neurons, and… although there is a transformer architecture, but eventually it is going to be a neuron, right? In a neuron.

01:09:28.000 --> 01:09:41.000
So, every model will be having a constant, right? Constant input dimension, and every model will be having a constant output dimension. So, we say that, okay, fine, so the recent model with GPT has released.

01:09:41.000 --> 01:09:50.000
They have 128K of context, 1,28,000 of a context length, means that is the output it will be able to yield in one single go.

01:09:50.000 --> 01:09:56.000
Right? For one single input, so it will be able to give me that kind of, or maybe 4K of context.

01:09:56.000 --> 01:10:04.000
Maybe 8K of context, right? So, similarly, for an embedding models, so whoever has built this particular model.

01:10:04.000 --> 01:10:14.000
So they try to set that, okay, fine, so this is going to give you this much of output, so 1536 as an output. You will be able to find out a model which is going to give you maybe just as a 700 dimension.

01:10:14.000 --> 01:10:21.000
Right? Maybe just as 1028 dimension, as an output.

01:10:21.000 --> 01:10:27.000
Okay, so now… Uh, moving ahead, let me…

01:10:27.000 --> 01:10:37.000
So here, now we are able to understand that, uh, how with the help of Yuri API, right, how with the help of Yuri API, I can try to pass any kind of a data.

01:10:37.000 --> 01:10:42.000
Right? I can try to pass any kind of a data, and eventually, I will be able to generate the embeddings.

01:10:42.000 --> 01:10:49.000
Yeah? Eventually, I will be able to generate the embeddings. Now, coming back to the hugging phase, right?

01:10:49.000 --> 01:10:58.000
So, here, like I said, even inside a hugging Face, you will be able to find out not just one model, but a thousands of models. Yeah, thousands of models you will be able to find out.

01:10:58.000 --> 01:11:04.000
And you can try to use even these models, like, even try to use these models.

01:11:04.000 --> 01:11:17.000
To perform this embedding operation. And here, so text embedding. Qun3 embedding, 0.6 billion parameter model, if you're going to use. So, this is the size of the model, 0.6 billion parameter model, this is what it means.

01:11:17.000 --> 01:11:25.000
Layer inside this problem is transformer layer, is basically 28, a stagger layer, and sequence length is 32K, right?

01:11:25.000 --> 01:11:34.000
Embedding dimension is 1024. The model that we are using, so what is the dimension? 1536, right?

01:11:34.000 --> 01:11:41.000
So, what is the dimension? This model is going to give you, in terms of embedding, 1024. So, whatever model that you are going to use in this entire world.

01:11:41.000 --> 01:11:48.000
All of these models comes with the specification. Simple. All this specification people are going to mention.

01:11:48.000 --> 01:11:55.000
Doesn't matter, you are going to use it, like, from where, but yeah. So, embedding dimension is 1024. Now, similarly, Quen 3 series.

01:11:55.000 --> 01:12:02.000
Embedding for B if you are going to use. So, what is the embedding dimension? 2560.

01:12:02.000 --> 01:12:11.000
So the final output, it is going to give you, in our case, is 1536, right? So, if you're going to use Quen 3 embedding, 4 billion parameter model, this one is the.

01:12:11.000 --> 01:12:18.000
Final dimension. If you are going to use Quen 3 embedding 8 billion parameters, so it is going to give you embedding dimension is equal to 4096.

01:12:18.000 --> 01:12:22.000
So, inside a quen series, you will be able to find out these three models.

01:12:22.000 --> 01:12:42.000
Now, let's suppose if I have to use one of these models, so I can just try to click, and they are going to give you a sample code as well. So, here, this is the very simple, like, code. So, where you are going to call a sentence transformer, and then you are going to create an object of sentence transform by passing this model name, or whatever model that you are going to use.

01:12:42.000 --> 01:12:50.000
And then you are going to pass your data. So, it is going to, like, take your data, and then it is going to.

01:12:50.000 --> 01:12:59.000
Encoded. Maybe I can try to copy this one. And then, directly, I can try to use this one. Like, when, I can try to use any of these models.

01:12:59.000 --> 01:13:08.000
So let's remove everything. I don't… anyhow, these, like, things are… commented. Uh, maybe I'll remove a query as well.

01:13:08.000 --> 01:13:15.000
It's not required. Uh, document, fine, I'm going to use it.

01:13:15.000 --> 01:13:20.000
Okay. So I have removed…

01:13:20.000 --> 01:13:26.000
And I'm not looking for a similarity search, by the way. I'm looking for just a document embedding. Fine?

01:13:26.000 --> 01:13:34.000
Document embedding is something which I'm looking for. So I've removed everything, guys. I have removed all the code, by the way, and let's try to execute it. So, first of all.

01:13:34.000 --> 01:13:42.000
Uh, it will try to download those models, and once it will be able to download it, I'm just trying to call model.encode.

01:13:42.000 --> 01:13:48.000
And I'm trying to pass my documents, these particular documents I'm trying to pass. Means my data, right?

01:13:48.000 --> 01:13:56.000
Means my data. So, let's see.

01:13:56.000 --> 01:14:07.000
There is an issue, I believe, in terms of, uh… library installation, so let me handle that part.

01:14:07.000 --> 01:14:13.000
And it's, uh…

01:14:13.000 --> 01:14:26.000
8 billion parameter model.

01:14:26.000 --> 01:14:32.000
Maybe I can try to use the smallest one.

01:14:32.000 --> 01:14:38.000
Yeah, QN3 embedding. I think this is the name of the model.

01:14:38.000 --> 01:14:44.000
Let me copy this one. Yeah, so this is the…

01:14:44.000 --> 01:14:53.000
This is the smallest model, right? 6.6 billion parameter model.

01:14:53.000 --> 01:15:09.000
Okay, so it's technically a library issue. Let me fix that part, in terms of installation.

01:15:09.000 --> 01:15:16.000
I had a kernel, so where I have done those installations.

01:15:16.000 --> 01:15:33.000
No module name, sentence transformer, okay.

01:15:33.000 --> 01:15:40.000
I'm just trying to show you guys that how we are going to use some of the open source model, which is widely available to all of us.

01:15:40.000 --> 01:15:47.000
And, uh, if I can try to use one, then you will be able to use other thousands of models as well. And on a hugging face.

01:15:47.000 --> 01:15:53.000
They always give you a sample code. Again, sample code is nothing. Every time you will be able to find out that they are trying to call a sentence transformer.

01:15:53.000 --> 01:16:04.000
They're trying to pass the model name, which will be given eventually inside a Hugging Face itself, so you just have to copy this name all the time, right? All the time, so you have to copy this model name.

01:16:04.000 --> 01:16:11.000
And then hit the hugging phase. So, what it will do, it will try to download everything. It will try to download each and everything, and then.

01:16:11.000 --> 01:16:17.000
It will try to, you know, uh… do the inferencing with those models. So, transgenerational formula installation is done.

01:16:17.000 --> 01:16:23.000
Now… Yeah, now I believe it's working.

01:16:23.000 --> 01:16:30.000
So, it is trying to download the model, and then it will try to perform this operation in my local.

01:16:30.000 --> 01:16:42.000
Okay, so enable for repo, but HFX… D… failing, falling back to a regular HTTP download.

01:16:42.000 --> 01:16:50.000
Okay, it's a warning. So just wait for some time, guys.

01:16:50.000 --> 01:16:58.000
And in between, so even you can try this out. So I've just pinged you a code inside your chat.

01:16:58.000 --> 01:17:06.000
And before that, so you have to do this installation. Otherwise, it is going to give you an error.

01:17:06.000 --> 01:17:11.000
Okay, so everything, uh, I believe, was able to run successfully.

01:17:11.000 --> 01:17:21.000
Now, if I'm going to call my document embedding. So, it has returned me two embeddings, right? Because I had two sentences, one, and then we have a comma, and we have this another sentence.

01:17:21.000 --> 01:17:30.000
First sentence was very, very long, second sentence was… sorry, first sentence was very, very, like, small, and second sentence was too much long. You can try to write your own sentence as well.

01:17:30.000 --> 01:17:39.000
But for both of these sentences, it is going to give me the similar dimension. I can even try to check what is the dimension, so maybe document embedding of.

01:17:39.000 --> 01:17:44.000
Jiroth. So, let's try to extract the very first one. So, Jiroth.

01:17:44.000 --> 01:17:49.000
And then, this is the first one. And I can try to check a length of it.

01:17:49.000 --> 01:17:58.000
So, what is the length of this one? And length is 1024, right? 1024. Now, if you will go over here, and documentation.

01:17:58.000 --> 01:18:07.000
This is what they have mentioned, right? That Q3 embedding, uh, 0.6 billion parameter model, so this is going to give you embedding dimension, means a vector dimension is 1024.

01:18:07.000 --> 01:18:11.000
And, uh, yeah, I'm able to get 1024, as simple as that.

01:18:11.000 --> 01:18:19.000
So I'm able to use even an open source model, a model which is, like, widely available over a hugging face.

01:18:19.000 --> 01:18:27.000
Without even an API. Right? But yeah, the thing is that they download a model in your local system. So, if model size will keep on increasing.

01:18:27.000 --> 01:18:37.000
Every time you will have to download those big models, and then you have to do an inferencing. Or maybe you have to host the same model on your cloud infrastructure somewhere.

01:18:37.000 --> 01:18:44.000
So that, without even using a URI API, let's suppose some of you will not try to use a URI API. That's okay, that's completely fine.

01:18:44.000 --> 01:18:53.000
Without even using a URI API, like I have, like, I'm able to convert my data into a vector space, you can do it.

01:18:53.000 --> 01:19:00.000
Is it making sense? Yes, guys?

01:19:00.000 --> 01:19:05.000
Yeah? Fine? So how we are able to convert our dataset?

01:19:05.000 --> 01:19:17.000
Into our numerical space, into a vector space. By using a URI API, part number 1, and without using a URI API. So let's suppose Uri API is not available to me. No issue at all.

01:19:17.000 --> 01:19:26.000
It's completely fine, right? So, I have tons of model available outside. I can try to use those models, and I can try to convert my dataset into a vector space.

01:19:26.000 --> 01:19:33.000
And this is something that we all can try to do it. I believe code is available, so this is a code which I have pinged.

01:19:33.000 --> 01:19:39.000
To all of you, yeah? So, again, I have pinged you this code, guys, by the way.

01:19:39.000 --> 01:19:44.000
And you can try this out, but before that, so you have to do this installation.

01:19:44.000 --> 01:19:48.000
Pip install transformer. Otherwise, it is not going to work for you.

01:19:48.000 --> 01:19:53.000
Okay, so both of the things, we are able to do it.

01:19:53.000 --> 01:20:04.000
So, conversion of this data is done as per our discussion, so we are now able to convert… we are now into a situation, so where we can try to convert our dataset into this kind of a structure.

01:20:04.000 --> 01:20:08.000
This kind of a structure, right? A vector space in which we will be able to convert it.

01:20:08.000 --> 01:20:13.000
Now, coming to a cosine similarity. Yeah, now coming to our cosine similarity.

01:20:13.000 --> 01:20:25.000
So, how we will be able to prove. That, okay, so we have multiple sentences, and what is a similarity between those two sentences, in a practical manner?

01:20:25.000 --> 01:20:37.000
Yeah? In a practical manner. So, how we will be able to understand it. So, to do that, let me, like, create some build-ups over here, yeah? Let me, like, create some sort of a build-ups over here.

01:20:37.000 --> 01:20:44.000
So, let's suppose I'm going to use this model. I will try to show you with URI API, I'll try to show you even with this one.

01:20:44.000 --> 01:20:53.000
So, let's suppose we have this, uh… This one, right? I'm going to delete this sentences, I'm going to write my own sentences over here. So, my very first sentence is.

01:20:53.000 --> 01:21:02.000
Uh, my name is… So, the Hansu, and I am a software engineer. This is my very first sentence. Now, the second sentence is.

01:21:02.000 --> 01:21:09.000
Um, so the Hansu is my name. And I'm a software engineer. Fine?

01:21:09.000 --> 01:21:13.000
And then my third sentence is. Apple is red.

01:21:13.000 --> 01:21:22.000
Fine, and it is a fruit. Banana is a low, and it is a fruit. Some random sentences I'm trying to write over here.

01:21:22.000 --> 01:21:30.000
Now, uh, Sudhansu… So the answer is giving a…

01:21:30.000 --> 01:21:38.000
Class 4, gen. Ai. And going to a start.

01:21:38.000 --> 01:21:43.000
Class for Gen AI. Interview. Fine? Interview abrasion.

01:21:43.000 --> 01:21:58.000
Okay, so this is another sentence, uh, that I have. And, uh, okay, so these are the multiple sentences, so sentence number 1, sentence number 2, sentence number 3, sentence number 4, sentence number 5. So, in total, I have taken 5 sentences.

01:21:58.000 --> 01:22:03.000
Okay, that's cool. Now, I'll try to pass all of these 5 sentences inside this one.

01:22:03.000 --> 01:22:07.000
Inside this particular model. So, it is going to give me 5 vector.

01:22:07.000 --> 01:22:17.000
Yes, it is going to give me 5 vector, just like this one. So, I am just trying to correlate my theory with my practical. So, just like D1, D2, D3, D4 we had.

01:22:17.000 --> 01:22:24.000
We had a vector for that one, right? So, here, I have written 5 sentences, and for 5 sentences, now if I'll go and check.

01:22:24.000 --> 01:22:29.000
So I have 5 vector. Simple, 1, 2, 3, 4, 5. Simple, 5 vector.

01:22:29.000 --> 01:22:34.000
Dimension is 1024, I'm trying to use a QUEN3 model, 0.6 billion parameter model, right?

01:22:34.000 --> 01:22:42.000
So, we had basic… we have basically 5 vector, vector number 1, vector number 2, we have vector number 3, we have vector number 4, we have vector number 5, we have.

01:22:42.000 --> 01:22:52.000
Now, I have to check. Right? Now I have to check. So, vector number 1, or technically Jiroth index vector, this one, is representing this sentence.

01:22:52.000 --> 01:23:05.000
The second one is representing this sentence. The third one is representing this sentence. The fourth one is representing this, and fifth one is representing this sentences, right? Whatever sentences that we have taken.

01:23:05.000 --> 01:23:10.000
Now I have to check that what is our similarity, yeah? What is a similarity between.

01:23:10.000 --> 01:23:18.000
Sentence number 1 and sentence number 2 means I have to check what is the similarity between this and this.

01:23:18.000 --> 01:23:23.000
What is the similarity between this sentence, or maybe Apple is red and it is a fruit?

01:23:23.000 --> 01:23:34.000
Yeah? What is a similarity between this and maybe this one? Yeah? What is the similarity between maybe this and this one? So, we have to check. I can try to use maybe a Euclidean distances, or I can try to use maybe a cosine similarity.

01:23:34.000 --> 01:23:41.000
Is it making sense to all of us, guys? So shall we go ahead and do a cosine similarity?

01:23:41.000 --> 01:23:48.000
Yes? Shall we go ahead and do a cosine similarity between these two vectors?

01:23:48.000 --> 01:24:01.000
Yes, everyone? So, till this point, I think we are clear, right?

01:24:01.000 --> 01:24:14.000
Yeah, we are clear, and we also know what is the mathematics behind a cosine similarity, so dot product of two vector divided by magnitude, multiplication of magnitude, right? Magnitude, and this is going to give you.

01:24:14.000 --> 01:24:23.000
Uh, technically, uh, cosine similarity between two vectors. Okay. So, I have decided, I have decided that I will try to check what is the similarity between sentence number 1.

01:24:23.000 --> 01:24:38.000
And standards number 2, right? Because in terms of English, it looks like that there is a lot of similarity, but we'll try to check even mathematically, that what is the difference between, like, these two vectors that we have, yeah? These two vectors that we have.

01:24:38.000 --> 01:24:52.000
So, to find out this cosine similarity, right, to find out this cosine similarity, so we can try to use a direct library, so there is a library called as, so from.

01:24:52.000 --> 01:25:00.000
From a skill learn.metrics pairwise, you will be able to find out a direct function as a cosine similarity.

01:25:00.000 --> 01:25:04.000
Yeah? If you would like to use this one, use it.

01:25:04.000 --> 01:25:12.000
If you don't want to use this add function, we know the mathematics. So manually try to calculate a dot product of vectors, two vectors.

01:25:12.000 --> 01:25:22.000
Divided by magnitude of one vector into magnitude of a second vector. So, in either way, you can try to go, that's completely fine. But yeah, so inside of Python, inside a secular library, there's the.

01:25:22.000 --> 01:25:31.000
Function which, uh, they have already given to you, and you can try to, you know, pass the embeddings, and you will be able to find out the cosine similarity.

01:25:31.000 --> 01:25:44.000
Fine, everyone? Yeah? So, in which way we should follow, guys? So, shall we follow a manual way, or shall we use the formula?

01:25:44.000 --> 01:25:49.000
Yes? Okay, SKL and formula manual. People are saying, like, okay.

01:25:49.000 --> 01:26:00.000
Fine. So, let's go manual, then. Manuel is even very easy, believe me, I'll show it to you, right? Manual. So, from NumPy, uh, NumPy.

01:26:00.000 --> 01:26:08.000
Uh, import, uh, basically. Uh, there is a dot. So, in a numerator, we have a dot product, right? So, vector A dot vector B.

01:26:08.000 --> 01:26:18.000
So you can try to import a dot, which is going to build my numerator, and then if I have to build my denominator, right. So, I have to, like, do a normalization.

01:26:18.000 --> 01:26:29.000
Uh, this one? This one, and this one, right? And then do the multiplication. So this is called as normalization, finding the magnitude is called as norms, or normalization.

01:26:29.000 --> 01:26:35.000
So, here, uh, we can try to import a library, so numpy.

01:26:35.000 --> 01:26:42.000
N-u-m-p-y. Numpy, and then…

01:26:42.000 --> 01:26:47.000
And then import norms. So, now, let's try to choose our vector, right?

01:26:47.000 --> 01:26:56.000
So, sentence 1. Sentence 1 is what? This one. This one is my sentence one, right? So, from document embedding.

01:26:56.000 --> 01:27:05.000
From document embedding, try to get Jiroth element, right? This vector. This is my sentence 1, and then I have a sentence 2. Sentence 2 is this? By the way?

01:27:05.000 --> 01:27:11.000
Okay? Not an issue. So, let's try to build a sentence variable. Sentence.

01:27:11.000 --> 01:27:18.000
2. Sentence 2 is nothing but document embedding. 1. Now I have to find out that what is the similarity between sentence 1 and sentence 2.

01:27:18.000 --> 01:27:31.000
Fine, with the help of cosine similarity. We know the formula, so formula for cosine similarity is dot of… light, dot of in a numerator, sentence 1 and sentence 2, right? Sentence 1 and sentence 2, divided by.

01:27:31.000 --> 01:27:39.000
Normalization, right? Normalization of sentence 1 into sentence 2. Execute. Now, what is a similarity?

01:27:39.000 --> 01:27:43.000
Similarity is 0.96. What is the meaning of 0.96, by the way, guys?

01:27:43.000 --> 01:27:51.000
It is closer to 1. It is closer to 1 simply means that there is a high similarity, right? There is a high similarity.

01:27:51.000 --> 01:27:58.000
Making sense, guys? Yes? Everyone, and I think this was my expectation, by the way. That was my expectation, right?

01:27:58.000 --> 01:28:11.000
I was expecting the same thing. Now. We'll do a similarity check between this sentence, sentence number 1 that I have, and my sentence number 3. Apple is red, and it is a fruit.

01:28:11.000 --> 01:28:17.000
And this one. So, let's see. So, what is the similarity? It is going to give it to me. So, this is my sentence number 3, right? Sentence number 3 is available at 012.

01:28:17.000 --> 01:28:27.000
So, fine. Sentence number 3. Sentence number, uh, three variables I'm going to create, and it's 0, 1, 2, right?

01:28:27.000 --> 01:28:35.000
012. Apple is red, and it is a fruit. Okay, fine, this is my sentence number 3. Now, let's try to find out, uh…

01:28:35.000 --> 01:28:42.000
This one, between sentence number… Jiro… and?

01:28:42.000 --> 01:28:47.000
The second one, and here, 0 I'm going to give, yeah?

01:28:47.000 --> 01:28:56.000
Sorry, I have written sentence number 1 right. My bad. So, Sunday's number 1, and Sundays number.

01:28:56.000 --> 01:29:05.000
Not too. So sentence 1 and 3. 1 and 3. Oh, what happened?

01:29:05.000 --> 01:29:13.000
Sentence number 3, I think I have not executed. Okay. So, now the… The cosine similarity between.

01:29:13.000 --> 01:29:17.000
Send us number 1 and send us number 3 is… very less, 0.38.

01:29:17.000 --> 01:29:29.000
Yeah? 0.38, so closer to zero, closer to zero means. It's orthogonal. Orthogonal means 19 degree, perpendicular to each other. So, in a mathematical term, it's basically orthogonal, but orthogonal simply means that.

01:29:29.000 --> 01:29:35.000
Two vectors are perpendicular to each other, right? This is how we try to represent perpendicular, right? Orthogonal to each other.

01:29:35.000 --> 01:29:44.000
And it looks like, yeah, that that's a valid point. Right, everyone?

01:29:44.000 --> 01:30:03.000
Yeah? That's a valid point, right? So, we are able to even check what is the similarity between sentence number 1 and sentence number 3. By sentence number 1 was, my name is Nanshu, and I'm a software engineer. My sentence number 3 was Apple is red, and it is a fruit.

01:30:03.000 --> 01:30:08.000
By the way, yeah? Uh, maybe I can do a comparison between, like, 3 and 4?

01:30:08.000 --> 01:30:14.000
Simple, right? 3 and 4. So, here… Sentence number…

01:30:14.000 --> 01:30:18.000
4 is equals to… let's say this, yeah?

01:30:18.000 --> 01:30:24.000
And, uh, then… Same thing, dot product, sentence number, uh…

01:30:24.000 --> 01:30:30.000
3 and 4, right? So, Sunday's number 3. Pour.

01:30:30.000 --> 01:30:37.000
3… Yeah? So, sometimes number 3 and 4.

01:30:37.000 --> 01:30:48.000
Comparison. Apple is red, and it is a fruit, and banana is yellow, and it is a fruit. So there is a similarity, it is a fruit, it is a fruit, and uh… there are, like, some differences.

01:30:48.000 --> 01:30:53.000
So, it is giving me, like, a good result, good score. So, 0.78 kind of a score it is able to give it to me.

01:30:53.000 --> 01:31:01.000
So, this is how I'm able to convert my data into a vector space. Now I'm able to do even a similarity.

01:31:01.000 --> 01:31:06.000
Check. Yes. Now, if I'm going to use a UD API, let's suppose.

01:31:06.000 --> 01:31:15.000
Technically, I will end up getting a vector. Right? I will end up getting a vector. Once I have a vector, once my data set is available in this form.

01:31:15.000 --> 01:31:26.000
Right? I will do the math. To check how similar this data set is with respect to all the other data sets, right? With respect to all the other data sets.

01:31:26.000 --> 01:31:32.000
As simple as that. And… So, this is, uh, something which we are going to use.

01:31:32.000 --> 01:31:36.000
Not just today, not just tomorrow. Almost going forward in every classes.

01:31:36.000 --> 01:31:41.000
So whenever we will be trying to, like, study langchain, Llama Index, Langgraph.

01:31:41.000 --> 01:31:49.000
Through AI, wherever we'll be doing a tool calling, or maybe we'll be… end up doing maybe, uh… Uh, like, an agent building.

01:31:49.000 --> 01:31:56.000
Right? Every time, or maybe RAG application, so every time we are going to use.

01:31:56.000 --> 01:32:08.000
Somehow… this technique to convert my data into a vector space and then process it.

01:32:08.000 --> 01:32:20.000
Yes, everyone? Making sense? Result is different in my machine. So, yes, result is not going to be exact same, point-to-point same. Reason is very simple.

01:32:20.000 --> 01:32:29.000
You are trying to use a generative model. Right? Whether you are using Quinn or whether you are using a Yuri API, Open3 small model, right?

01:32:29.000 --> 01:32:39.000
You are using a generative model. Generative model gives me a different result and gives you a different result. Obviously, there will be a similarity between my and you, if we are using the same model.

01:32:39.000 --> 01:32:50.000
But what do I avoid it is not going to be the same. It's just like you are asking some question to a chat GPT, and same question I have asked to chat GPT, or maybe you are asking the same question twice, thrice, or five times. Every time.

01:32:50.000 --> 01:32:52.000
It is not going to give you a similar answer, right?

01:32:52.000 --> 01:33:01.000
Because it's a generative model.

01:33:01.000 --> 01:33:13.000
Getting my points? That why you will get a different number, and why I'm getting a different number.

01:33:13.000 --> 01:33:20.000
So, one question regarding the conversion of text to a vector, what's happening in a background? I know we are using a pretend model, and it uses transformer with n layers.

01:33:20.000 --> 01:33:26.000
But do we have to know about that? If you are aware about, uh, architecture of Transformer.

01:33:26.000 --> 01:33:32.000
It's well and good, because there is a chances that people are going to ask you. Now, your next question will be.

01:33:32.000 --> 01:33:47.000
How and where I will be able to learn the entire transformer architecture. So, if you will go to my previous generative AI NLP class, right, so you will be able to find out that I have explained the complete research paper called as Attention is All You Need.

01:33:47.000 --> 01:33:54.000
Over there, a transformer architecture, a multi-headed transformer architecture, layer by layer, I have already explained.

01:33:54.000 --> 01:34:02.000
So maybe that will be a… good reference point for all of you, so we are in detail, everything has been explained.

01:34:02.000 --> 01:34:10.000
Not able to run in a local, we have to shift to the… I think you can run in a local or collab, anything is fine, I just need a Jupyter Notebook installation, right?

01:34:10.000 --> 01:34:16.000
So, it doesn't matter whether you are running in local or you are running in a collab, like.

01:34:16.000 --> 01:34:29.000
Okay. Can you repeat why we are getting a different vector? Think in this way. If I'm going to ask a question to a chat GPT, and if you are going to ask a same question, no changes at all, right?

01:34:29.000 --> 01:34:33.000
Uh, so do you think that we both are going to get the same result? Same answer?

01:34:33.000 --> 01:34:40.000
Or if you are going to ask a same question twice to a chat GPT in a different, different interface, just open up a different interface.

01:34:40.000 --> 01:34:45.000
And then try to ask the same question. So do you think that it is going to give you the same answer?

01:34:45.000 --> 01:34:52.000
No. A reason is very simple. It's… that's the reason it's called as generative, right? Generative model. Generative means?

01:34:52.000 --> 01:35:01.000
Every time, based on the weights between the model, it is going to generate, produce a new kind of a data.

01:35:01.000 --> 01:35:05.000
So, that is a reason you will end up getting a different answer. I will end up getting a different answer.

01:35:05.000 --> 01:35:10.000
Similarly, so the model that you are using over here, the embedding model, it's also a generative model, right?

01:35:10.000 --> 01:35:24.000
It is trying to take over sentences as an input, an English sentence as an input, and then it is trying to generate its respective embeddings.

01:35:24.000 --> 01:35:44.000
Fine, everyone. I think we are good now.

01:35:44.000 --> 01:35:52.000
Yeah? Okay. So, till this point, it's completely fine, right? But what was a major agenda for this week, by the way?

01:35:52.000 --> 01:36:05.000
What was the agenda? We were trying to talk about, like, a vector, we are talking about the embeddings, now we know what is a vector, what is a embeddings, and how we can check the similarity between two data sets. So, all this low-level concepts are clear.

01:36:05.000 --> 01:36:10.000
What is the main agenda of this class? A vector database.

01:36:10.000 --> 01:36:16.000
Right? A vector database. So, what a vector database will do, by the way?

01:36:16.000 --> 01:36:22.000
What a vector database will do, yeah? We are going to store this data.

01:36:22.000 --> 01:36:26.000
Simple. The data that we are able to produce, this data, the highlighted one.

01:36:26.000 --> 01:36:39.000
So, in case of a SQL system, we try to create a table, right? And in case of a… in tables, we try to insert the data. In case of NoSQL, document, technically a JSON format, you can say, that is called as document key-value pair, right?

01:36:39.000 --> 01:36:47.000
We try to store it inside a NoSQL databases. In case of a vector database, we are going to store this one, this data.

01:36:47.000 --> 01:36:54.000
And then, when we are going to search, right, when we are going to search. So, let's suppose.

01:36:54.000 --> 01:37:03.000
I'm trying to search that, okay, so let's suppose I have already stored this data, yeah? I have already stored this data, and then I have this particular data set, I'm able to store, right?

01:37:03.000 --> 01:37:08.000
Now, I have a query vector. Let's suppose I have a query, and a query says that.

01:37:08.000 --> 01:37:12.000
I have a query vector. Our query says that, that who is.

01:37:12.000 --> 01:37:18.000
Sudahansu, right? This is my query, yeah? So, based on the query.

01:37:18.000 --> 01:37:27.000
I have to pull our data. From where? From a vector DB. This is the data which I have stored inside the vector DB, right?

01:37:27.000 --> 01:37:32.000
So, how, based on the query, I will be able to pull the data.

01:37:32.000 --> 01:37:38.000
Cosine similarity, right? So, what I will do, I will try to convert even this query.

01:37:38.000 --> 01:37:42.000
So, this data, let's imagine, this data is available in my.

01:37:42.000 --> 01:37:47.000
Vectordb, yes? This data set is available in my vector databases. Okay, that's cool.

01:37:47.000 --> 01:38:03.000
This was my raw data set. Now, user has fired a query, yeah? User has fired a query. So, whenever user has fired, this is the query, and based on the query, you have to extract a relevant output, a relevant answer, which is available inside your.

01:38:03.000 --> 01:38:13.000
Vectordb, which is already available inside your VectorDB. So, here, first of all, we'll try to convert my query into a embeddings by using an exact same method.

01:38:13.000 --> 01:38:19.000
Right? By using the exact same method, I will try to convert my query.

01:38:19.000 --> 01:38:28.000
Not this document. I will try to convert my query. Q query into our embeddings. Fine?

01:38:28.000 --> 01:38:39.000
Query, underscore EMBD, let's suppose I'm writing a… variable. So… I will try to convert my query into a embedding. So, for my query.

01:38:39.000 --> 01:38:44.000
This is my embedding. This is my embedding. So now I have an embedding available for a query.

01:38:44.000 --> 01:38:51.000
Yes? And then, I have so many documents out there. I have so many documents out there.

01:38:51.000 --> 01:38:59.000
Yeah? I have so many documents out there. Now, means all of this dataset is available inside my database.

01:38:59.000 --> 01:39:05.000
So what I will do… so, with respect to this query, with respect to this query, this embeddings of the query.

01:39:05.000 --> 01:39:11.000
I will try to check with first, with second, with third, with fourth, with fifth.

01:39:11.000 --> 01:39:19.000
Fine? I'll write a check. The distance, a similarity with 1st, 2nd, 3rd, 4th, 5th. I'll try to check a similarity with all of it.

01:39:19.000 --> 01:39:29.000
Right? And whichever is going to give me more similarity. That is going to be my answer. So that is going to be the answer of that particular query.

01:39:29.000 --> 01:39:41.000
Simple, right? So, here we have, uh, embedding. Now. So, distance 1, let's suppose I'm going to create. So, distance 1 means, like, query embedding, and then, uh, sentence 1.

01:39:41.000 --> 01:39:49.000
Right? So what is the distance between, like, these two? Then… distance 2, let's suppose if I'm going to create, so with sentence 2.

01:39:49.000 --> 01:39:56.000
So, distance to… distance… Uh, three, let's suppose I'm going to create. So, query embedding.

01:39:56.000 --> 01:40:03.000
And this one. Then, distance… 4. With our fourth sentence, I'm going to calculate.

01:40:03.000 --> 01:40:10.000
So, fine with 4 cylinders, we have 5, right? But that's fine. So, 1, 2, 3, 4. With all of these 4, I'm able to calculate the distance.

01:40:10.000 --> 01:40:18.000
Now, let's try to print a distance, right? So, maybe try to print a distance 1.

01:40:18.000 --> 01:40:24.000
Altogether. Distance…

01:40:24.000 --> 01:40:34.000
2, 3, and 4, right? So, distance 1 is this, distance 2 is this, distance 3 is this, distance 4 is this. Now give me the answer, guys.

01:40:34.000 --> 01:40:41.000
I had a query. I had a query. What was that query? My query was, who is Sudhanshu?

01:40:41.000 --> 01:40:46.000
Yup, my query was what? My query is, who is Sulhanshu? That was my query, right?

01:40:46.000 --> 01:40:52.000
Now, which sentence is going to be the top answer for my query?

01:40:52.000 --> 01:40:59.000
Yeah, which sentence is going to be top answer for my query?

01:40:59.000 --> 01:41:04.000
Ah, distance 2, but use 500. I'm using just 4, I think we all understand that part, right?

01:41:04.000 --> 01:41:11.000
Limit. So, as per this one, which sentence is going to be the answer of my query?

01:41:11.000 --> 01:41:18.000
I think we all know the answer, right? This is having a most similarity, closer to 1, right? Closer to 1.

01:41:18.000 --> 01:41:24.000
So, this one, and this is the distance for… distance 2, right, with sentence 2.

01:41:24.000 --> 01:41:29.000
And what was the sentence too, by the way? Sentence 2 was, Stansu is my name, and I'm a software engineer.

01:41:29.000 --> 01:41:41.000
Yeah? So this is closer, this is closer to the query sentence that we have called as who is Dhanshu. Now, this is how, and this is where we are going to use a DB.

01:41:41.000 --> 01:41:45.000
So, we are storing our embeddings into a databases, so whenever there will be a query, user is going to fire a query.

01:41:45.000 --> 01:41:54.000
It will go, and it will try to check that, okay, which one is similar, and it is going to eventually give the answer.

01:41:54.000 --> 01:41:59.000
Is it making sense to all of us guys?

01:41:59.000 --> 01:42:08.000
Yeah? Is it making sense? Okay. Now, so this is something which will internally happen inside a DB, again and again and again.

01:42:08.000 --> 01:42:15.000
So whenever you are going to fire a query, you are going to store the data. So, again, now let's come to the use case-wise.

01:42:15.000 --> 01:42:23.000
So, what will be the real-time use cases? So, for example, all of you are working in a company, right? Or maybe you are a student.

01:42:23.000 --> 01:42:33.000
And you are trying to prepare a nodes. Yes? So, basically, all of you are a student, and you are trying to prepare our notes, or maybe you are working in a company, let's suppose.

01:42:33.000 --> 01:42:39.000
And, uh, you have a lot of HR document, a lot of finance document, or maybe project document, right?

01:42:39.000 --> 01:42:44.000
We all used to keep a lot of documents while studying, while working, every time.

01:42:44.000 --> 01:42:52.000
Now, I wanted to create a database, right? So, where I can store all of those documents.

01:42:52.000 --> 01:42:58.000
Whatever data is there inside the document. And then I would like to build a chat interface.

01:42:58.000 --> 01:43:02.000
So, where it should answer, or it should give me always an answer.

01:43:02.000 --> 01:43:07.000
Based on the existing knowledge base, not based on the global answer.

01:43:07.000 --> 01:43:10.000
So I'm not looking for some direct answer from a chat GPT.

01:43:10.000 --> 01:43:17.000
What I'm expecting over here is that you should give me an answer from my data.

01:43:17.000 --> 01:43:29.000
Yeah? My data, because ChatGPT will not be having, or any other AI model, will not be having any idea about the… private data, right? Your private data. So, what is going inside your finances, what is going inside your HR department, what kind of a notes that you have taken.

01:43:29.000 --> 01:43:36.000
The debut will not be having those ideas, right? And you need an answer from that.

01:43:36.000 --> 01:43:54.000
Yeah? You need an answer in that format. No issue. Convert all of… read all of your PDFs or document, or whatever you have, right? Convert each and everything into a… embeddings. Store it into a vector DV, and in a chat interface, whenever I'm going to chat.

01:43:54.000 --> 01:43:59.000
It is going to do a similarity search. And then, it will try to bring.

01:43:59.000 --> 01:44:05.000
A dataset for me.

01:44:05.000 --> 01:44:14.000
Making sense, guys? Yes? A lot of use cases, right? Everywhere you go, you guys can go and use it, and that is something that we will be building in a class.

01:44:14.000 --> 01:44:21.000
But yeah, I believe fundamentals are clear, because. Uh, that is important. Building-wise, we can build everything in a 15-minute, that's not an issue at all.

01:44:21.000 --> 01:44:31.000
But yeah, if fundamentals are not clear, all of you are going to struggle, uh, going forward. So, I believe now it's done.

01:44:31.000 --> 01:44:40.000
Yes? So that was the whole idea. Now, so once we are able to… now we are able to understand the fundamentals, just the fundamentals, right?

01:44:40.000 --> 01:44:46.000
So, tomorrow, what I will do. So, I'll try to introduce some of the vector databases, like.

01:44:46.000 --> 01:44:53.000
Facebook AI similarity as such. One vector database, ChromaDB. Another vector database, Pinecone.

01:44:53.000 --> 01:45:05.000
Another vector database, Waviate, another vector database. So, I'll try to introduce all of these vector databases. Some of these vector databases, you will be able to install in your local.

01:45:05.000 --> 01:45:11.000
Some of these vector databases are already giving you a cloud interface, so you don't have to do any kind of a local system installation.

01:45:11.000 --> 01:45:16.000
So just go ahead and store all of those data there itself. After converting into the embeddings.

01:45:16.000 --> 01:45:33.000
Yeah? Uh, nowadays, even MongoDB have started giving you a vector… so nowadays, almost every vendor, right, every database provider are, like, a… Giving you a vector database facilities, but these are some of the famous ones. Facebook AI Similarity Search, and then Pinecone, ChromaDB, Waviate, Quadrant.

01:45:33.000 --> 01:45:38.000
Right? And, uh, that is the reason I have mentioned those vector databases inside your.

01:45:38.000 --> 01:45:44.000
Syllabus. 9?

01:45:44.000 --> 01:45:51.000
How to use a Google collab, please give one small class. I mean, like, do we need a class to use a Google Collab?

01:45:51.000 --> 01:45:57.000
I'm not expecting this, guys, like, uh, I don't think that I'm teaching someone from a class 11, 12, or maybe 8 class.

01:45:57.000 --> 01:46:07.000
Student. See, we, like, Google Collab, right? Go to Google. You are able to ask a question, but you are not able to ask the same thing to a chat GPT, right?

01:46:07.000 --> 01:46:09.000
You're not able to ask the same question to a Yuri.

01:46:09.000 --> 01:46:18.000
Simple, Google C-O-L-A-B. I mean, like, there is no… and if you think that your mentor should give this kind of answer to you.

01:46:18.000 --> 01:46:22.000
Believe me, you are not moving anywhere. Not even after one year.

01:46:22.000 --> 01:46:30.000
So, simple, search Google Collab, click, now use it. I mean, like, is this a rocket science? Do I need to take a class on that?

01:46:30.000 --> 01:46:42.000
I mean, like, that is your expectation from me? Like, now Google Collab is up, write a code now. Start writing a code.

01:46:42.000 --> 01:46:52.000
You don't have to go into YouTube or somewhere and then, like, you know, ask a question or, uh, follow, because for everything, you don't need a class, right? At least for using Google Cloud, who needs a class?

01:46:52.000 --> 01:46:57.000
For using a work lab.

01:46:57.000 --> 01:47:09.000
Based on today's discussion, what are the topics like transformer we need to refer to improve our understanding? See, if you have to do a deep… if you have to get into a deep understanding, literally a very, very deep understanding of a transformer.

01:47:09.000 --> 01:47:15.000
Then… here. Let me show it to you.

01:47:15.000 --> 01:47:20.000
And again, I'm recording one more video, so maybe you can wait for, like, a couple of days.

01:47:20.000 --> 01:47:25.000
Uh, for my news, uh, series of the Transformer, and if you would like to, like, a note now.

01:47:25.000 --> 01:47:39.000
So I have already explained that. Uh… Generative AI with NLP.

01:47:39.000 --> 01:47:46.000
Yeah, this one, an agentic AI fine-tuning. So, here, if you'll go, this was my previous, like, live classes.

01:47:46.000 --> 01:47:53.000
Through AI, the SQL power, memory… LM application, vector embedding…

01:47:53.000 --> 01:48:02.000
Yeah, so GPT practical format, before this. Transformer. Transformer and GRU. So, this is the place where I have taken the full-length lecture.

01:48:02.000 --> 01:48:06.000
Of, I think, 2 hours, and I have explained each and everything.

01:48:06.000 --> 01:48:11.000
In terms of, like, a theory, and then right after it, practical, also.

01:48:11.000 --> 01:48:16.000
How to build a GPT from the scratch. It means how to train your own model.

01:48:16.000 --> 01:48:25.000
Without using any previous model. That's also I have discussed.

01:48:25.000 --> 01:48:37.000
Here's a does Uri API allow a tool access? I wanted to enable the tool functionality to build, uh, identic AI… Like, how open it right? Yeah, it provides, actually, a tool access, by the way. And we have already released the SDK for that.

01:48:37.000 --> 01:48:45.000
Do we need to learn a prompt writing? Nothing. Just come to the class, and we'll be taken care of everything.

01:48:45.000 --> 01:49:07.000
Okay, so this is it, guys. So, this is it from my today's class, by the way. Tomorrow, I think I have already set the agenda. So, based on the same fundamental that I have discussed today. So tomorrow, I'm going to show you an example with respect to a Facebook AI Similarity Search, a pinecone Babyt, because those implementations are not at all tough. It's a one single line of code.

01:49:07.000 --> 01:49:14.000
And you will be able to store any kind of a data, right? And then eventually, you will be able to do any kind of queries as well.

01:49:14.000 --> 01:49:23.000
If you are able to understand everything from today's class, right, then it will be very, very, very, very easy. If you're not able to understand, so please go through a lecture once again.

01:49:23.000 --> 01:49:30.000
And, uh, yeah, come to the tomorrow's class. So that we'll be able to cover our vector data basis.

01:49:30.000 --> 01:49:36.000
Like I said, so some of the installation we are going to do in our local system, and some of those vector databases we will end up using from a.

01:49:36.000 --> 01:49:49.000
Like a cloud provider, basically. And it's free. So, they are giving you, like, free access, so no need to, like, you know, attach a card. Just do a login, and I'll show it to you. I'll show it to you, that particular part.

01:49:49.000 --> 01:49:54.000
So that our vector databases will be clear, and then going forward, in every classes, we will end up using.

01:49:54.000 --> 01:50:06.000
A vector database for most of the application. So that's the class for tomorrow, that's an agenda, that's a topic for tomorrow. I have already made an announcement last week itself, so I am on leave for next week.

01:50:06.000 --> 01:50:11.000
23rd and 24th, so there is no class. Uh, last week I have made an announcement.

01:50:11.000 --> 01:50:20.000
Today, I'm making announcement once again. Tomorrow, once again, I'll make an announcement, and even I'll ping you in a… even I have pinged in a group last week, but I'll ping you once again, right?

01:50:20.000 --> 01:50:27.000
So, it's my planned leave, and I'm telling you beforehand. Now, 14 days before, I have already told you, again, I'm giving you a reminder that I'm on leave.

01:50:27.000 --> 01:50:31.000
For next week, so if you are planning something so you can just go ahead.

01:50:31.000 --> 01:50:42.000
You will not miss the class. Yeah? Assignment, there is no assignment for today. I'm just waiting for all of you to complete a challenge which I have given you last week itself.

01:50:42.000 --> 01:50:47.000
That challenges are already available inside your dashboard. I have also pinned you those challenges inside your group.

01:50:47.000 --> 01:50:59.000
So, first, complete that one. Tomorrow, I'll give you the challenge once again. Once, I'll end up discussing VectorDB. So, I have a lot of challenges. A small, small mini project, you can say. So, I'm going to give you those mini projects as challenges.

01:50:59.000 --> 01:51:09.000
Uh, and, uh, yeah. I enjoy the two days class, thank you. Okay, so hope all of you are able to understand. Class was not boring, all of you are able to enjoy and learn.

01:51:09.000 --> 01:51:20.000
Yes, everyone? Yeah? Now, one more thing I would like to highlight over here, that, uh… I think we all are aware about Avani, so try to use Avani at least once.

01:51:20.000 --> 01:51:29.000
Try to give a mock interview. So, I was talking to many students in a one-to-one mode, and what I was able to find out, uh, that.

01:51:29.000 --> 01:51:37.000
That people are even feeling sigh. They are, like, feeling hesitated to even talk to a machine.

01:51:37.000 --> 01:51:45.000
Believe me, that's my, like, experience with most of you. Like, not most of you, so I had a, like, I keep on discussing a lot, like, things.

01:51:45.000 --> 01:51:58.000
With all of you, right? Maybe in a group and maybe as an individual. And this is what I was able to figure out. So try to use Avani, at least try to give one two-round of a mock interview.

01:51:58.000 --> 01:52:02.000
So that you will be able to understand that, uh. What you should do and what you should not.

01:52:02.000 --> 01:52:15.000
Right? So, that is… you will be able to get a real-time feeling and a real-time feedback, by the way, and believe me, Avani is giving you much better feedback as compared to any human being who is going to take your interview.

01:52:15.000 --> 01:52:35.000
Right? It is even able to ask you our follow-up questions. So not just, like, a direct question, no. Even follow-up question, it is able to ask you. Even it is… Asking you to do a coding, right? So, and some of my students has given me a feedback, and even there, they have cracked the interview last week itself, right? So they said that, okay, it's literally amazing.

01:52:35.000 --> 01:52:40.000
Right? I felt more confident. So, after, like, going through, uh, Avni.

01:52:40.000 --> 01:52:49.000
That's part number one. Part number two, right? So, obviously, you have to create a resume. So, Resume AI is already available. You can go ahead and you can try to use it.

01:52:49.000 --> 01:53:00.000
Apart from that, so I'm going to start, once I will be back from my leave, which is, like, 2 days of Saturday-Sunday leave I'm taking, uh, so after that, so from 25th of August.

01:53:00.000 --> 01:53:13.000
We are going to start this interview batch, uh… So I believe most of you have a Euron+, and with Euron+, so you don't have to go and buy anything. It's completely fine. So, try to come to this class.

01:53:13.000 --> 01:53:17.000
Even though if you don't have an understanding about the whole system, or whole generative AI, it's completely fine.

01:53:17.000 --> 01:53:26.000
It will at least try to, like, give you some sort of a, you know, a memory, so where you will be able to, like, retain maybe 10%, but that's okay.

01:53:26.000 --> 01:53:32.000
But try to join this class if you have a time. If you don't have a time, so maybe you can go through a recording, but yeah.

01:53:32.000 --> 01:53:35.000
Try to join this class as much as you can. Here.

01:53:35.000 --> 01:53:46.000
Expectation is very much clear. I'm not going to discuss anything from the scratch. The way I'm discussing in this class, because agenda for this interview batch is different. Agenda for this batch is completely different, right?

01:53:46.000 --> 01:53:50.000
So in my regular batches, I do discuss everything from a scratch.

01:53:50.000 --> 01:53:59.000
Our fundamentals, but in case of interview, it will just go like an interview. So, I'll be talking about a storytelling, I'll be talking about our resumes, I will be talking about a back-to-back.

01:53:59.000 --> 01:54:05.000
Question and answer, question and answer, question and answer, question, answer. On a variety of the topic. And believe me.

01:54:05.000 --> 01:54:13.000
Uh, even last time I have taken a generative AI interview bootcamp, I think 3 or 4 months back, uh, interview bootcamp, and many people was able to make a transition since then.

01:54:13.000 --> 01:54:17.000
And even from this batch, same thing is going to happen.

01:54:17.000 --> 01:54:24.000
Yep, same thing is going to happen. Apart from that, so there is another live batches, which we have announced so far.

01:54:24.000 --> 01:54:33.000
And that is, uh, Python with DSA. Now, this batch, I'm not taking this batch. So, uh, there is a very good mentor called as Ramindra, Ram.

01:54:33.000 --> 01:54:38.000
And, uh, he's from an IT background, he has cracked a Google.

01:54:38.000 --> 01:54:52.000
He is going to join Google in a month of September. By the way, as of now, he's working as a lead in a wake fit. I believe we all know the company, WakeFit, basically. So, he's working as a lead over there, and he's joining Google next month.

01:54:52.000 --> 01:54:56.000
Uh, before 14th, and then he's, like, starting a batch with us.

01:54:56.000 --> 01:55:05.000
So if you would like to learn something from a scratch, something from a basic, because he's going to, like, take everything, like a Python and DSA.

01:55:05.000 --> 01:55:16.000
From a very, very, very, very scratch. So, you can join this particular batch as well. So, with your own… that's the advantage of Euron+, that you don't have to go and, you know, uh, buy things again and again and again.

01:55:16.000 --> 01:55:23.000
But yeah, try to join as many classes as possible, and there is no overlap. There is no overlap between this class, at least, and that class.

01:55:23.000 --> 01:55:31.000
So, it's up to you. That's our announcement, guys. So, if anyone is interested, you can let them know, or maybe if you are interested, you can try to join.

01:55:31.000 --> 01:55:41.000
Uh, I'll simply recommend one thing to our students, or maybe to a professional, that try to spend as much time as possible on your Saturday to Sunday.

01:55:41.000 --> 01:55:48.000
Uh, with learning. Because, believe me, that is going to give you a lot of benefit in the next 6 months of time.

01:55:48.000 --> 01:55:53.000
If you're not doing it, then it's okay, it's up to you.

01:55:53.000 --> 01:56:05.000
Yeah. Okay, so I'm done with today's class, guys. I'll try to start a doubt clearing, so now those who's not having any kind of a doubt, they can drop… Yeah? They can drop from this class.

01:56:05.000 --> 01:56:11.000
Those who's having a doubts, uh, so I think we have a raise hand option. We all know the drill.

01:56:11.000 --> 01:56:17.000
Yeah, let me lower all the hand first. Okay, so I've lowered all the hands, now raise the hand.

01:56:17.000 --> 01:56:21.000
And, uh, yeah, so those who would like to share their screen.

01:56:21.000 --> 01:56:28.000
Even you can try to ping me that I would like to share my screen, so I will make you as a panelist.

01:56:28.000 --> 01:56:32.000
And those who would like to just ask a doubt, then ask a doubt.

01:56:32.000 --> 01:56:39.000
Any plan to upload a video in a master series? I'm doing it, that's running slow, uh, because it's, like, consuming a lot of my time.

01:56:39.000 --> 01:56:44.000
And, uh, I have a limited bandwidth. So I'm doing it in all the batches, almost.

01:56:44.000 --> 01:56:47.000
Even last week, I have uploaded a couple of, like, a big data lectures.

01:56:47.000 --> 01:56:59.000
Spark architecture, Kafka architecture, and uh… this one, a Cloudera setup in a local machine.

01:56:59.000 --> 01:57:07.000
Dsa is required for generative AI. See, irrespective of Geneti BI, data science, machine learning, if you are targeting a product-based company.

01:57:07.000 --> 01:57:12.000
It's required. If you are not targeting a product-based company. It's not required.

01:57:12.000 --> 01:57:17.000
So, DSA is required for all kind of a job, even if you are going to join as a.

01:57:17.000 --> 01:57:22.000
Front-end developer, back-end developers, cybersecurity experts. In operator-based companies.

01:57:22.000 --> 01:57:26.000
Their couple of rounds will go… you will have to go through a DSA.

01:57:26.000 --> 01:57:33.000
As simple as that. Doesn't matter which file you are targeting.

01:57:33.000 --> 01:57:44.000
How we can join a live class of generative AI interview bootcamp. The way you have joined this class, so over there, you will be able to find the link, and then click on the link at that particular time, timing is already mentioned, days are already mentioned.

01:57:44.000 --> 01:57:48.000
As simple as that. For 10 plus years of experience, DSA is required.

01:57:48.000 --> 01:57:57.000
Product-based companies, even if you have a 15-20 year of experience, they are asking a DSA. Believe me, they are asking, they are taking one lot of interview. If you are targeting those premium.

01:57:57.000 --> 01:58:04.000
A super, super premium product-based companies, because they consider DSA as a filtration criteria.

01:58:04.000 --> 01:58:13.000
Very simple. So, that depends upon your target, doesn't matter if you have 10-year experience, 15-year experience, or maybe you are having a 20-year experience, or you are a fresher.

01:58:13.000 --> 01:58:25.000
Right? Company-wise. But SQL also asked. Sql, yeah, obviously, if you're applying for a data science, big data, and all those things, SQL will be mandatory, right, by the way.

01:58:25.000 --> 01:58:33.000
I have my SQL lecture, so those who would like to go through a SQL lecture, go through it. I have recorded it in a very, like, neat and clean way.

01:58:33.000 --> 01:58:43.000
Okay, so… do provide a little bit without paying all the courses, the bootcamp additionally? Yeah, basically, that is the meaning of a, uh, do pro your own user.

01:58:43.000 --> 01:58:49.000
Uh, yes, plus, basically. Plus, we have plus. So we use plus words.

01:58:49.000 --> 01:58:55.000
So yes, this is what a meaning of plus is.

01:58:55.000 --> 01:59:01.000
7 one-hour SQL video is there. Yeah, so I do teach everything in a 7 to 10.

01:59:01.000 --> 01:59:07.000
Um, I think, I think there are, like, more than that. I believe 10-hour.

01:59:07.000 --> 01:59:13.000
Of videos I have uploaded just for SQL. So, you can go through it in detail.

01:59:13.000 --> 01:59:24.000
Okay, fine guys, starting a question answering now. And, uh, Mukesh is saying that he would like to share the screen. Fine, Mukesh, go ahead, share your screen. I'm promoting as a panelist. Anyone else, guys, who would like to share their screen?

01:59:24.000 --> 01:59:35.000
Anyone else? Computer visual lecture, yeah, so many lectures are actually pending, so I'm just trying to complete it. Hopefully, I will do it. But yeah, we'll do it, that is for sure.

01:59:35.000 --> 01:59:42.000
It is taking time, so that's a different context.

01:59:42.000 --> 01:59:47.000
Can you provide a PySpark interview question? I think I already have that, uh, somewhere.

01:59:47.000 --> 01:59:51.000
Ping me personally will give it to you. So, I have that Pi Spark quotient with me.

01:59:51.000 --> 02:00:02.000
Okay, what do you experience, what is the scope of this course? See, doesn't matter whether you are coming from a .NET or some other development, if you are getting into a Gen AI, obviously there is a chance there is a possibility, and there is a market.

02:00:02.000 --> 02:00:12.000
So, you are welcome. Okay, Mukesh, please share the screen. I think you have that access, and in between, so let's take a doubt from Krishna first.

02:00:12.000 --> 02:00:20.000
Then, from Avinas, then from Pushpa, then from Karthik, then from Nares, yeah. So, one by one.

02:00:20.000 --> 02:00:26.000
Amongus, first of all, let me resolve your doubt, yeah. You're sharing their screen.

02:00:26.000 --> 02:00:32.000
Because I think you are on mute.

02:00:32.000 --> 02:00:33.000
Yeah, you are, Devil case, yes.

02:00:33.000 --> 02:00:37.000
Yeah, I'm irritable, sir? Yeah. So, I try to, um…

02:00:37.000 --> 02:00:51.000
Transformer to install, but I'm getting, uh, some error saying that no matching distributions found for transform.

02:00:51.000 --> 02:00:52.000
Yeah.

02:00:52.000 --> 02:00:59.000
Oh, new release. What is your Python? Your Python version is 3.13. Try to use Python 3.10. With 3.13, it's unstable. It's a latest release of Python, right? So… Most of these, like, transformers or TensorFlow or PyTorch installation will not work and give you an issue. So, better to go ahead with 3.10.

02:00:59.000 --> 02:01:00.000
Okay, so I need to reinstall, or I need to go and install it.

02:01:00.000 --> 02:01:11.000
Always. No, just create a… create a new… create a new environment. Just simple create a new environment. Do you know how to create a new environment?

02:01:11.000 --> 02:01:12.000
Okay, let me… give me the screen access, I'll do it.

02:01:12.000 --> 02:01:15.000
Uh, no, sir. Sure.

02:01:15.000 --> 02:01:30.000
Yeah, so I… screen access… Hmm, okay, I can access your system now.

02:01:30.000 --> 02:01:38.000
Kunda is not recognized? Okay, fine. So, I'm going to use a Python then.

02:01:38.000 --> 02:01:46.000
But it's on environments of Python 3.10, let's suppose. M-v-e-n.

02:01:46.000 --> 02:01:55.000
V, and environment name is… Uh, test, let's suppose.

02:01:55.000 --> 02:02:24.000
This is… not recognize…

02:02:24.000 --> 02:02:33.000
Command is correct, why? Uh, Python VNB, power cell…

02:02:33.000 --> 02:02:59.000
Hmm, that's correct.

02:02:59.000 --> 02:03:04.000
3.10. M-v-e-n-v.

02:03:04.000 --> 02:03:10.000
Best. Ident 3.10 is not recognized as a…

02:03:10.000 --> 02:03:26.000
Okay.

02:03:26.000 --> 02:03:35.000
Python hyphen hyphen Wilson. It's a 3.13, so fine.

02:03:35.000 --> 02:03:42.000
Let's do one thing, let me download Python 3.10, so hyphen 3.10.

02:03:42.000 --> 02:03:48.000
Iphone M? V-e-n-v.

02:03:48.000 --> 02:03:55.000
Best…

02:03:55.000 --> 02:04:25.000
3.10…

02:05:06.000 --> 02:05:20.000
Yeah, Conda installation, if someone has done right, so it will be always better, and it will be… generally, I use a Conda, so even in my recording, you have seen that I'm using a conda, but uh… Conda is not installed. I have already checked in the very first place, so… I have to find out the alternative.

02:05:20.000 --> 02:05:22.000
Okay.

02:05:22.000 --> 02:05:29.000
But yeah, if you're going to install Conda, right, so… it will always be easy. So, condyle…

02:05:29.000 --> 02:05:36.000
Create hyphen n environment name done. Uh, no…

02:05:36.000 --> 02:05:48.000
Install now. Even we have a UV package manager, so we can install it with that. But yeah, let me try this approach.

02:05:48.000 --> 02:05:51.000
So, true version will be, uh, working, right? Like, no need to uninstall one.

02:05:51.000 --> 02:05:56.000
You can even… yeah, you can switch, uh, between, like, these two versions.

02:05:56.000 --> 02:06:03.000
Okay.

02:06:03.000 --> 02:06:04.000
Okay.

02:06:04.000 --> 02:06:10.000
If you have a conda, then it will be, like, uh… better package manager. So, you can try to keep even hundreds of versions, that's cool. And, like, even with this, you can keep hundreds of, like, versions.

02:06:10.000 --> 02:06:19.000
But in terms of managing things, Conda will be easier.

02:06:19.000 --> 02:06:26.000
In between, so let's go ahead with some question. Uh, yeah, Krishna, please go ahead with the question.

02:06:26.000 --> 02:06:27.000
Yeah, good morning. Mm-hmm.

02:06:27.000 --> 02:06:32.000
Yeah, good morning, sir. So, uh, on a higher level, can you just explain how a centers.

02:06:32.000 --> 02:06:35.000
Is getting converted into a vector, just a high level. Sentence is getting converted into the vector.

02:06:35.000 --> 02:06:45.000
How the… how the… Sentence is getting converted into a vector, okay? So…

02:06:45.000 --> 02:07:15.000
Fine. Let me share my screen.

02:07:50.000 --> 02:08:01.000
Okay, I'm sharing my screen to answer Krishna's question, so… just…

02:08:01.000 --> 02:08:06.000
A minute. I believe my screen is visible, right, Krishna?

02:08:06.000 --> 02:08:08.000
Yes, it is now, yeah.

02:08:08.000 --> 02:08:13.000
Yeah, so basically I'm showing you one interface, so which has been used for explaining the transformer, right? So, where.

02:08:13.000 --> 02:08:21.000
We are going to give our data, as you can see, from one end, right? And then internally, so we are having our vectors.

02:08:21.000 --> 02:08:33.000
So, as you can see over here, so we have our vectors of, like, a certain dimension. So vectors simply means the vector that we have discussed today, right? Vector discussed today. But yeah, there is a concept called as KQV concept inside a transformer, and if you would like to know more about it, so maybe.

02:08:33.000 --> 02:08:40.000
The lecture which I have referred to you, my own lecture. So, over there I have, like, done that. But your question was to.

02:08:40.000 --> 02:08:51.000
Like, uh, how it is trying to give me the embeddings at the end of the day, and how it is trying to learn. So, inside this entire architecture, as you can see, there are multiple layers. So, there are multiple layers, means.

02:08:51.000 --> 02:09:00.000
There are 12 layers as of this architecture, right? And if I'll talk about a GPT initial one, so there was 96 layers in total.

02:09:00.000 --> 02:09:06.000
So, here, my data will go through all of these layers in a backup propagation. It is trying to learn each and everything.

02:09:06.000 --> 02:09:16.000
And then, somewhere on this output part, somewhere on this output part, so it is going to generate some of the data.

02:09:16.000 --> 02:09:22.000
So maybe it is trying to generate a visualize with a percentage accuracy of this one, then this one, then this one.

02:09:22.000 --> 02:09:33.000
No. So, before that, right? So, right before generating this particular data set, so when it will go through, when your input, so let's suppose I have written my name is.

02:09:33.000 --> 02:09:39.000
Sudhonshu. So, when this entire input will go through this entire transformer architecture.

02:09:39.000 --> 02:09:46.000
So, it will try to use a weight or attention layer that we have over here. Technically, it's a neural network.

02:09:46.000 --> 02:09:51.000
And then, in between, it is just going to give me the numerical.

02:09:51.000 --> 02:09:57.000
Computation, or based on the KQV, so key value and this, uh, query vector, so it is going to give me the numerical value.

02:09:57.000 --> 02:10:03.000
And which is going to happen in between. This is how I used to get our embeddings.

02:10:03.000 --> 02:10:04.000
If it makes sense. And, uh, one more thing is, uh.

02:10:04.000 --> 02:10:06.000
Basically.

02:10:06.000 --> 02:10:11.000
I mean, you mentioned that we have few models with the different sizes, right?

02:10:11.000 --> 02:10:12.000
So, how… how can we say that the higher dimension model is having higher accuracy?

02:10:12.000 --> 02:10:16.000
Hmm, hmm.

02:10:16.000 --> 02:10:24.000
No, we can't say in that way. So, I would rather prefer to say that that higher context window models will be having a highest accuracy.

02:10:24.000 --> 02:10:30.000
Means, so the model, if model is able to understand more lengthy sentences.

02:10:30.000 --> 02:10:34.000
And it will be able to generate more output on terms of only sentences, I'll say it is better.

02:10:34.000 --> 02:10:43.000
So, for example, so I'll always say that, that a model context window size 128K will always be better as compared to 4K window.

02:10:43.000 --> 02:10:49.000
Yeah? So, I'll rather prefer to use that particular bottle. And that is the reason, so when I was trying to show you even a hugging face, right?

02:10:49.000 --> 02:11:03.000
So when I was trying to show you a hugging face, so here they have given you the sequence lens. That sequence length 32K, sequence lens is 32K, sequence line is 32K. So, almost, these models are, like, same in terms of generating the output of the sequence.

02:11:03.000 --> 02:11:08.000
Although layers was different. Layers was different, so number of, like, parameters has changed.

02:11:08.000 --> 02:11:16.000
And then in between, so they have changed the… dimension of the neurons, which was generating the embedding for the equivalent data set.

02:11:16.000 --> 02:11:18.000
But, uh, I'll rather go ahead with this one.

02:11:18.000 --> 02:11:25.000
Yeah, but we can say that the 8B version is more accurate or more preferable.

02:11:25.000 --> 02:11:27.000
Than the .6B, right?

02:11:27.000 --> 02:11:37.000
Because number of learnable parameters is more. But again, so that is never guaranteed… going to give you a guarantee that it is going to be the best model.

02:11:37.000 --> 02:11:42.000
Now, these are the global models which people have released, and they have made sure that.

02:11:42.000 --> 02:11:49.000
When I'm trying to train this model, so I have given enough epoch, I have basically given, uh, enough.

02:11:49.000 --> 02:12:01.000
Data, or you can say a context. To this particular model. But let's suppose if user like me and you are trying to train, and I have created a model of maybe an ABM parameter, and then 4BM parameter.

02:12:01.000 --> 02:12:05.000
There is a possibility that your 4 billion will outperform my 8 billion.

02:12:05.000 --> 02:12:06.000
Yeah.

02:12:06.000 --> 02:12:17.000
The reason is very simple again. So let's suppose. I am… I'm trying to run through the 8 billion, but maybe my data is not sufficient. Maybe I have not gone through the epochs, maybe I have not done the fine-tuning of the parameters.

02:12:17.000 --> 02:12:30.000
Right? In case of 8 billion. And there is a possibility that I have done the same thing in my 4 billion. So I have done a better fine-tuning means better parameter optimization I have done over here, better data set I have chosen over here, better formatted and structure and relevant data set I have chosen over here.

02:12:30.000 --> 02:12:38.000
So, there is a possibility. So, in general, I can't say that if your model parameter is more, your model is great. No.

02:12:38.000 --> 02:12:39.000
That is… that is not an argument, by the way. Because model parameter always depends upon my architecture, so there is a possibility.

02:12:39.000 --> 02:12:44.000
Okay.

02:12:44.000 --> 02:12:52.000
That, uh, I have created this neural network, right? And basically, you have created this neural network.

02:12:52.000 --> 02:13:01.000
And maybe your neural network size is bigger, right? Size is bigger. So… in an ideal case, this neural network will be having more parameter as compared to this neural network.

02:13:01.000 --> 02:13:02.000
Right? So, this is the… this is the meaning of this parameter. This is the meaning of this 4 billion, 8 billion.

02:13:02.000 --> 02:13:08.000
Here.

02:13:08.000 --> 02:13:12.000
So, there is a possibility that your model is having bigger in size, means more number of parameter.

02:13:12.000 --> 02:13:21.000
Means more number of weights to update. And my model is having a list. That doesn't mean that, that my model will be worse than your model will be best.

02:13:21.000 --> 02:13:31.000
So, number of parameters is not a criteria. To define a model is good or model is bad. There are different criteria, because number of parameters is what?

02:13:31.000 --> 02:13:33.000
Just these connections, right? Just this weights.

02:13:33.000 --> 02:13:39.000
Yeah, yeah. So, so is it the same with the GPT-20 billion and 120 billion also?

02:13:39.000 --> 02:13:46.000
Uh, GPT-20 billion, like I said, so when you are training a model, so 120 billion means.

02:13:46.000 --> 02:13:47.000
This context, the context which I'm talking about, right? Let's suppose this is my 20 billion, and this is my 120 billion.

02:13:47.000 --> 02:13:52.000
Yeah. Cool, cool.

02:13:52.000 --> 02:14:03.000
Means more number of weights, more number of parameter. And you will be able to justify that your 120 billion parameter model is working well, right? Only in one situation. So, if.

02:14:03.000 --> 02:14:09.000
You have trained your model in a well manner. If you have given enough amount of the data, if you have given enough.

02:14:09.000 --> 02:14:16.000
Epoch. Because if you are trying to give enough or a huge amount of the data, and if you're running for a multiple or many epochs.

02:14:16.000 --> 02:14:26.000
Maybe more than this particular model, then you are investing a lot on a GPU. So if you have done that, then only you can say that my 120 is better than 20 billion. Otherwise.

02:14:26.000 --> 02:14:30.000
I will not be able to say that same thing with a surety.

02:14:30.000 --> 02:14:32.000
Okay, so one last question. So, suppose, let's say, I have 10 documents.

02:14:32.000 --> 02:14:37.000
Hmm. Hmm.

02:14:37.000 --> 02:14:38.000
Hmm.

02:14:38.000 --> 02:14:46.000
I said, okay, one document and 10 lines. Suppose if I'm using the text embedding model, the basic .6, which is having 1024 embedding size.

02:14:46.000 --> 02:14:47.000
Hmm. Hmm.

02:14:47.000 --> 02:14:57.000
And the 8 billion, which is having 4096. It… does that mean that my one document, one sentence, is getting mapped to 1024.

02:14:57.000 --> 02:15:00.000
Parameters, and while. Finding the similarity.

02:15:00.000 --> 02:15:10.000
It is, you know, finding… I mean. Comparing…

02:15:10.000 --> 02:15:11.000
Correct. Yes.

02:15:11.000 --> 02:15:15.000
No, again, got your question. So you're saying that you are trying to use, basically, this model to do a, like, a conversion at the time of query, maybe you will try to use some different model, and then… You will try to. So, basically, you are trying to do an apple-to-orange comparison, which is not preferable at all.

02:15:15.000 --> 02:15:22.000
Yeah. What if I'm doing in the same model? I mean, suppose, let's say, 0.6, 0.6 and 8B, 8B I'm doing.

02:15:22.000 --> 02:15:27.000
So, hold on. Hmm.

02:15:27.000 --> 02:15:28.000
So…

02:15:28.000 --> 02:15:44.000
So, points is, points is 8B, 8B you are doing, then it's a justified one, right? Because, see, every model has learned. Learned miss what? So every model is trying to, like, establish a relationship between what will come before, what will come after. And accordingly to generating your embeddings, right. Means, it is understanding your sentence, your text, by the way, right?

02:15:44.000 --> 02:15:50.000
So if you are using 6B and 6B comparison, and if you are using A to B and AB comparison, that is completely fine.

02:15:50.000 --> 02:15:56.000
Okay, makes sense. Thank you, that's it.

02:15:56.000 --> 02:15:57.000
Yeah, they…

02:15:57.000 --> 02:16:01.000
But otherwise, like, you can't go ahead and use, like, a 6B and 8B. You can't do the comparison, because that will be, like, apple-to-orange comparison, which is… Which you will not be able to justify the output.

02:16:01.000 --> 02:16:07.000
Understood. So, by the way, can you share that transformer visualizer?

02:16:07.000 --> 02:16:08.000
The URL. Yeah, yeah, yeah.

02:16:08.000 --> 02:16:15.000
This one, the visualizer which I have shown you. Technically, this visualizer will not be able to give you a clear understanding.

02:16:15.000 --> 02:16:16.000
Okay, okay, no problem.

02:16:16.000 --> 02:16:30.000
Uh, let me… so yeah, this visualization is not going to give you a clear understanding. If you are new to a transformer, so it will be able to… I'm going to share this link, don't worry.

02:16:30.000 --> 02:16:35.000
Yeah, I'm going through it, yeah.

02:16:35.000 --> 02:16:36.000
V, right?

02:16:36.000 --> 02:16:41.000
Yeah, so link is available, but just go through my lecture once, because… There is… there is something called as K, there is something called as Q, there is something called as V, there is something called as multi-headed attention, there is something called as positional encoding, when we try to…

02:16:41.000 --> 02:16:47.000
Insert our data inside this one. So, unless and until you are not going to understand all of these things in a sequence, right.

02:16:47.000 --> 02:16:56.000
And unless Nati, you are not clear with our multi-header attention, which has been shown over here, right? That it is trying to form the multi-headed attention, and then MLP layer on top of this.

02:16:56.000 --> 02:17:01.000
It will be very difficult. Once you will be able to understand that part from the lecture, and then if you'll come here.

02:17:01.000 --> 02:17:03.000
Believe me, in a one-shot, you will be able to understand.

02:17:03.000 --> 02:17:05.000
Okay, next. Thanks, that's it.

02:17:05.000 --> 02:17:10.000
Yeah. So, okay, so I think, uh, Mukesh was sharing his screen. Let me stop sharing mine.

02:17:10.000 --> 02:17:12.000
Yes, I'm sitting.

02:17:12.000 --> 02:17:19.000
Hmm. Yeah, Muges, please share your screen.

02:17:19.000 --> 02:17:22.000
Yeah. So, salary to repair or modify?

02:17:22.000 --> 02:17:32.000
Okay, so it's done. Do anything, like, add or remove individual features, yeah…

02:17:32.000 --> 02:17:34.000
Anything, anything is fine.

02:17:34.000 --> 02:17:43.000
Okay. Yeah.

02:17:43.000 --> 02:17:59.000
Yeah, so I think it's installed now. You can try to… Open up your…

02:17:59.000 --> 02:18:19.000
2.13 by 10… 1.0…

02:18:19.000 --> 02:18:27.000
3.10 hyphen M. V-e-n-v, test.

02:18:27.000 --> 02:18:30.000
Hmm, now it's able to identify you. 31-0.

02:18:30.000 --> 02:18:36.000
Okay.

02:18:36.000 --> 02:18:37.000
So, virtual environment has been created. Now, if I have to, like, activate this virtual environment.

02:18:37.000 --> 02:18:41.000
Okay.

02:18:41.000 --> 02:18:47.000
So there must be a folder called as test. So, I can just try to…

02:18:47.000 --> 02:18:54.000
Activate this virtual environment.

02:18:54.000 --> 02:19:03.000
So, test… And then, slash… Inside this, we have our scripts.

02:19:03.000 --> 02:19:11.000
So inside that, we have a activate. So, S-C-R-I… Scripts slash… I do it… okay.

02:19:11.000 --> 02:19:19.000
So this has been activated. Now, so you have to, like, you have already created this Python 3.10.

02:19:19.000 --> 02:19:25.000
Test environment, so in your Jupyter notebook. I'm not able to see the above part.

02:19:25.000 --> 02:19:27.000
Okay, let me remove this one. One second.

02:19:27.000 --> 02:19:33.000
Yeah, remove.

02:19:33.000 --> 02:19:35.000
Yeah, and write it down.

02:19:35.000 --> 02:19:44.000
Yeah, so Python, I can select a different kernel. Environment, so test, this is visible, right? Python 3.10.

02:19:44.000 --> 02:19:50.000
So now this has been attached. And, uh, let's suppose if I'm installing…

02:19:50.000 --> 02:20:15.000
Transformer, so…

02:20:15.000 --> 02:20:22.000
Okay, so in between, uh, we can go ahead with the question. Yeah, Ranjit, what is your question, by the way?

02:20:22.000 --> 02:20:23.000
Mm-hmm. Mm-hmm.

02:20:23.000 --> 02:20:30.000
Yeah, I have a couple of questions, eh. One is that, uh, I have an issue with your platform. What happens is that when I try to join.

02:20:30.000 --> 02:20:31.000
It's asking me to register again, so this was very confusing.

02:20:31.000 --> 02:20:35.000
Mm-hmm.

02:20:35.000 --> 02:20:36.000
And, uh…

02:20:36.000 --> 02:20:45.000
No, no, that is a Zoom link, right? So, in a Zoom link, I think it is… Asking you to enter the name and email ID, something like that. Just a basic information it takes. Uh…

02:20:45.000 --> 02:20:52.000
But it has, uh… I'm already there, means why does it ask again every time you want to join it?

02:20:52.000 --> 02:20:58.000
Uh, I think we'll have to change some setting in a Zoom. That is not our platform setting, that is a Zoom setting, so.

02:20:58.000 --> 02:21:02.000
We'll have to, like, change that one.

02:21:02.000 --> 02:21:04.000
Everybody has this issue, or I'm the only one who have?

02:21:04.000 --> 02:21:10.000
Uh, no, no, no, I think everyone, everyone will see the same thing, exact same thing. The window that you are able to see, right?

02:21:10.000 --> 02:21:12.000
Uh, it's the same for every one of us.

02:21:12.000 --> 02:21:18.000
Okay, another question that I have is, uh, API keys. I tried to create today.

02:21:18.000 --> 02:21:19.000
Mm-hmm.

02:21:19.000 --> 02:21:24.000
But it then led me, and it says no API keys, but when I click that button.

02:21:24.000 --> 02:21:25.000
Mm-hmm.

02:21:25.000 --> 02:21:33.000
It doesn't do anything. So, I never had this issue before, because I used to, in my… in your last lecture, I mean, not last lecture, I mean, last, uh.

02:21:33.000 --> 02:21:34.000
Okay.

02:21:34.000 --> 02:21:41.000
I had no problem, never, but now I'm not able to create. So, can you see what's… why is…

02:21:41.000 --> 02:21:47.000
Yeah, sure. So please, please share your screen. I don't, I don't think that this should be the issue, because I'm able to do it, so it's same for you.

02:21:47.000 --> 02:21:48.000
But yeah. Okay, okay.

02:21:48.000 --> 02:21:52.000
Yeah, IASMO had never issued, but now I have. So, how do I share my machine?

02:21:52.000 --> 02:22:04.000
Yeah, so I'm promoting as a panelist, you will be able to share your machine, yeah?

02:22:04.000 --> 02:22:05.000
Yeah.

02:22:05.000 --> 02:22:13.000
Yeah, exactly, join as a panelist, yeah. So, Ranjit… sorry, uh, Mukesh, right? So, Mukesh, transformer, people transformer, and, uh, uh, you are using P13. However, version is fine, you should be considering this, this, this, this, this.

02:22:13.000 --> 02:22:18.000
Okay, so could not find the person that certifies the requirement transformer for worse on this one. Okay, not an issue.

02:22:18.000 --> 02:22:23.000
So you are… so I think sentence transformer was the one, right? Library?

02:22:23.000 --> 02:22:29.000
Uh, yeah, it's already done. This one. Yeah, request also I had. I don't know why it is coming now.

02:22:29.000 --> 02:22:35.000
Request will be… request will be available by default. So, you have to change API key over here.

02:22:35.000 --> 02:22:36.000
So just… just… Okay, so if you can copy and paste there.

02:22:36.000 --> 02:22:41.000
Yeah, okay. So, you already generated.

02:22:41.000 --> 02:22:51.000
Hold on a second…

02:22:51.000 --> 02:22:54.000
Oh, it's… did it? Yeah, like, not copied.

02:22:54.000 --> 02:23:00.000
Just, yeah, copy the key, and let me… I'm doing it, leave your screen. I'm doing it.

02:23:00.000 --> 02:23:02.000
Okay.

02:23:02.000 --> 02:23:15.000
So, here, copy and paste, and then hit. Simple. No module name request. Okay, so in this Jupyter one.

02:23:15.000 --> 02:23:16.000
Yeah, I inst… okay, uh, okay, for this version, I didn't install it, okay.

02:23:16.000 --> 02:23:19.000
It is not able to find out the request, not an issue, so… Yeah, so this is the new kernel that I've selected, right?

02:23:19.000 --> 02:23:21.000
Yeah, correct. And also, NumPy also required, I think.

02:23:21.000 --> 02:23:27.000
Fine, this isn't still… Okay, so we'll do NumPy as well.

02:23:27.000 --> 02:23:35.000
So… N-U-M-B-U-I, execute.

02:23:35.000 --> 02:23:41.000
Just a minute.

02:23:41.000 --> 02:23:48.000
Yeah. So, pip install on time. Hmm.

02:23:48.000 --> 02:23:59.000
And for only picture.

02:23:59.000 --> 02:24:09.000
Numpy is done, and now, so, executed, it'll be done. Yeah, it's working. So, here is your embedding, Mukesh.

02:24:09.000 --> 02:24:19.000
That you are able to get, yeah? Fine. I think, Mukesh, you are on mute.

02:24:19.000 --> 02:24:20.000
Yes, sir, got it, yeah, thank you.

02:24:20.000 --> 02:24:29.000
Got it? Yeah, okay. So, you can unshare the screen. Yeah, Ranjit, I think I can see your screen. So, can you please give me a control of your screen? I'll do it quickly.

02:24:29.000 --> 02:24:54.000
Uh… Let me release the control for Mukase, is…

02:24:54.000 --> 02:24:59.000
Okay, so just click on this Create API key.

02:24:59.000 --> 02:25:01.000
Yeah, so it didn't let me do anything. Oh, name here.

02:25:01.000 --> 02:25:09.000
Yeah, give any name? Yeah, any name, anything test X, Y, Z, whatever, yeah, click on Create.

02:25:09.000 --> 02:25:11.000
Okay. Oh.

02:25:11.000 --> 02:25:16.000
Done. This is your key, copy it, use it.

02:25:16.000 --> 02:25:21.000
I mean, did we used to do, like, this before also?

02:25:21.000 --> 02:25:23.000
Yeah, it was before, it was him. Even till last week, it was the same. Like, I think we have changed our system 2 months back.

02:25:23.000 --> 02:25:28.000
I believe…

02:25:28.000 --> 02:25:29.000
Since last two months, it was, like, we have not changed it, this system at least.

02:25:29.000 --> 02:25:45.000
Uh-huh. Yeah, because I have not used your key in the last two months. Before, I used to do, but it was just without it. Okay.

02:25:45.000 --> 02:25:52.000
Oh, okay.

02:25:52.000 --> 02:25:53.000
Okay.

02:25:53.000 --> 02:25:57.000
Oh. Uh, so before that… before that interface was different. So before that, we were not given an option to create an API key, so by default, when you will log in, there was an API key, and what we have done, so we have done the little bit of modification, just like any international platform does it, right? That you can… so, before that, there was no option to deactivate or delete your API key. Now you have the option to delete and deactivate. Let's suppose by mistake, you have shared the.

02:25:57.000 --> 02:26:00.000
Yes. Mm-hmm. Hmm.

02:26:00.000 --> 02:26:05.000
Right. So now you can go and delete it, see, there is a delete option.

02:26:05.000 --> 02:26:08.000
Okay, okay, it makes sense now. Thank you very much for the explanation.

02:26:08.000 --> 02:26:09.000
No, I have another question. S.

02:26:09.000 --> 02:26:13.000
Yeah.

02:26:13.000 --> 02:26:17.000
And this question is on vision language model. Remember, you said that it reminds you.

02:26:17.000 --> 02:26:19.000
Logan? Uh, yeah, yeah, I said that, and I said that that I have asked my team to, you know, uh.

02:26:19.000 --> 02:26:23.000
And, uh.

02:26:23.000 --> 02:26:32.000
Like, uh, given access of one of the Wizard model, but I think they have not done the implementation so far.

02:26:32.000 --> 02:26:33.000
So, I'll just ask my team once again, let's see when they do it.

02:26:33.000 --> 02:26:35.000
Okay, could you remind them, please?

02:26:35.000 --> 02:26:42.000
Yeah, okay, another question that you have is, uh. What if I have to generate a financial report?

02:26:42.000 --> 02:26:43.000
Hmm.

02:26:43.000 --> 02:26:51.000
Okay, and uh… and I wonder, that financial approach would be persistent. I generate, or you generate, or somebody else.

02:26:51.000 --> 02:26:53.000
Make all the data, your financial data. So, so this LLM, yeah, models, do you think that they will generate.

02:26:53.000 --> 02:26:59.000
Hmm.

02:26:59.000 --> 02:27:09.000
Uh, like, they will be… it will be 100% consistent every time, or there will be some… Well, let me know. And the real value of finance, I mean…

02:27:09.000 --> 02:27:15.000
Uh, I don't think so. If you are giving your real data, and if you are using a best model.

02:27:15.000 --> 02:27:18.000
And if you are keeping your temperature low, right, there is an option called a temperature, right? If you're keeping your temperature low.

02:27:18.000 --> 02:27:22.000
Yeah, the idea of putting up a temperature is zero, so will it be consistent?

02:27:22.000 --> 02:27:32.000
Let's suppose point 1, right? So, in that case, yes, you can say that there will be a very less chance of doing a hallucination.

02:27:32.000 --> 02:27:38.000
Very less chance. I can't… I can't say that, or no one can say that with 100% accuracy, that it won't.

02:27:38.000 --> 02:27:41.000
Right. But I think 99.9% accuracy is more than enough. In terms of numbers, right, if you're feeding a number, believe me, it will be always correct.

02:27:41.000 --> 02:27:46.000
But, uh… It will be always correct. So because in banking or in financial, they cannot take chance of even 0.1… 0.1%, I mean, yeah.

02:27:46.000 --> 02:27:53.000
Yeah, almost.

02:27:53.000 --> 02:28:03.000
That I agree, see. So, it will do a hallucination when you try to give a prompt, and then you try to ask them to generate something.

02:28:03.000 --> 02:28:08.000
Again, it's not like every time in every cases it is going to do a hallucination. Hallucination will happen when you are going for a generation. Fuse or prompt, or maybe geo-sort prompt.

02:28:08.000 --> 02:28:13.000
Mm-hmm. Okay. Okay.

02:28:13.000 --> 02:28:20.000
Right. Where you are trying to, like, ask, and then the system is generating from the scratch. Let's suppose I'm doing RAG.

02:28:20.000 --> 02:28:32.000
Mm-hmm.

02:28:32.000 --> 02:28:33.000
Mm-hmm.

02:28:33.000 --> 02:28:42.000
This is where RNG comes into picture, right? So let's suppose I'm doing RAG, I have my financial document, and what I want, that, okay, so I will use the LLMs, but I will try to use my own vector DB, so where I have stored my original data. So in that situation, it will never hallucinate. The reason is, again, very simple, because before hitting the LLMs, right? So, it goes to a RAG based on your query, it tries to extract the data.

02:28:42.000 --> 02:28:47.000
Right, and that data will go to the LLM just for the reformatting purpose.

02:28:47.000 --> 02:28:57.000
Not for generation purpose, right? So, there is a difference, right? So, reformatting and a generation. So, if you're going ahead with the RAG approach, and then if you're asking your, like, a system to give you an answer.

02:28:57.000 --> 02:28:58.000
It is going to be 100% accurate, because it is not even asking your LLM to generate.

02:28:58.000 --> 02:29:02.000
Hmm.

02:29:02.000 --> 02:29:09.000
There is no chance of hallucination, so you can't even, like, uh… you can keep temperature, anything that is completely fine.

02:29:09.000 --> 02:29:18.000
Mm-hmm, okay, okay. So another question I have is, uh. I suppose, uh, I want.

02:29:18.000 --> 02:29:22.000
Scrap data… Chrome Deck Scanner.

02:29:22.000 --> 02:29:28.000
I tried. I tried similar to Twitter. Twitter, I was able, no issue, okay?

02:29:28.000 --> 02:29:40.000
But when I try with a DAC scanner. It will… it gives up. So, do you have any idea? Can you give me any kind of…

02:29:40.000 --> 02:29:46.000
Some way of handling… Dex scanner, you know, it is a crypto.

02:29:46.000 --> 02:29:47.000
Mm-hmm, hmm, hmm.

02:29:47.000 --> 02:29:55.000
Data. And it moves very, very fast. For example. I want to know, in one hour, last one hour.

02:29:55.000 --> 02:30:03.000
How many, uh, tokens have attained 400%. In 5 minutes. Okay.

02:30:03.000 --> 02:30:09.000
Suppose that's my, uh, question, okay. So, if you have to develop some program.

02:30:09.000 --> 02:30:15.000
How would you dwelt with? Maybe just give me some kind of idea so that I can get the data.

02:30:15.000 --> 02:30:20.000
And I can scrap what I need, the information.

02:30:20.000 --> 02:30:28.000
So, scrapping-wise, we have been using a fire crawl, basically, and uh, yeah, it was working well.

02:30:28.000 --> 02:30:37.000
Apart from that, so even my team was using a couple of more, uh, you know, scrapping APIs nowadays to scrap some of the data.

02:30:37.000 --> 02:30:48.000
But none of those scrappers was, like, 100% perfect, I would say. Like, in every case, there was, like, a lot of cases where none of them was working, and we were looking for the alternative.

02:30:48.000 --> 02:30:54.000
So, scrapping-wise, I don't think that you can rely on any of the scrapper which is available in our market 100%.

02:30:54.000 --> 02:30:59.000
But have you tried, uh, DAC scanner, or you're talking about journal? I mean?

02:30:59.000 --> 02:31:03.000
This scanner for a crypto, I heard about that, but I have never tried that.

02:31:03.000 --> 02:31:04.000
But it's… it's very complex. Dex.

02:31:04.000 --> 02:31:12.000
So, DEX, right? So you're talking about DX? Yeah, I have already tried, but uh… Like, uh, not much, I would say.

02:31:12.000 --> 02:31:13.000
Maybe I'll have to re-explore once again. For this one.

02:31:13.000 --> 02:31:19.000
Uh-huh. Do you think it will be possible for you to explore it? Give me some idea?

02:31:19.000 --> 02:31:27.000
All possible-wise, yeah, maybe I can ask some of my team members to do the exploration, and what is the possibility of fetching a data from here.

02:31:27.000 --> 02:31:28.000
Yeah.

02:31:28.000 --> 02:31:34.000
Because it's, uh, crypto reading, so I think it should be possible. I don't see a challenges over here.

02:31:34.000 --> 02:31:38.000
And I don't think that they have blocked any of their, like, ports.

02:31:38.000 --> 02:31:40.000
Basically, in terms of scrapping the data.

02:31:40.000 --> 02:31:44.000
Yeah, no, because they have the APIs, right? We use the DAX APIs, and that is not an issue.

02:31:44.000 --> 02:31:53.000
Oh… So, if they have an API, then why we need a strapping? Then call the API.

02:31:53.000 --> 02:31:59.000
But somehow, I was not successful. In getting the data.

02:31:59.000 --> 02:32:10.000
I mean, I was able to basically log in, okay? But then after that, uh… Are you using ChatGPT, and I believe I tried, maybe, Eurona, so…

02:32:10.000 --> 02:32:11.000
Hmm.

02:32:11.000 --> 02:32:17.000
But I couldn't get there. I mean, if you can give me some kind of a… Maybe your team can help me a little better.

02:32:17.000 --> 02:32:18.000
Yes. Okay.

02:32:18.000 --> 02:32:24.000
Let me check the API. It's API endpoint. I'm just checking it. So, unable to actually call the API provider URL, okay, fine.

02:32:24.000 --> 02:32:33.000
So… that's next… I'm just checking the API part.

02:32:33.000 --> 02:32:37.000
Because if every endpoint they have released it, then there is no issue at all.

02:32:37.000 --> 02:32:41.000
So, typically, it ranges from a request per minute you can send, okay?

02:32:41.000 --> 02:32:44.000
Hmm, they have official APIs.

02:32:44.000 --> 02:32:47.000
So, what didn't you type now? Okay.

02:32:47.000 --> 02:33:17.000
I'm giving you, I'm giving you that.

02:33:23.000 --> 02:33:25.000
I'm just checking its API endpoint, so… Give me some time.

02:33:25.000 --> 02:33:39.000
Okay. Hmm. Let me get chill.

02:33:39.000 --> 02:34:09.000
Let's see…

02:36:53.000 --> 02:36:59.000
So basically, you were looking for pairs, right, over there, token pairs you are looking for.

02:36:59.000 --> 02:37:06.000
You know, yes, yeah, the payers that are increasing in certain amount of time.

02:37:06.000 --> 02:37:08.000
Okay, so the latest one you were looking for. I'm able to… I'm able to get something by hitting its API.

02:37:08.000 --> 02:37:13.000
Yes, yeah, later.

02:37:13.000 --> 02:37:14.000
Okay. Really?

02:37:14.000 --> 02:37:24.000
Uh, so… So this was the output that you were looking for?

02:37:24.000 --> 02:37:25.000
Uh, how do I see that? What you're trying to show me?

02:37:25.000 --> 02:37:31.000
Dilated one? Okay, is it not visible? Okay, let me share my screen once again.

02:37:31.000 --> 02:37:32.000
So, I'm sharing my screen. I think you can see it on my screen.

02:37:32.000 --> 02:37:40.000
No.

02:37:40.000 --> 02:37:41.000
Okay, maybe you can just stop sharing your screen, then only you can see it.

02:37:41.000 --> 02:37:43.000
I'm not able to see anything yet.

02:37:43.000 --> 02:37:45.000
Okay, stop saying, okay. Yeah, no, I can see.

02:37:45.000 --> 02:38:00.000
Now you can see my screen.

02:38:00.000 --> 02:38:05.000
So, basically, this is a chain, and this is the pair that it is returning. Then again, chain Solana, and then pair. So chain osmosis, and then pair. So, again, chain osmosis.

02:38:05.000 --> 02:38:10.000
Mm-hmm. Okay.

02:38:10.000 --> 02:38:11.000
And then period is time to rep- uh, like, a return.

02:38:11.000 --> 02:38:16.000
Yeah, bucket is not showing what is the… put onto it, and about it.

02:38:16.000 --> 02:38:17.000
Oh, he's, uh… Okay, okay, oh.

02:38:17.000 --> 02:38:23.000
No, that I can pull. So, I mean, like, this is… This is something that I can get it right, so…

02:38:23.000 --> 02:38:24.000
Okay. Oh, okay, okay, okay, now I understand.

02:38:24.000 --> 02:38:29.000
Uh, that is also possible through Pulit, basically. Yeah, simple matter of thing is that if ATPI is returning something, then I can get it, as simple as that.

02:38:29.000 --> 02:38:33.000
Mm-hmm.

02:38:33.000 --> 02:38:40.000
Can you put that code here on the chat, and let me see, and try, and then move from there?

02:38:40.000 --> 02:38:41.000
And if I have it… if I have some problem, then I, uh, maybe next time we can talk.

02:38:41.000 --> 02:38:45.000
Yes, sure. Should.

02:38:45.000 --> 02:38:51.000
Yeah, so I'm just, like, pinging this entire file itself into your generative AI group.

02:38:51.000 --> 02:38:52.000
Okay. Okay.

02:38:52.000 --> 02:38:59.000
So, you can download it, yeah. Just remove daughter URL, I don't know why it is appending .url, so I have just pinged you inside the group itself.

02:38:59.000 --> 02:39:02.000
Okay. Okay. Uh…

02:39:02.000 --> 02:39:05.000
Just remove that yellow part.py, only .py.

02:39:05.000 --> 02:39:10.000
Okay. Yeah, yeah, so I'll be able to then carry on from here.

02:39:10.000 --> 02:39:12.000
Yep. Okay.

02:39:12.000 --> 02:39:20.000
But I haven't yet seen. Okay, there's another question, another question, but information. These days, I'm talking with the government of, uh.

02:39:20.000 --> 02:39:21.000
China for their national, uh, programs. To… to fund my company.

02:39:21.000 --> 02:39:27.000
Mm-hmm.

02:39:27.000 --> 02:39:33.000
So, on generative AI. So, I believe we are moving it to the high gear, so we'll know.

02:39:33.000 --> 02:39:38.000
What comes out of this? Oh. But see, that's further information.

02:39:38.000 --> 02:39:39.000
Hmm, okay, okay, that's great. Yeah.

02:39:39.000 --> 02:39:45.000
Okay, yeah. And so, uh, so about the core, the way do I see the code with you?

02:39:45.000 --> 02:39:46.000
So, basically, uh, yeah, open up into a VS Code, remove the URL part. You will see as a Python file.

02:39:46.000 --> 02:39:51.000
I don't…

02:39:51.000 --> 02:39:53.000
Vs Code. Oh.

02:39:53.000 --> 02:40:02.000
Open up in any coordinator, not a VS Code only, in any coordinator, just open it up, and uh… You can see it.

02:40:02.000 --> 02:40:06.000
Let me open it. Yeah.

02:40:06.000 --> 02:40:14.000
Okay, next question. Uh-huh, yeah, you have… you have a question?

02:40:14.000 --> 02:40:15.000
Let me… let me ping a co-direct. Yeah, now code is available inside of JAD.

02:40:15.000 --> 02:40:20.000
Okay, I opened this, but… I'm not sure what are you… maybe I'm not…

02:40:20.000 --> 02:40:23.000
Assalamu alaykum.

02:40:23.000 --> 02:40:26.000
Hmm.

02:40:26.000 --> 02:40:27.000
Actually…

02:40:27.000 --> 02:40:38.000
Ye ha hai.

02:40:38.000 --> 02:40:39.000
Hmm.

02:40:39.000 --> 02:40:50.000
Iga. 1536.

02:40:50.000 --> 02:41:08.000
Hmm. Hmm, hmm, hmm. Okay, okay, take it, because some of the application. You have our multiple sentences hai, check it. Toh, yaha par, by default, Java biskender a pass karah, toh, with the help of comma, ki ye sentence alag hai, or ya sentence alag hai.

02:41:08.000 --> 02:41:22.000
Toh is a particular model kil ye output layer hai, wahapa differentiator, sentence ke bich mein, comma hai, ki bah, ye alag sentenc ya lagi ye lagi, ya lagi, ya lagi.

02:41:22.000 --> 02:41:29.000
For example, agar Yuri Mehamdal. Toh ye kaapko humne function diaga, yeh function consider karah, let's suppose krif ek input.

02:41:29.000 --> 02:41:36.000
Do you get this cool? Jaise ye vara, list of sentences hai. Take a list of sentences hai.

02:41:36.000 --> 02:41:48.000
For loop mein, basically, ki, for I in this entire document. For example, agari ko me kulikana, is document ko mere ko iznai yuri keturumin.

02:41:48.000 --> 02:41:53.000
Okay, get emitting function to merepas yaha para hai, or let's suppose ki get emitting function jo hahe, aapka data leita hai.

02:41:53.000 --> 02:42:01.000
Wo, as a input. I in.

02:42:01.000 --> 02:42:06.000
Document. Document embedding. Document embedding kaya hai.

02:42:06.000 --> 02:42:12.000
So, documenting mending merpas data hai. Sorry, document manager, right? Take it.

02:42:12.000 --> 02:42:20.000
Oh, yeah, par hamlic dengue.

02:42:20.000 --> 02:42:33.000
One by one, one by one, lake sko aya. Isma ham kartenge.

02:42:33.000 --> 02:42:42.000
Generate embedding function call curling. Kind we generate embedding function mein ko pathata aki wo ingressa hai, toh pala i is equal to this passcarang effect, this passcurring effort, this passcurring effort, this passcarang hai.

02:42:42.000 --> 02:42:53.000
And istase hamya hapar. Is scar generate embedding, find out curling. Acha bolda not defined.

02:42:53.000 --> 02:42:55.000
And they can have a mere questions, but they can. Another question, Hannah?

02:42:55.000 --> 02:43:01.000
At the second question for that. Second question will be ARM.

02:43:01.000 --> 02:43:08.000
Differentiate hochuka hai. Ignorant. About the kara, second question for the Yaram.

02:43:08.000 --> 02:43:21.000
Checka. In a reverse calculation also, kasempathylega ki is sentence ka matlab kyata, right?

02:43:21.000 --> 02:43:31.000
Sentencer.

02:43:31.000 --> 02:43:35.000
Hmm.

02:43:35.000 --> 02:43:39.000
Is coming, yeah, yeah, expect some tents lay around.

02:43:39.000 --> 02:43:46.000
Ah, exactly, exactly to Wahihogana. For example, yaga mirapas buk hai. Buku may be aapko jo data aapar read kar hoge, pore ke puriah book ka.

02:43:46.000 --> 02:44:06.000
Uska length kha hona chi, uska length itna character hona chi, uska length itna word hona chai, that you are going to define.

02:44:06.000 --> 02:44:15.000
Model output kita majita hai. Model that I have output 1536.

02:44:15.000 --> 02:44:30.000
As simple as that. You model the output jo hai, wo humes, let's suppose ki hamne yaha part 2 hajar word ke extendens dal di hai. Model ha mesa uska jo output generate karnega 1536 mehi generi dimension mein. U, same dimension mein usko chai karega represent karne ka, kyogi?

02:44:30.000 --> 02:44:40.000
Aapar matching karana hai. And then, you will do the comparison.

02:44:40.000 --> 02:44:49.000
Paul, good question, like, huh?

02:44:49.000 --> 02:44:56.000
And you can structure semantic through three cases. Sometimes I may define ganne padding, yeh hamara sundansa.

02:44:56.000 --> 02:45:07.000
For example, example do, ki, like, year data, biba thaw EHGs.

02:45:07.000 --> 02:45:08.000
Hmm.

02:45:08.000 --> 02:45:15.000
Yeah, they can. For example, 90 bokeh.

02:45:15.000 --> 02:45:24.000
Huh, okay. So, Wahith… Hmm. Hmm.

02:45:24.000 --> 02:45:30.000
Hmm, hmm.

02:45:30.000 --> 02:45:31.000
Hmm, hmm.

02:45:31.000 --> 02:45:41.000
32k hein. Luna.

02:45:41.000 --> 02:45:47.000
Hmm.

02:45:47.000 --> 02:45:48.000
For embedding your baskets.

02:45:48.000 --> 02:46:03.000
Apka? Okay. Book available, hai, aur humko us nainke book sections push na hai, auru answer us books aanishi. That is your problem statement, right?

02:46:03.000 --> 02:46:24.000
Process kaoga madat niapro. Ab is book kyandar merepas multiple chapters hai. Maldah multiple, basically, pages hai, auric page kanda, meerpa data available.

02:46:24.000 --> 02:46:49.000
Page by page, page by page, page by page. Page by page, raw data. Simple. Jho raw data hai available, raw data mau aprening hai, balay PDF seh, balay wo Word document ho ha text document a, merit karang hai. Now, just say hi, yaha parham is data ko read karte hain. So, let's suppose ki hum y chate hain, ki yaha pad jo ek paragraph hai, wo ek sentence bane. Matav asa embedding amuso convert karne.

02:46:49.000 --> 02:46:55.000
Of length may be 1536, or maybe 1028, whatever it is, right?

02:46:55.000 --> 02:47:06.000
So, 1536 kaham yaha par eigmine menay. Toh ye banjayaga, jige, one by one read karke. A ye saara ka sara embedding hum kayakarang store kar denge apne database. Tami to database kam ka mai ka na.

02:47:06.000 --> 02:47:16.000
Database kenda. Your entire book has been processed.

02:47:16.000 --> 02:47:27.000
Now, Jabap koshan buhoge. Kit thi ka, explain me, uh… like a Newton's principle, or explain me the Archimedes principle, basically, or maybe a Newton's rule.

02:47:27.000 --> 02:47:39.000
From this particular book. Toh yega agarrega ki Our Lady mere saare ke saare book converted kiske andar hai, embeddings converted hain. Cheek, saaring books jo hai, embeddings. Toh, jabhi aap yaha se queri pucho gaisko.

02:47:39.000 --> 02:47:51.000
Nearest embedding, ha wok ya hai, matau kansa asa paragraph hai, ya kansa asa page hai, jusko ape store kya wa hai.

02:47:51.000 --> 02:48:02.000
Joel is queries a match card rah hai. For example, Gabradam Bushake tell me about Newton's principle, right? The concepts are paragraph adjustment, Newton's principle ke bharayan batay, ya, feel ape tell me about a Newton's false principle, or first rule. Take care.

02:48:02.000 --> 02:48:20.000
Toh, this is how you will solve the problem. Now, coming to the next step, so already, YAM HAMNE project Bahutwar karaya hua hai apone class kyanda to aapko code bhi bina weight kiwe abhi min saktah hai.

02:48:20.000 --> 02:48:32.000
So, let me show you that part as well. Yes, onlya? Isn't my confusion, eh?

02:48:32.000 --> 02:48:33.000
He's like, school. Kalhan.

02:48:33.000 --> 02:48:37.000
And then he says… embedding, aapke… queries have similar.

02:48:37.000 --> 02:48:51.000
Parameter k. Uskan bohotan top K. So, let's suppose top case is equal to Raham thirak dih, toh kyaka rega.

02:48:51.000 --> 02:49:04.000
What? Top 10? For example, ajaate ajihal example kianda, jo already, prove kar cho ke haem. Cheek.

02:49:04.000 --> 02:49:15.000
Kiya? Right? Now, top consata, ta apne wala kichaluya. Let's suppose ki humbling gin hai, miracle na aap top tin doh.

02:49:15.000 --> 02:49:27.000
Top consult dohe. Tahalat Matching Yehoga, second matching yoga, and third top kiah Hoga yoga, right?

02:49:27.000 --> 02:49:33.000
Use one. It's a payroll score goes in a language model, go ahead.

02:49:33.000 --> 02:49:34.000
To civil servants.

02:49:34.000 --> 02:49:44.000
Exactly, but it was called the top key.

02:49:44.000 --> 02:49:54.000
Toh kya kar regga? Second ranked Yehuah, third ranked Yahuwa.

02:49:54.000 --> 02:49:55.000
Generate kar de ga hamara a answer.

02:49:55.000 --> 02:50:04.000
Right. Mary? Si.

02:50:04.000 --> 02:50:19.000
Aaparan. Language model based.

02:50:19.000 --> 02:50:40.000
Not for generation, for formatting. Kib bahi. 100.

02:50:40.000 --> 02:50:47.000
Right. Toh sentence ka meaning tohai ray kaapne sur kya vai, aurusa ki wo aata dura ho.

02:50:47.000 --> 02:51:04.000
Human-readable Navo. Kepas. Human retail hoga. And isi concept kohan bolte kya.

02:51:04.000 --> 02:51:15.000
Rag. Retrieval augmented generation.

02:51:15.000 --> 02:51:25.000
Exactly. Apne wala top 1, right? Toh output toh, by default, aapko jyodega ga ye, wo yahi theega, top one. Yahi denay usko, check hai.

02:51:25.000 --> 02:51:26.000
Honey. Honey.

02:51:26.000 --> 02:51:30.000
Refrigerate it.

02:51:30.000 --> 02:51:47.000
Even though hum ko broken melah hai palace. And that is technical RAG.

02:51:47.000 --> 02:51:50.000
Yeah, just sometimes game headings are here toho alam ko pataya yin.

02:51:50.000 --> 02:52:17.000
Yeah, bh. Embedding iska hai, meher quality pata hai na.

02:52:17.000 --> 02:52:18.000
Move on from… and move on. Thank you, thanks.

02:52:18.000 --> 02:52:28.000
So that I already know. Again, tap poker jaldi pranalah hai, toh ape kang kiji ha baha chess seh hamne record karke pura project huaja karke, realt aji aji aapkar sah, dihatnya aaphug aapk ru ki.

02:52:28.000 --> 02:52:43.000
Tiger. Recording tha, basically. Cheek hai.

02:52:43.000 --> 02:52:54.000
Pdf record leh, matla jo medical history hai wo basically ape leh, hai, panche past indiga.

02:52:54.000 --> 02:53:04.000
Or, basically, Uskoham dal nahy vector database kyandar, aur iska theme ye hai medi chatbot program ki aap, let's suppose ki aap ek Dr. And aap ke lakho patient se.

02:53:04.000 --> 02:53:12.000
Though, obviously, our subcode database must search karogay uske namse se ushe, jada chaka hai, ki aapka pa se bot type ka hai.

02:53:12.000 --> 02:53:24.000
Just go south. Past history the book kuchwa aapne aya bataar hai, toh ye walafka.

02:53:24.000 --> 02:53:30.000
Diga? And then, yeah, we have a curl website, and it's just CS.

02:53:30.000 --> 02:53:41.000
Though RG bastard is ke a lava johe mere boss aare bache se mala bohot project milja aapko yung pada ski liae. Bahut bahut ala le variety.

02:53:41.000 --> 02:53:46.000
Already available to jaky aap baji kasak tour, nahi karna hai, ta aapki badge ka weight karo.

02:53:46.000 --> 02:54:01.000
Huhai hai. Rag. Toh, apijo…

02:54:01.000 --> 02:54:04.000
So, connecting, embedding with the vector database, sample ROG flow line.

02:54:04.000 --> 02:54:14.000
Nissan.

02:54:14.000 --> 02:54:15.000
Hello, sir.

02:54:15.000 --> 02:54:18.000
Okay, question clear? Or then…

02:54:18.000 --> 02:54:22.000
These are question clears, thank you, sir.

02:54:22.000 --> 02:54:28.000
Anna. Okay, Pushwa, please go ahead with your question.

02:54:28.000 --> 02:54:29.000
Yeah, good morning.

02:54:29.000 --> 02:54:34.000
Uh, yeah, good morning, sir. Uh, I do not have a question related to today's class, but, uh.

02:54:34.000 --> 02:54:35.000
Logan?

02:54:35.000 --> 02:54:41.000
Uh, this is related to the ML, uh, modeling, uh, which I'm doing for the, uh, time series forecasting.

02:54:41.000 --> 02:54:42.000
Okay.

02:54:42.000 --> 02:54:50.000
Uh, using Arima and Sarima. But the data is huge, and I have to create multiple models per.

02:54:50.000 --> 02:54:57.000
Like, uh, items and all. So it has to create multiple models.

02:54:57.000 --> 02:55:00.000
But it is taking a lot of time, uh, to complete the entire, uh, come up with the entire models.

02:55:00.000 --> 02:55:05.000
London GPU, ma'am, then. Run that on GPU. It will optimize the time.

02:55:05.000 --> 02:55:07.000
Uh, if… Okay, uh, Deepu…

02:55:07.000 --> 02:55:15.000
Just go with… go ahead with… go ahead with H100 kind of a GPUs. H100, A1. And it's not that costly, like, it'll cost you, like, a $2, $3 rupees, uh, for 1 hour, 2 hour.

02:55:15.000 --> 02:55:25.000
Um, actually, that is… I'm doing it for some of the company, but that GPU availability is not there in the local system.

02:55:25.000 --> 02:55:26.000
What they have provided. But…

02:55:26.000 --> 02:55:32.000
No, in local… in local, ma'am, it will not be available. Obviously, you can't expect a GPU, like, heavy GPU, right? 24gb, 48GB is fine.

02:55:32.000 --> 02:55:34.000
Uh-huh.

02:55:34.000 --> 02:55:39.000
But you can't expect, like, an 80GB GPU or H181 in local. It will always be available on a cloud platform.

02:55:39.000 --> 02:55:47.000
So, you can just go ahead with the GPU and launch the notebook over there within 2 minutes, you will be able to launch it, and run it over there.

02:55:47.000 --> 02:55:51.000
If you're just taking a lot of time in training, it will be reduced.

02:55:51.000 --> 02:55:52.000
Immediately.

02:55:52.000 --> 02:55:58.000
Okay, one more doubt I have is, like, uh, can I, uh, like, train the models, like, uh.

02:55:58.000 --> 02:56:04.000
Like, providing the inputs in chunks, like, uh, for one year, for next two years.

02:56:04.000 --> 02:56:11.000
Can I retrain the same model, uh, and it has to consider the previous… whatever it has trained? Is it possible?

02:56:11.000 --> 02:56:18.000
That is called as fine-tuning, ma'am. That is possible. It's called as technically fine-tuning.

02:56:18.000 --> 02:56:22.000
You mean to say, like, uh, for example, I'm using one Narima model, and I'm giving the.

02:56:22.000 --> 02:56:23.000
Hmm.

02:56:23.000 --> 02:56:33.000
Um, turning it to… for one-year data. Then, I want the same model to be trained on, uh, next, uh, year data.

02:56:33.000 --> 02:56:34.000
So, will that override the previous trend, or whatever it has got trained?

02:56:34.000 --> 02:56:37.000
Hmm, hmm.

02:56:37.000 --> 02:56:49.000
See, ma'am, I'll tell you the fundamentals of model. See, what is the meaning of fundamentals of model? Fundamentals of model means the weights, right? Let's suppose I'm talking about Y is equal to MX plus C. Arima is the extension of Y is equal to mx plus c. Basically, arima and sarima is technically a extension of linear model itself, right?

02:56:49.000 --> 02:56:55.000
Got it.

02:56:55.000 --> 02:57:05.000
Liname model itself, with, like, some sort of a, like, you can say, like, a lag and, uh, lag of the data, or maybe some sort of an interval of the data. But technically, it's a linear model.

02:57:05.000 --> 02:57:11.000
Now, what is the meaning of any models? So, there will be a relationship. Who is going to define the relationship? A parameters, right?

02:57:11.000 --> 02:57:15.000
For example, in Y is equal to MX plus C, so M and C is the parameter.

02:57:15.000 --> 02:57:16.000
Yeah.

02:57:16.000 --> 02:57:23.000
Yes, MNC is the parameter. Similar parameter, you will be able to find out in every model, doesn't matter some machine learning or deep learning.

02:57:23.000 --> 02:57:24.000
Yeah.

02:57:24.000 --> 02:57:31.000
So, when you are saying that I have an existing model, it simply means that there is a fixed value of M and C that you are able to reach out.

02:57:31.000 --> 02:57:32.000
Correct. Yes.

02:57:32.000 --> 02:57:40.000
Then only you say that I've trained the model, right? Means, you are able to reach out to some relation, right? Some relation. So, some relation between your X and Y.

02:57:40.000 --> 02:57:41.000
With the help of these parameters. Now you are saying that you have a new data.

02:57:41.000 --> 02:57:45.000
Got it.

02:57:45.000 --> 02:57:46.000
Right? And you try to extend a same model on that data.

02:57:46.000 --> 02:57:50.000
Yes.

02:57:50.000 --> 02:57:51.000
Got it.

02:57:51.000 --> 02:57:58.000
Right? Now, what will be the approach? Extracted the previous weight, this value of MNC, which you can print, right? If you have a model, if you have already trained the model. So, you will be able to extract the value of the parameter, trainable parameter.

02:57:58.000 --> 02:58:02.000
Uh-huh.

02:58:02.000 --> 02:58:08.000
Yeah.

02:58:08.000 --> 02:58:09.000
Okay.

02:58:09.000 --> 02:58:15.000
Right? Now, set that trainable parameter in a beginning itself. Which is called as parameter initialization, or in case of a deep learning, we say weight initialization. So, there is a concept.

02:58:15.000 --> 02:58:29.000
Where we can start a weight from a random. Right? Let's suppose I don't have any idea. So, obviously, I'll start from the random, and there is something called as… we can start a waiting session from a particular parameter, because at the end of the day, it's a matrix, right?

02:58:29.000 --> 02:58:30.000
Okay.

02:58:30.000 --> 02:58:40.000
So, I can initialize it. Yeah, so this time initialization is what? You are trying to reference from… take a reference from the previous model which you have already built. So, technically, it's possible.

02:58:40.000 --> 02:58:41.000
Okay, okay, thanks.

02:58:41.000 --> 02:58:49.000
And that is called as fine-tuning. Yeah. Okay, so next question raised, please go ahead with your question, then, yeah.

02:58:49.000 --> 02:58:50.000
Yeah. Hi, Sudanjo. Thank you for the lecture, it was so good.

02:58:50.000 --> 02:58:55.000
Yep, nice. Thank you.

02:58:55.000 --> 02:59:04.000
Uh, I have a basic question, uh, like, uh… See, uh, we have seen a lot of embedding models are available in Hugging Face, right?

02:59:04.000 --> 02:59:11.000
And they were also mentioned the OpenAI is also giving some models, text small 002 ADA, something.

02:59:11.000 --> 02:59:12.000
Mm-hmm, hmm, hmm.

02:59:12.000 --> 02:59:18.000
So, uh, basically, on what basis I should select an embedding model?

02:59:18.000 --> 02:59:26.000
Uh, based on the benchmark, basically. So, let's suppose if I'm trying to, like, use GPT versus Aquin, right? Let's suppose I was using Quinn.

02:59:26.000 --> 02:59:31.000
Now, if you'll see the benchmark, GPT versus Quinn, model benchmarking.

02:59:31.000 --> 02:59:40.000
Obviously, GPT model has been trained on more amount of the data for more epoch. Accuracy is very, very high, so technically, I'll go ahead with the GPT model, not with the Quinn.

02:59:40.000 --> 02:59:45.000
So, uh, so basically, you were claiming that it is not based on dimension size, because.

02:59:45.000 --> 02:59:46.000
Squinish… Gwenish showing…

02:59:46.000 --> 02:59:58.000
No, not based on the dimensions, that is, not at all, not at all. So, see, there could be a model… see, dimension means what? Dimension means my network architecture. In my network architecture, if I have defined that my outer layer, output layer, is going to be 10.

02:59:58.000 --> 03:00:11.000
Norton is a dimension. If you have defined that output layer is going to be 1000, then 1000 is the dimension, but the tag doesn't mean that, that those 10 or 1,000, right, those 10 or 1000 will be better or worse as compared to each other.

03:00:11.000 --> 03:00:17.000
Because that depends upon what? That depends upon the middle layer, right? The weights that you have in between the hidden layers.

03:00:17.000 --> 03:00:18.000
Under… yeah.

03:00:18.000 --> 03:00:23.000
Yeah? So, output layer, I can define 10, I can define 1000, I can define even 10,000, but.

03:00:23.000 --> 03:00:27.000
That doesn't mean that my model will be good or my model will be bad.

03:00:27.000 --> 03:00:33.000
Okay, gotcha. Uh, and uh, next question, uh. I'm sure regarding, uh, uh…

03:00:33.000 --> 03:00:39.000
Regarding the… so, once we're converting into embeddings, we are storing into DB, basically.

03:00:39.000 --> 03:00:40.000
Hmm, hmm.

03:00:40.000 --> 03:00:46.000
So, you mentioned different databases, like, uh, ChromaDB, FireSys, several DBs are available in the market.

03:00:46.000 --> 03:00:47.000
Hmm, hmm.

03:00:47.000 --> 03:00:54.000
So, no, when coming to retrieval part. So, user query will be converting into embedding first, correct?

03:00:54.000 --> 03:00:55.000
Hmm, hmm. User query will be… yes, user query will be converting into emitting first, yes.

03:00:55.000 --> 03:01:07.000
Uh, then… Uh, then it will, uh, find the, uh, similarity search, and it will get whatever, uh, top you mentioned, 3 or 5 or 10, whatever you mention, it will get.

03:01:07.000 --> 03:01:09.000
Yeah, yeah. It will get, yes.

03:01:09.000 --> 03:01:17.000
Right? So… so it will perform, you know, this calculation with, uh, uh, every vector present in the database.

03:01:17.000 --> 03:01:24.000
Exactly, yeah. So, as of now, it is going to perform this calculation with every vector available into our databases.

03:01:24.000 --> 03:01:35.000
There is an optimization, so where I can go ahead with more indexing, more hashing, which is called as a hybrid approach. So, which will at least try to reduce a search by a certain volume, but.

03:01:35.000 --> 03:01:38.000
Ideally speaking, it is going to do a calculation with all.

03:01:38.000 --> 03:01:45.000
Okay. So, uh, in that case, why do I need… why do I need to use a quadrant or external database? Because.

03:01:45.000 --> 03:01:53.000
I can store my data into text, I retrieve and I do the same calculations, and I can make it. Is it possible?

03:01:53.000 --> 03:01:56.000
Exactly, this is what Facebook is similarity search is going to do.

03:01:56.000 --> 03:02:02.000
Or even, let's suppose you are not using a Facebook AI similarity search, right? So today, we were able to find out the similarity.

03:02:02.000 --> 03:02:09.000
Yeah. Yes, yes. Got it. Right.

03:02:09.000 --> 03:02:10.000
Yes.

03:02:10.000 --> 03:02:17.000
So I don't need a database. See, even today, I was able to find the similarity, right? And have I used any kind of a database? No. It was a plain, simple Python code, right? But yeah, in case of a data basis, in case of that packages, they have optimized it a little bit.

03:02:17.000 --> 03:02:18.000
Okay.

03:02:18.000 --> 03:02:29.000
They have optimized it. For example, if I talk about quadrant, right? So, they are giving you a self-hosted service. Me and you are talking about these embeddings and all those things, right? But let's suppose there is a guy who is not aware about anything.

03:02:29.000 --> 03:02:30.000
Yeah. Okay. Yeah.

03:02:30.000 --> 03:02:39.000
He just need a database. For example, my team, right? My development team, so they don't understand AI, actually, right? So we were building a job system.

03:02:39.000 --> 03:02:50.000
Now, in job systems, we were trying to scrap a 50,000 job on a daily basis, and from other sites, a user is going to ask… for example, you are the user, so you will go and ask a machine learning, data science, and then you will try to put some.

03:02:50.000 --> 03:03:00.000
Conditioned part-time, full-time, and then maybe condition as a country or something, right? So, I have 50,000 jobs, and then there is a query that you are sending, right?

03:03:00.000 --> 03:03:08.000
For my jobs. Now, my team was not aware about all the similarity search and all these embeddings and vector, right?

03:03:08.000 --> 03:03:13.000
They just understand that I need a DB, where I'll bring all the scrap, all the job, I'll keep it inside the DB.

03:03:13.000 --> 03:03:23.000
They just understand that part, right? And from a user side, so user will send a query, so I'll try to search, and I'll try to give you the result. So they were looking for a best possible DB, so which can do a better matching.

03:03:23.000 --> 03:03:24.000
Understood.

03:03:24.000 --> 03:03:33.000
Based on the context, they have used a vector DB. So that's the reason these databases are available, the services, because at the end of the day, it's a piece of code which is running in a backend, right?

03:03:33.000 --> 03:03:34.000
Yeah.

03:03:34.000 --> 03:03:43.000
But whether you are going to use it on a cloud or maybe in a local installation. So… for those kind of a people, these things are required. People like me and you, if you don't want to use any of the databases.

03:03:43.000 --> 03:03:46.000
I'm like, it's fine. We have not used it today itself.

03:03:46.000 --> 03:03:56.000
And one more question, Suzanne. So, how do you handle duplicate values in this case? Does it follow indexes like traditional databases uses?

03:03:56.000 --> 03:03:58.000
No, not at all, not at all. Not at all.

03:03:58.000 --> 03:04:02.000
So, how do you… how do you… if I… if I same document, I uploaded two times.

03:04:02.000 --> 03:04:03.000
So, how do I, you know, how do I handle this duplicates in that case?

03:04:03.000 --> 03:04:11.000
Mm-hmm, mm. So, based on the similarities such again, so based on the mapping, whether this is available or not, so if it is.

03:04:11.000 --> 03:04:18.000
Coming as 100%, right? 100% for most of the record. It simply means that those records are already available, those contexts are already available inside your DB.

03:04:18.000 --> 03:04:23.000
Okay, so either I should ignore that, or I should delete and insert the fresh data.

03:04:23.000 --> 03:04:31.000
Exactly. So you can… then you can write a condition, if condition, that, okay, fine, so if similarity is going to be absolute 1, in that case, remove it.

03:04:31.000 --> 03:04:32.000
Yeah.

03:04:32.000 --> 03:04:38.000
Okay, fine. And last question, uh, regarding, uh, Avni. I have, uh, given interview in Avni last week.

03:04:38.000 --> 03:04:39.000
So, uh, it is not a question. Basically, I want to give you the feedback regarding the system.

03:04:39.000 --> 03:04:43.000
Okay, okay.

03:04:43.000 --> 03:04:44.000
Mm-hmm, mm-hmm.

03:04:44.000 --> 03:04:51.000
Uh, so, uh, and I guess in fourth, uh, fourth round of interview, I don't know what is the reason. Suddenly, it got stuck.

03:04:51.000 --> 03:04:56.000
So, uh, whatever I'm speaking, it is not able to, you know, answer me back.

03:04:56.000 --> 03:04:57.000
Uh…

03:04:57.000 --> 03:05:03.000
Okay. Okay, then there could be a multiple possibility. One is your system internet, another one is your, like, processing speed, because everything is processing in a client side.

03:05:03.000 --> 03:05:17.000
So, technically, on your system side. Uh, third possibility could be a problem with API interfacing with Abini, but again, that depends upon the internet, because the real-time conversation, so we are sending a packet back-to-back to our system, so where we are doing, uh, inferencing, right?

03:05:17.000 --> 03:05:19.000
No. No.

03:05:19.000 --> 03:05:35.000
So, if there is a packet loss, then Avni will not be able to understand. So, it requires a good internet. We are trying to optimize it more and more, right, so that it will be able to work even with a very less internet, or even it will be able to handle some packet losses in between.

03:05:35.000 --> 03:05:38.000
Uh, but yeah, so that could be a possible reason for that.

03:05:38.000 --> 03:05:50.000
Yeah, uh, yeah, I understood that. So, uh, in that case, when I do, uh, end, uh, end interview, right, at least it should evaluate, uh, till that time what is happened, and it should show me the score, right?

03:05:50.000 --> 03:05:58.000
But when I do end session, because system is not responding, so it directly deleted one interview from my account.

03:05:58.000 --> 03:06:01.000
And it is not showing me any result.

03:06:01.000 --> 03:06:09.000
Hmm. So, actually, I have asked my team to, you know, show a result round by round, because I have received this problem, because anyone who's reading over the internet.

03:06:09.000 --> 03:06:17.000
Might have the problem, they may face the problem. So round by round that, okay, fine. So, store, uh, basically take one round, do the checkpointing. Then, again, take one another round.

03:06:17.000 --> 03:06:23.000
Dual checkpointing. So based on round, or maybe based on the time. So, every 5 minutes, do the check pointing.

03:06:23.000 --> 03:06:24.000
So I've just asked them, yeah.

03:06:24.000 --> 03:06:34.000
And one more… yeah. Yeah, and one more suggestion. So, when Avni is asking the question, I can be able to open another tabs and see, uh, so is it possible to disable those options?

03:06:34.000 --> 03:06:41.000
I mean, like, it's a practice one, right? So, we are, like, even building internally, and we are planning to.

03:06:41.000 --> 03:06:48.000
Release something for the companies. Uh, this same product, so that company will be able to use this system. And many companies are already using it, so even I have…

03:06:48.000 --> 03:06:49.000
Jesus. I have a sin. Yeah.

03:06:49.000 --> 03:06:59.000
My students, so… yeah, so my student, I… many students, they have given an interview in recent days, and none of the human beings has taken their interview. Any kind of a system has taken their interview.

03:06:59.000 --> 03:07:04.000
So, we are also trying to, like, commercialize it, so that we can earn some sort of a revenue out of it.

03:07:04.000 --> 03:07:12.000
Uh, in that one, we'll give all those options, all those security features, like, uh, you can't share the screen. It will be completely proctored, by the way.

03:07:12.000 --> 03:07:25.000
And last suggestion would be, if I want to skip any round, I think that option should be there, because if I'm doing… let's say I'm not interested in DSA round, because I didn't prepare it. So.

03:07:25.000 --> 03:07:35.000
Basically, when I'm using first time, no, I don't know, uh, these many rounds will be there. I had a 20 minutes of free time, and I started, uh, so… I don't have an option to skip this round.

03:07:35.000 --> 03:07:42.000
Yeah, internationally, we have not given that. That is fine, that can be done easily from our side, not a big feature at all.

03:07:42.000 --> 03:07:50.000
But internally, we have not given that, so that, see, you're going through a mock-up, right? So, when you are going through a mock-up, our intention is… was to create a system.

03:07:50.000 --> 03:07:53.000
Which will force you to see everything, at least. So put these things in your mind, that, no, this is required.

03:07:53.000 --> 03:08:07.000
Yeah, understood.

03:08:07.000 --> 03:08:08.000
Yeah.

03:08:08.000 --> 03:08:16.000
Many times, when we give you a feature, right, that, no, you can skip this round, this round, this round, so maybe you will not even look into that, and you will click on the button. Now, when you are going through some friction, so at least you will see a question, because you will be able to go to the next one, how? By saying that, okay, go to the next, go to the next, go to the next.

03:08:16.000 --> 03:08:17.000
If you're gonna say these things to Avani, Savini will keep on going to the next, next, next question, and will finish the round.

03:08:17.000 --> 03:08:21.000
Yes, yes.

03:08:21.000 --> 03:08:22.000
Yeah.

03:08:22.000 --> 03:08:27.000
Right? But in that process, uh, you go through all those questions. You at least see those questions for 5 seconds, 10 seconds.

03:08:27.000 --> 03:08:28.000
Perfect. Perfect.

03:08:28.000 --> 03:08:42.000
Right. And we have created this one for a mock-up, not for taking a real-time companies… for taking the real-time company interview, but not in a real situation, for a mock-up. So, I want… to force my student that, okay, see everything.

03:08:42.000 --> 03:08:48.000
Doesn't matter whether you are able to answer it or you're not able to answer it, that's okay, but see everything, at least.

03:08:48.000 --> 03:08:56.000
Somewhere it will be in your mind, and when it will be in your mind, you will go through it before going for the interview, or maybe while doing a preparation.

03:08:56.000 --> 03:08:57.000
Yeah, I gotcha. Got it, Sandra. Thank you so much for the opportunity.

03:08:57.000 --> 03:09:05.000
Maria, rest of the things, so, like, I have already asked my team to do the implementation, so I think, uh.

03:09:05.000 --> 03:09:09.000
Couple of more features they will release, uh, for our Avni is, uh, like, some days.

03:09:09.000 --> 03:09:10.000
Thank you so much. Thank you.

03:09:10.000 --> 03:09:16.000
Yep, thanks, thanks, Nikka. Okay, go, next one, Arman, go ahead, then Avinaz, go ahead.

03:09:16.000 --> 03:09:18.000
Anyone can go ahead, solve this. Yeah, Robin, yes, Avon?

03:09:18.000 --> 03:09:25.000
You know, good afternoon, sir, this is a month. Yeah, someone's first question is that you showed the embeddings regarding to tests, so…

03:09:25.000 --> 03:09:26.000
Hmm. This can be done to our images too, yes.

03:09:26.000 --> 03:09:31.000
This can be done with images, too.

03:09:31.000 --> 03:09:32.000
This is a similar kind of pattern will follow.

03:09:32.000 --> 03:09:49.000
Because images are a NumPy array, by the way, in terms of a system, right? It's just a matrix. Images are nothing but it's a… matrixes. Uh, but, uh, you can do this kind of embeddings with respect to the images, and then you can try to find a similarity search.

03:09:49.000 --> 03:09:57.000
But it will be… like, uh, not a good approach. So, for image, uh, we have already, like, a…

03:09:57.000 --> 03:10:00.000
Multiple different, different approaches to do a similarity search.

03:10:00.000 --> 03:10:07.000
Okay, so the next question is, sir, as you said. Uh, we will store each and every text into embeddings, into our database. So, in storing the database, it will not perform any kind of operations, like.

03:10:07.000 --> 03:10:13.000
Hmm.

03:10:13.000 --> 03:10:15.000
Similarity or something. When we put the user query.

03:10:15.000 --> 03:10:28.000
No. Yeah, so similarity will be done when you write a query, right? Then you will try to go ahead with the matching. When you're trying to store it, then it will just convert into an embedding, store the data. Simple.

03:10:28.000 --> 03:10:37.000
Awesome. And so, by any chance that you… Whatever you told about revenue, so in any of the, uh…

03:10:37.000 --> 03:10:42.000
Portion of the project we are creating a system like that.

03:10:42.000 --> 03:10:48.000
So, sorry, sorry, can you please repeat once again, Arman? I think I was… Oh.

03:10:48.000 --> 03:10:49.000
Uh-huh. Uh huh.

03:10:49.000 --> 03:10:55.000
Maybe a part of it, not the whole system. Like, you can… So, as you told about Avni in the previous question, so is there any course or something in your own present that we are creating a system like Avni by ourselves?

03:10:55.000 --> 03:11:05.000
Uh, in your… this particular courses, I have mentioned Yuri. See, and that too, like, uh, you know, after having a lot of discussion with my team and everyone, I was able to.

03:11:05.000 --> 03:11:16.000
Mentioned URI system, at least, in this one, uh, because it's not difficult for us to deliver, especially for me, it's not difficult to deliver the project. I can come, I can, like, deliver this project.

03:11:16.000 --> 03:11:22.000
But, uh, I have seen that that expectation of the students are to, you know, write a code line by line.

03:11:22.000 --> 03:11:29.000
Now, for projects like Avni, project like Yuri, where we have, like, a 1 lakh, 1.5 lakh, 2 lakh line of a code.

03:11:29.000 --> 03:11:41.000
Thousands of modules. It is not possible, right? Even… it took, like, a month and month of time, not just one or two months, it took multiple months of time, multiple team members to create that project.

03:11:41.000 --> 03:11:49.000
And generally, when we come to the class, right, so that is the expectation, and with that expectation, obviously, can't discuss those kind of projects.

03:11:49.000 --> 03:12:11.000
But yeah, don't worry, I have already included Yuri. So, Yuri will be discussed, that is a very, very good project, a very heavy one, by the way. Again, it's not an easy… project for any one of us, and that will give you a lot, means literally a lot of idea, and that too, in real, real time. We are not, like, saying a real time for the sake of saying real, no. In a real, real time, because all of you are using it, right?

03:12:11.000 --> 03:12:15.000
So all of you are using it, Yuri, on a regular, daily basis. Thousands of people are using it on a daily basis.

03:12:15.000 --> 03:12:23.000
So that is something that will create on a scale. Till domain binding, till hosting domain binding, everything, till, like, a CDN and everything.

03:12:23.000 --> 03:12:27.000
3dm catching with memory, context, everything.

03:12:27.000 --> 03:12:33.000
Well, if I would have stored it for a source for our families that I can store it.

03:12:33.000 --> 03:12:34.000
Sorry, sorry, sorry, I think your voice is low.

03:12:34.000 --> 03:12:40.000
Anything else is. Sir, if I want to explore the code by myself, or for you, so is there any possibility to do that?

03:12:40.000 --> 03:12:49.000
As of now, we have not released it, so hope you understand. If we release everything, then again, not good for company.

03:12:49.000 --> 03:12:53.000
Yeah, that's not in a favor, favor of company, at least.

03:12:53.000 --> 03:13:02.000
Okay, so maybe, is there any possibility to create a part of that? For example, I upload my document, and then I'm having a real-time conversation related to that particular document only.

03:13:02.000 --> 03:13:08.000
That is possible, yeah, that is possible. If we are able to create a DIY, it is not possible. We have done that, right?

03:13:08.000 --> 03:13:09.000
What is it going to?

03:13:09.000 --> 03:13:18.000
Practically, it's possible, because it's live. We had been able to do it, so we can do it again another time.

03:13:18.000 --> 03:13:24.000
Yeah, okay. Okay, so next is Rithik. Please go ahead with your question.

03:13:24.000 --> 03:13:25.000
Yeah, I think you are able.

03:13:25.000 --> 03:13:32.000
Hello, am I audible? Yeah, sir, my question is that how much, uh, how much amount of food?

03:13:32.000 --> 03:13:35.000
Past APR is important, uh, for this generative vehicles.

03:13:35.000 --> 03:13:42.000
See, FastAPI is not important for generative AI or data science or anything, wherever you are building a real-time system.

03:13:42.000 --> 03:13:49.000
Like, wherever. You are building a real-time system. In every real-time system, there will be a front-end, there will be a back-end, there will be a middle layer.

03:13:49.000 --> 03:13:50.000
Yes, sir.

03:13:50.000 --> 03:13:58.000
That is important. Whether you are building some application in generative AI, or machine learning, or in big data, or maybe other development, or maybe a blockchain anywhere.

03:13:58.000 --> 03:14:03.000
These three components will come into a picture by default in any project.

03:14:03.000 --> 03:14:04.000
Yes.

03:14:04.000 --> 03:14:10.000
Now, obviously, when we talk about a backend, so you are going to write a function, backend means what? Function, right? Instead of function, you are going to write.

03:14:10.000 --> 03:14:13.000
So, which will communicate with our databases, other services, and all those things.

03:14:13.000 --> 03:14:20.000
Now, you have to expose it. To our front end, or maybe to our middle layer, you have to bind it.

03:14:20.000 --> 03:14:26.000
Api is required. Doesn't matter whether FastAPI, Flask, or maybe, you know, Node, you are writing an API, but API will be required.

03:14:26.000 --> 03:14:34.000
And maybe you are supposed to expose your system to the outside world, to the entire world, so that entire world will be able to use your function. So again, API will be required.

03:14:34.000 --> 03:14:40.000
Right? So, API is something which is a core part of any development.

03:14:40.000 --> 03:14:53.000
Any development. So, if you are going into a development, you are supposed to be aware about it, and if you are aware about a fast API, or any framework in an API, believe me, if you are going to touch even a Node.js.

03:14:53.000 --> 03:14:58.000
With no time, you will be able to understand Node.js. No need to, like, explore a Node.js from a zero.

03:14:58.000 --> 03:15:05.000
Right? Because concept of API is not going to change. Only syntaxes and languages are going to change.

03:15:05.000 --> 03:15:06.000
Yes.

03:15:06.000 --> 03:15:11.000
Yeah, so I always suggest that, uh. Learn it. At least one framework.

03:15:11.000 --> 03:15:18.000
Then you can replicate into a 10 easily, without even my, like, help or anyone else's help.

03:15:18.000 --> 03:15:25.000
So, right, right now, I'm working as a Python developer, and primarily I'm working on a fast API.

03:15:25.000 --> 03:15:26.000
Then… then… then it's fine. Yeah.

03:15:26.000 --> 03:15:30.000
And I have an experience of both. Yeah.

03:15:30.000 --> 03:15:31.000
Yeah, you are already aware about API, then. If you're working as a Python developer, then there is no question to be asked.

03:15:31.000 --> 03:15:36.000
Yeah, so thanks.

03:15:36.000 --> 03:15:37.000
They're right.

03:15:37.000 --> 03:15:42.000
No, no, I'm just asking that how much amount of, uh, first API is required in a generative AI course.

03:15:42.000 --> 03:15:49.000
Okay, so it's required for all the development, not irrespective of generative AI or whatever you are doing.

03:15:49.000 --> 03:15:57.000
Yeah, yeah. I mean, ask, go ahead, please. Yep.

03:15:57.000 --> 03:15:58.000
Yeah, thank you.

03:15:58.000 --> 03:16:07.000
So, thank you so much, sir, for today, Lexia. So, sir, my doubts are… most of the doubts are solved by asking whether the person doubts.

03:16:07.000 --> 03:16:08.000
But, uh, my route is running transformers, as you teach a transformer.

03:16:08.000 --> 03:16:11.000
Okay, okay.

03:16:11.000 --> 03:16:12.000
Topic play, not in detail. So, should I watch, uh, Wolf Select Your Series?

03:16:12.000 --> 03:16:18.000
Mm-hmm.

03:16:18.000 --> 03:16:23.000
Which you mentioned gendered TAI with penalties. Part of you and I was only a certical lecture.

03:16:23.000 --> 03:16:24.000
For learning disabilities.

03:16:24.000 --> 03:16:30.000
No, no, for now, just focus on one. See, I… so there are some people who is having a requirement of, you know, doing things in a fast track.

03:16:30.000 --> 03:16:44.000
Obviously, for that, like, we have offerings available only one platform, but as a regular learner, or as a, you know, a first-time learner, I will always suggest to all of you that, first, do one.

03:16:44.000 --> 03:16:48.000
Then second, then third.

03:16:48.000 --> 03:16:49.000
Yeah, yeah.

03:16:49.000 --> 03:16:56.000
Yes, sir. God is there. And, uh, my second doubt is regarding, sir, I was your student course.

03:16:56.000 --> 03:16:57.000
So, I want to practice more so which platform is best for me.

03:16:57.000 --> 03:17:02.000
Mm-hmm.

03:17:02.000 --> 03:17:03.000
Because…

03:17:03.000 --> 03:17:15.000
Nothing, just, just, just, just do that much, and start building the application. See, you're not supposed to practice, like, keep on practicing things again and again and again. Even SQL interview questions, you will be able to find out as a book on a Euron platform, and you already have access of that.

03:17:15.000 --> 03:17:22.000
So, just try to download that book, and practice all those questions. Whatever question that we have given, and believe me.

03:17:22.000 --> 03:17:27.000
It's more than enough. Literally, it's more than enough. Plus, even inside your batches, I have already given the assignment.

03:17:27.000 --> 03:17:35.000
Right? Solve those assignments, again, more than enough, so no need to explore here and there, and then start using it in application. Simple.

03:17:35.000 --> 03:17:40.000
Because that I am a fisher, and I'm targeting… currently happening for data management to raise questions, and.

03:17:40.000 --> 03:17:42.000
That's what I'm asking, but some mentors ask their SQL questions, so…

03:17:42.000 --> 03:17:59.000
Sure. Hmm. No, no, it's fine, it's fine, it's fine. So, just, just, like, follow that approach, uh, solve the assignments, and solve, like, a book question we have given to you, and that question is coming from the company. So, company-wide question is already mentioned, uh, inside a SQL book.

03:17:59.000 --> 03:18:06.000
One of my students, one of my experienced students, by the way. So I have, like, uh, maintained a book out of it.

03:18:06.000 --> 03:18:10.000
And apart from that, nothing is required. Start using an application.

03:18:10.000 --> 03:18:19.000
We should not keep on learning things. Like. I have seen people who keep on solving a DSA question on lead code, 10,000 questions.

03:18:19.000 --> 03:18:20.000
Yeah, it is.

03:18:20.000 --> 03:18:25.000
Thousand question, then 2,000 questions. I mean, like. Why? Go and build something that'll give you more values.

03:18:25.000 --> 03:18:30.000
And, uh, last one. I just recently used our website, it was awesome.

03:18:30.000 --> 03:18:31.000
Hm?

03:18:31.000 --> 03:18:37.000
But, uh, there is a limit for… monthly limit, free interviews. Can you increase up to 5?

03:18:37.000 --> 03:18:38.000
Recorded this, too.

03:18:38.000 --> 03:18:39.000
Okay, Avni, you are saying, right? Yeah, even I'm planning to, like, increase the limit, so 2 or 5, basically.

03:18:39.000 --> 03:18:45.000
Yes, yes, sir.

03:18:45.000 --> 03:18:53.000
Boy.

03:18:53.000 --> 03:18:54.000
Yes, yes, yeah, of course.

03:18:54.000 --> 03:19:03.000
So, we are just observing, because it's a real-time conversational system, right? So, we are getting a lot of, like, a hit on our server, so we are just, like, and we are affordable, right? But, uh… Servers are not affordable, so we have to pay our bills for AWS anyhow. Uh, we have to pay our bills for, like, a tech stack and APIs that we are using anyhow.

03:19:03.000 --> 03:19:09.000
So, we are, like, in a process of doing a math that, okay.

03:19:09.000 --> 03:19:20.000
Uh, how much we will be able to, you know, invest more in this one, because we have to go for a long time, right? It's not like I have started Avni today, and then tomorrow I will have to close it. Avni.

03:19:20.000 --> 03:19:30.000
At least. So, we are doing a math, uh… As we are affordable, so that is one of the good things, but again, one of the bad things as well for us, at least.

03:19:30.000 --> 03:19:40.000
That we are not having much of runway, so… They're just doing a math. Once that math will be done, and if that math is going to give me a positive result.

03:19:40.000 --> 03:19:41.000
I just have to change one word in a backend from 3 to 5, that's it.

03:19:41.000 --> 03:19:45.000
Yes, I did.

03:19:45.000 --> 03:19:46.000
Yes, it is, you know, of course, sir. Thank you for your insight.

03:19:46.000 --> 03:19:58.000
So, changes is not major, but yeah, that, uh, we are just, like, you know… stuck into that mathematics part.

03:19:58.000 --> 03:20:07.000
Okay. Okay, so next one, Uday, please go ahead and, uh, yeah, Mukesh, you… Manish, sorry, Manish, you can share the screen, Manish. Uh, you have permission.

03:20:07.000 --> 03:20:16.000
In between, so, Uday, please go ahead with the question.

03:20:16.000 --> 03:20:17.000
Yeah. Yeah, Manis itself as well, yes.

03:20:17.000 --> 03:20:26.000
So, is it visible? So, actually, I had doubt in this circle command prompt. Actually, I have this…

03:20:26.000 --> 03:20:27.000
Mr. Say. Okay, I'll paste here.

03:20:27.000 --> 03:20:34.000
Mm-hmm, hmm, image generation looks fine.

03:20:34.000 --> 03:20:42.000
No, no, don't paste here, so it is pasting line by line. So, just do one thing, uh, click on this, uh, down triangle.

03:20:42.000 --> 03:20:54.000
Besides a down triangle, beside this surplus option. You're using command prompt, right? So it is trying to take it, like, one by one. Just click on this down triangle. Beside this command prompt, right? So, we have another tab option.

03:20:54.000 --> 03:20:57.000
Click on that. No, no, no, click on that. Tab, tab, tab.

03:20:57.000 --> 03:21:02.000
In the command prompt itself, just up, up, like, above. Okay, give me, give me… yeah, click over there, and then from there, open up PowerShell.

03:21:02.000 --> 03:21:08.000
Okay, okay, okay, okay, I got it, I got it.

03:21:08.000 --> 03:21:09.000
Published it, okay.

03:21:09.000 --> 03:21:15.000
Hmm. Now, paste it here.

03:21:15.000 --> 03:21:16.000
We are also… it's…

03:21:16.000 --> 03:21:21.000
Best place to try to paste it. Is it pasting line by line?

03:21:21.000 --> 03:21:22.000
No, it's pasting in one, yeah. It's pasting in one, no. Again, line by line.

03:21:22.000 --> 03:21:26.000
It's giving…

03:21:26.000 --> 03:21:29.000
So, give me a remote control, I'll try. You don't have a WSL, right? So, it's not showing Ubuntu, by the way.

03:21:29.000 --> 03:21:36.000
August.

03:21:36.000 --> 03:21:46.000
Cloud cell is fine.

03:21:46.000 --> 03:21:49.000
You don't have an Ubuntu interface? In Ubuntu, so you will not face this problem.

03:21:49.000 --> 03:21:56.000
No, sir, no, sir, not downloaded.

03:21:56.000 --> 03:22:02.000
You have to install a WSL in Windows, then. Uh, for the development domain application.

03:22:02.000 --> 03:22:07.000
And those power cells, default, default, okay. No, you don't have Ubuntu.

03:22:07.000 --> 03:22:14.000
Just install WSL, and then you will be able to do a curl.

03:22:14.000 --> 03:22:23.000
Otherwise, like, uh… just use a curl from some other terminal, but not here, so you don't have any option for that.

03:22:23.000 --> 03:22:25.000
Okay. Should I download Ubuntu?

03:22:25.000 --> 03:22:42.000
Yeah. Yeah, you can download a WSL, actually. So, see, I'll show you my system. So, I am using a Windows machine itself, right? If you can stop sharing your system… Let me share my screen.

03:22:42.000 --> 03:22:43.000
Yeah, so I believe my screen is visible, right? So, see, if I'm going to open up a command prompt.

03:22:43.000 --> 03:22:48.000
Yeah, yeah.

03:22:48.000 --> 03:22:54.000
Right, so I used to get, basically, this option. This one. Ubuntu.

03:22:54.000 --> 03:22:55.000
Okay.

03:22:55.000 --> 03:23:01.000
Right, Ubuntu. So, if I'm into this Ubuntu interface, if I'm going to copy and paste this girl command.

03:23:01.000 --> 03:23:09.000
It will not give me, like, a issue. It will try to paste it as a single line. Second option that you have is, basically, just go to a curl command.

03:23:09.000 --> 03:23:16.000
It's not like you will not be able to execute it now, you can execute it even now. Uh, the only thing is that you have to change a little bit. Let me show you that.

03:23:16.000 --> 03:23:21.000
So, URI API, and then maybe a code example, not this one.

03:23:21.000 --> 03:23:28.000
Uh, SDK AI framework available, models, endpoints. Hmm. So let's suppose I'm using this curl.

03:23:28.000 --> 03:23:30.000
Or image generation you have been using, right? So generally, this is the curl which I'm using.

03:23:30.000 --> 03:23:33.000
Yeah, yeah.

03:23:33.000 --> 03:23:39.000
Maybe I can try to open up my… uh… notebook.

03:23:39.000 --> 03:23:46.000
And next, then this one. And just make it as a single line. It's a multiple line, right?

03:23:46.000 --> 03:23:55.000
So, just remove slash, slash, slash, which is representing a… multiple lines. Now copy it, it became single line.

03:23:55.000 --> 03:24:01.000
And then paste it in a command prompt itself. Let's see. Paste anyway, it's a single line.

03:24:01.000 --> 03:24:10.000
Sorry. So I have to convert more. This… it will become bit lengthy, but that's okay.

03:24:10.000 --> 03:24:14.000
Okay, mainly in single line, uh, command prompt will work, right?

03:24:14.000 --> 03:24:21.000
Yeah, single-line command prompt will also, you know, itself. So, command prompt will always work in a single line.

03:24:21.000 --> 03:24:22.000
Ubuntu, for example.

03:24:22.000 --> 03:24:28.000
You know, multiple lines, so basically it tweaks… Yeah, so it is not able to understand slash and slashin, or slash, basically, which is… which means…

03:24:28.000 --> 03:24:29.000
New line. Whereas your, like, an Ubuntu terminal, so it will be able to understand. Even your Mac will not give you an issue.

03:24:29.000 --> 03:24:35.000
Okay.

03:24:35.000 --> 03:24:43.000
So now, this is my single line. Now, if I'm going to copy and paste, I have to, like, use my API token, by the way.

03:24:43.000 --> 03:24:51.000
So let me use my API token. Create new, create.

03:24:51.000 --> 03:24:57.000
Yeah, so…

03:24:57.000 --> 03:25:04.000
My API token… So V… hmm.

03:25:04.000 --> 03:25:10.000
Knight became the single line. Okay, so then what is the issue? Pasted, single line.

03:25:10.000 --> 03:25:16.000
Now see, I'll get the output. Yeah, so success, something like this?

03:25:16.000 --> 03:25:26.000
And uh… could not resolve Black Forest Lab. Maybe I have made some mistake in terms of converting it to a single line, but that's okay.

03:25:26.000 --> 03:25:34.000
Hi. Uh, response is URL black forest lab, it is telling me.

03:25:34.000 --> 03:25:39.000
Plugs, and it's in 2. Single.

03:25:39.000 --> 03:25:47.000
A beautiful sunset over the mountain.

03:25:47.000 --> 03:25:55.000
It's a long quote, it will… Hmm. So, this is… if you are going to send the hit in this way, so…

03:25:55.000 --> 03:26:07.000
If your command is correct, it will be able to give you the output, as simple as that.

03:26:07.000 --> 03:26:17.000
So, there is a issue. Let me rectify…

03:26:17.000 --> 03:26:47.000
This one…

03:26:53.000 --> 03:27:02.000
Hmm. Now, it is what happened? On the merged, closed brushes, okay.

03:27:02.000 --> 03:27:17.000
Slugging issue…

03:27:17.000 --> 03:27:33.000
Ah, so… Success is equals to false.

03:27:33.000 --> 03:27:40.000
So, error message, unexpected token in JSON, okay. 500.

03:27:40.000 --> 03:27:54.000
So I'm able to hit the API, but it's giving me 500.

03:27:54.000 --> 03:27:59.000
Oh.

03:27:59.000 --> 03:28:09.000
So it's better to have this one, uh, because here it will never give you an issue. So, here, it will always, like, try to do a copy and paste the way you are, like, sending a command.

03:28:09.000 --> 03:28:12.000
Okay. So even in PowerShare also, it takes Singular. Impolish it, Windows.

03:28:12.000 --> 03:28:21.000
Sorry, which one? Pavosa also takes the single line, yeah, so that's a problem.

03:28:21.000 --> 03:28:22.000
Okay.

03:28:22.000 --> 03:28:28.000
Okay, I got the output, see? I think I got the output API together, yeah, I got the image based on this one. So, now.

03:28:28.000 --> 03:28:33.000
And to convert this one into a single line, uh, you don't have to do much, you have a URI, right?

03:28:33.000 --> 03:28:35.000
Yeah, yeah. Okay, thank you.

03:28:35.000 --> 03:28:41.000
Copy and paste everything into Yuri. And ask the… see, I'm able to create the image, right?

03:28:41.000 --> 03:28:42.000
Yeah, yeah.

03:28:42.000 --> 03:28:47.000
So, even inside this PowerCell, uh, sorry, command prompt or PowerCell, I'm able to, like, get my output.

03:28:47.000 --> 03:28:57.000
But the condition is single line. If I have to convert something into a single line, I have a URI, I'll just copy and paste my curl and say that, okay, convert that into a single line.

03:28:57.000 --> 03:28:58.000
It's better to download Ubuntu, right?

03:28:58.000 --> 03:29:06.000
And done. It's better, actually, like, to keep Ubuntu even interface into your Windows, and Windows is giving you this option now.

03:29:06.000 --> 03:29:07.000
So, better to… better to have it. Yeah, thank you. Yeah, Uday, please go ahead with the question.

03:29:07.000 --> 03:29:16.000
Okay. Okay, thank you.

03:29:16.000 --> 03:29:22.000
Yeah, hello there. I'm Aribel. Yeah, Sahid, please go over the question.

03:29:22.000 --> 03:29:23.000
Hosan, please go over the question. Yes, please go ahead, yeah.

03:29:23.000 --> 03:29:38.000
Hey, hi, sir, good afternoon. So, we have… today, we have created, like, we have used the embedding models, okay, from the open source, and then we have used a sentence transformers. So, I would like to understand support via I'm going to create one kind of, like.

03:29:38.000 --> 03:29:41.000
Sorry, can you please repeat once again?

03:29:41.000 --> 03:29:45.000
My question is that we have used some OpenAI models for the embeddings, right?

03:29:45.000 --> 03:29:47.000
Yes, yes, Drew. Hmm, hmm.

03:29:47.000 --> 03:29:55.000
Then we have used that sentence transformers, okay? So I would like to understand the relations between both. Suppose… and my next question would be basically that.

03:29:55.000 --> 03:30:07.000
Sentence transformer, actually, we have used… see, we have used, actually, two things today. One is a URI API. So, Yuri API was trying to provide you an OpenAI model access. Simple, right?

03:30:07.000 --> 03:30:08.000
And we were able to convert into, like, embeddings. With the help of APIs, uh, which is already hosted.

03:30:08.000 --> 03:30:11.000
Mm-hmm, mm-hmm.

03:30:11.000 --> 03:30:16.000
The second approach was, I have downloaded a model from a hugging phase.

03:30:16.000 --> 03:30:17.000
Mm-hmm, mm-hmm.

03:30:17.000 --> 03:30:24.000
And over there, so to download those models from a Hugging Face, so I have installed a library, and that was actually a sentence transformer.

03:30:24.000 --> 03:30:27.000
So, in one of the approach, I think my screen is visible, right?

03:30:27.000 --> 03:30:28.000
Right. Mm-hmm.

03:30:28.000 --> 03:30:37.000
Here, I'm downloading this model. In my system, from Hugging Face. So it's basically a public model, which is an open source model. I'm just using it.

03:30:37.000 --> 03:30:38.000
Mm-hmm, mm-hmm.

03:30:38.000 --> 03:30:45.000
Right? Now, there is no API, and it is not live. Now, here, so anyone can use it from this entire world, without even doing a downloads.

03:30:45.000 --> 03:30:49.000
So, it's a different, different approach, but yeah, ultimately, I'm trying to do the embeddings.

03:30:49.000 --> 03:31:02.000
Okay, so embedding, so suppose, uh, no, if I come to the real-time, real-time, like, suppose I have a, like, a… book, or some kind of, like, a PDF, so I have multiple PDFs. So, first, I need to do the embeddings, right?

03:31:02.000 --> 03:31:04.000
Yes, you have to do the embeddings.

03:31:04.000 --> 03:31:11.000
See, after that embeddings, basically, what we have to do, basically, that… that has become, like, when we call as embedding, nothing is going to be like a vector database first.

03:31:11.000 --> 03:31:17.000
The symbol, you have to do an embeddings, and as an output of the embedding, you will be able to get this vector, right? You will be able to get these numbers, technically, right?

03:31:17.000 --> 03:31:20.000
Correct.

03:31:20.000 --> 03:31:23.000
And then you are going to store this one into a databases.

03:31:23.000 --> 03:31:25.000
Correct.

03:31:25.000 --> 03:31:30.000
The way we try to store any record, right? For example, if I have an employee record, or if I have a hospital record.

03:31:30.000 --> 03:31:36.000
So, somewhere in our database, we are storing it so that everyone will be able to access it, everyone will be able to run the query on top of that one.

03:31:36.000 --> 03:31:50.000
Right? So, similarly, this is my data. Let's suppose this data I'm generating from our books, right? I'm processing maybe 10,000 books and maybe, like, millions of pages inside the books. So, this is the data which I'm generating, and then, like, storing into a DB.

03:31:50.000 --> 03:31:56.000
Now, once data is available into a DB, then I'll send a query. So, when I send a query, like, even in the SQL, if you are aware about SQL.

03:31:56.000 --> 03:31:59.000
So we write a query, right? We write a SQL query.

03:31:59.000 --> 03:32:10.000
Yeah, and SQL query is having its own provision to do a search, one by one, it will try to do the record matching, and then finally it will be able to give the result. Or maybe based on the partition, it is going to search the record.

03:32:10.000 --> 03:32:14.000
Yeah? So here, we are using a similarity search.

03:32:14.000 --> 03:32:23.000
Okay, so my… My question would be basically… my question is basically that, so for creating a vector database, we need one embedding models.

03:32:23.000 --> 03:32:24.000
Yeah, to a store, you need an embedding models, and there are many vector databases, which I'll show you tomorrow.

03:32:24.000 --> 03:32:28.000
Correct?

03:32:28.000 --> 03:32:35.000
They are even giving you embedding models inside their own vector data interface, but they will charge you for that.

03:32:35.000 --> 03:32:49.000
Okay, so suppose then, again, that if I say in terms of that, when I do the searching and, like, a… I wanted to ask some QD, it will go and search from the vector database, and it will send it to the real-time LLM models, like ChatGPT and all those, correct?

03:32:49.000 --> 03:32:58.000
If you are building a ROG application, if you are not building RAG application, if you are just doing a query search.

03:32:58.000 --> 03:32:59.000
Okay, if I want… okay.

03:32:59.000 --> 03:33:06.000
So, it will search and give you the result. Simple. I mean, like, sending a… sending a LLMs is not important, by the way. I mean, like, it's not monetary. So, it depends upon my cases. Let's suppose if I'm trying to build a.

03:33:06.000 --> 03:33:13.000
Rag kind of a case, so where it should send a query and then do the analysis, do some, like, four more tasks on top of this.

03:33:13.000 --> 03:33:21.000
Rephrase it, refine it, and then give me the output. Okay, fine, I can just send the data to the LLMs, which we do into RAG.

03:33:21.000 --> 03:33:27.000
If I don't want that, right? I'm just looking for the raw output. Okay, that's cool, like, I will not send to the LM is not required in that case.

03:33:27.000 --> 03:33:43.000
Okay, you mean to see if I need the data, suppose I have a… I have a, basically, a library, or basically I have a… a kind of SharePoint data, SharePoint data, I have most of, like, my designs or my documents are stored in one place.

03:33:43.000 --> 03:33:44.000
Like, on my OneDrive. So basically, I can do that, basically, embeddings.

03:33:44.000 --> 03:33:49.000
Exactly. Hmm.

03:33:49.000 --> 03:33:57.000
So, for the embeddings, I can use some OpenAI models or some other models I can use for the embeddings. If I need a raw data, it will give me the raw data, I don't need to go with that.

03:33:57.000 --> 03:33:59.000
Hmm. Hmm.

03:33:59.000 --> 03:34:04.000
Rack-based model, if I need that something to be summarized, something to be more like a…

03:34:04.000 --> 03:34:11.000
Um, yeah, so maybe if you're asking, like, if you're asking, maybe you have a medical record, right? You have stored a medical record.

03:34:11.000 --> 03:34:26.000
Now, so if you are looking for a medical record of yours, so, like, uh, give me a medical record of basically Saeed. Okay, it will go, it will check, there are, like, hundreds of medical records, but Sahit's medical record, it will try to identify, and it will try to, like, extract it.

03:34:26.000 --> 03:34:27.000
Mm-hmm, mm-hmm. Okay.

03:34:27.000 --> 03:34:40.000
Now, your medical report is maybe 100 pages long. Right? So then you can send those data to whom? To LLM. Say that, okay, fine, summarize it. Now, if doctor would like to give you, maybe, a suggestion, right? So, it will try to fetch your medical record and send it to the LLM saying that, okay, fine, give some suggestion for this particular quer…

03:34:40.000 --> 03:34:45.000
Cure all this disease, that disease. Simple. So, it will be able to, uh, do it in that way.

03:34:45.000 --> 03:34:52.000
So, in short, if I say that, creating a RAC-based application, I may need two models, right?

03:34:52.000 --> 03:34:54.000
One is the LLM, and one is the embeddings. Hmm.

03:34:54.000 --> 03:34:58.000
Embedding models. So, for those, basically, I have to pay the charges for both of the models.

03:34:58.000 --> 03:35:02.000
Yes, yes, and plus storage. Vector database storage as well. And plus, if you're hosting it, then for hosting, again, charge.

03:35:02.000 --> 03:35:08.000
Victor. So nothing is… nothing comes free.

03:35:08.000 --> 03:35:15.000
Nothing is coming as free, nothing at all. So, see, the resumer system, you must have seen Resume AI, right, that we have built.

03:35:15.000 --> 03:35:29.000
Now, resumer works in 3-way. So, one, you can upload the resume, any kind of a resume, maybe, like, your resume is completely scrapped, but, like, there's no… line length induction and anything, it will be able to rephrase it, part number one.

03:35:29.000 --> 03:35:30.000
Part number two, you can upload a resume. Along with that, upload a JD.

03:35:30.000 --> 03:35:32.000
Mm-hmm. Mm-hmm.

03:35:32.000 --> 03:35:36.000
So it will understand your resume, it will understand your JD, it will create it. Part number three.

03:35:36.000 --> 03:35:39.000
So you can try to create a resume from the scratch, right?

03:35:39.000 --> 03:35:45.000
So, whenever you're trying to upload your resume, we are calling LLMs in the backend.

03:35:45.000 --> 03:35:54.000
Yeah, it makes sense. So, basically, that for the vector database, like a Chrome, DB and stuff like, for the, uh, Facebook also, we have to pay those, like, for those also, we need an API keys, like, how exactly it is?

03:35:54.000 --> 03:36:01.000
No, no, no, no, no. Facebook? Facebook, Facebook, AI, similarity search, you just install it. It's an open source database. You can install it.

03:36:01.000 --> 03:36:02.000
Okay? Mm-hmm. Mm-hmm.

03:36:02.000 --> 03:36:10.000
But you will install it in a local. But let's suppose if you are building an application, if you're, like, making your application global.

03:36:10.000 --> 03:36:11.000
Then you have to host even that database over there, right? See, MySQL is free, MySQL, you can download it, you can practice it in a local system, right? Agree?

03:36:11.000 --> 03:36:17.000
Mm-hmm.

03:36:17.000 --> 03:36:18.000
Mm-hmm.

03:36:18.000 --> 03:36:29.000
Yep. But if you have to build a real-time application, industry-grade application, then for same MySQL, either.

03:36:29.000 --> 03:36:30.000
Mm-hmm.

03:36:30.000 --> 03:36:37.000
You go take a EC2 instances, host it over there, and then do the query, right? Otherwise, you can try to take some of the, you know, cloud instances which they are managing. It means managed DB you are going to consider.

03:36:37.000 --> 03:36:42.000
So, anyhow, either you are going to host it, or you are going to take someone else's hosted machine.

03:36:42.000 --> 03:36:49.000
You will pay, if you're going real time. So, Facebook has similarity such as… And ChromaDB, you can do a local installation, which we'll do tomorrow.

03:36:49.000 --> 03:36:50.000
I have a one…

03:36:50.000 --> 03:36:58.000
Right. Fourth quadrant. Yeah, for cotton and pinecone, they have already given you a, you know, interface and cloud, and they have already given you a free credits as well, without adding card.

03:36:58.000 --> 03:37:04.000
Okay, come for the experimentation, because they know if you are going to like it, you will pay it.

03:37:04.000 --> 03:37:05.000
Yeah, yeah.

03:37:05.000 --> 03:37:12.000
One more follow-up question, nothing anymore. See, some of the, like, I have heard about it, I'm not sure about it, you'd be able… you would be… you would be the work first, like, no, best person to give me the answer.

03:37:12.000 --> 03:37:19.000
Suppose I'm creating a rag-based replication, I can… I got the information that I need, uh, all these models, all the databases.

03:37:19.000 --> 03:37:33.000
But if I go with using some kind of, like, directly we are not using another company, we are not using directly that OpenAI, we are hosted somewhere, what we are doing that, either we are going with it.

03:37:33.000 --> 03:37:34.000
Mm-hmm, mm-hmm.

03:37:34.000 --> 03:37:41.000
Aws, or we would normally go with Ajuul, right? So, I believe that all the completed ragbass, I just need to connect my workspace, or like, we can say the SharePoint, or…

03:37:41.000 --> 03:37:48.000
Some of the library, it will automatically do the vectorization, embeddings, plus it may give me that, like, a user interface or something.

03:37:48.000 --> 03:37:51.000
We have to, we have to… no, not automatically, so no such system is available as of now. Not automatically, but you have to do the mapping, that, okay, fine, this is my drive.

03:37:51.000 --> 03:37:55.000
Okay. Okay.

03:37:55.000 --> 03:38:00.000
Where my data will come, and then you have to provision, basically, one of the vector DB on any of this cloud platform.

03:38:00.000 --> 03:38:05.000
So, you have to write a middleware script that, okay, fine, read a file from here, convert that into this embedding.

03:38:05.000 --> 03:38:10.000
Victor Bimadoux.

03:38:10.000 --> 03:38:11.000
Models.

03:38:11.000 --> 03:38:17.000
Again, inside Amazon Bedrock and all those other services, they are giving you an inbuilt model. But charges are very high, literally. See, wherever they are making your life easy, you are going to pay more.

03:38:17.000 --> 03:38:28.000
Right, but ultimately, it will do the same thing, right? So let's suppose you are using Yuri, or maybe you have hosted, because database storage is going to be one-time tasks. Let's suppose I have millions of books, right? I have millions of books.

03:38:28.000 --> 03:38:38.000
And for all of those millions of books, I have to process it, and I have to convert that into an embedding. So what will be my preferred approach? My preferred approach is to download some big open source model.

03:38:38.000 --> 03:38:45.000
Right. Convert that into an embedding so that I can at least save those, like, conversion… for converting millions of pages and books.

03:38:45.000 --> 03:38:58.000
Into an embeddings from a cloud, right? And then dump those embedding, because it's a one-time job, right? See, query is not going to be a one-time job. So for query, anyhow, you have to store the data and pay for it, because query depends upon your traction.

03:38:58.000 --> 03:39:04.000
Right. Uh, customer, basically. But, uh… Well, embedding, so, which is our one-time job. I can make it a bit cheaper.

03:39:04.000 --> 03:39:17.000
Yeah, no, no, I wanted to talk to you more on this, because, uh… See, the one thing is that particularly I'm really a little interested, that because, see, in our project or our company, okay.

03:39:17.000 --> 03:39:24.000
Every week, we normally, on that, like, a safe agile mode, that's fine, but every release, in a year, we are going to have two releases.

03:39:24.000 --> 03:39:25.000
And each of these may be having, like, a more than 60, 70, 80 o'clock. We can say more than 100 documents.

03:39:25.000 --> 03:39:29.000
Hmm.

03:39:29.000 --> 03:39:39.000
Hmm.

03:39:39.000 --> 03:39:40.000
So do it, no, schedule it. Yeah. Hmm.

03:39:40.000 --> 03:39:45.000
So, every 6 months, I have to compile that, I have to get the embedding. So, suppose I created till… So, the suggestion would be from you that, okay, till January, I did the compile all the data. After the January, then again, I have to recompile the data and embed all those models.

03:39:45.000 --> 03:39:54.000
Yeah, generally, generally, we try to schedule it, so run the crown job, maybe use some scheduler, like an airflow or something, right? There are, like, hundreds of schedulers which are available.

03:39:54.000 --> 03:40:07.000
So, or maybe into a Databricks itself, there is a task scheduler, right? So, that script, you have to run it in every one month, or every 6 months after checking some conditions, so just go ahead, schedule your time, date, and all those things, and frequency. Then, okay, run every week, run every month.

03:40:07.000 --> 03:40:11.000
Your job is done. System will send all those data automatically, you don't have to worry about anything.

03:40:11.000 --> 03:40:15.000
Just so I need to do that, like, a chatbot kind of thing, so do this, do the, like, searching.

03:40:15.000 --> 03:40:20.000
One-time job, not even a chatbot, one time, task scheduling. Why do we need even a chatbot?

03:40:20.000 --> 03:40:29.000
No, no, because once the data is available in the vector, then easily I can do that, like, uh… prompts, or they can… we can send a message to that… to get the data, right?

03:40:29.000 --> 03:40:30.000
Okay, got it. Nice information, sir. Thank you so much. Okay, you helped me a lot.

03:40:30.000 --> 03:40:34.000
Ha, then, obviously, yes, yes, yeah, yeah, yeah. Yep, thanks. Yeah.

03:40:34.000 --> 03:40:41.000
Okay, yeah, please go ahead with the question.

03:40:41.000 --> 03:40:46.000
Aruna, please go ahead with the question. Nares, please go ahead to the question.

03:40:46.000 --> 03:40:47.000
Yeah, now anyone can, I can see only 4 or 5 ends. Yeah, it's good.

03:40:47.000 --> 03:40:50.000
Hosted. Hosa, I don't hear… I don't hear it, yeah. Today is my first class, uh, here.

03:40:50.000 --> 03:40:59.000
Jaron, please go ahead, yeah. Okay, okay.

03:40:59.000 --> 03:41:00.000
Okay.

03:41:00.000 --> 03:41:06.000
And, uh, I understood the vector things very well, and I just wanted to know, whatever the previous class has already done, since, uh, 2nd of August.

03:41:06.000 --> 03:41:07.000
So, I need to go through the sledges, right? And the doula, uh, setups.

03:41:07.000 --> 03:41:14.000
Okay. Well, first week, it's fine. First week was an induction session, so you can skip that part. First and second, like, second and third, the lecture that I've taken, right?

03:41:14.000 --> 03:41:19.000
Yeah. Yeah.

03:41:19.000 --> 03:41:29.000
Uh, you can skip that part, just go through the lecture that I have, like, given to this class in last week, last two lectures, where… because that is important, where I have talked about Fast API, and then.

03:41:29.000 --> 03:41:33.000
I have given the challenges to my class. So just last one week. No need to go through the first one, it's fine. If you have.

03:41:33.000 --> 03:41:37.000
Okay.

03:41:37.000 --> 03:41:46.000
That gone through the entire platform, so induction is done.

03:41:46.000 --> 03:41:47.000
Okay.

03:41:47.000 --> 03:41:53.000
Yeah, yeah, yeah, yeah. Okay, one more thing. As I'm working as a, uh, Selenium automation, uh, site, so is Gen AI will help in that, uh, to give more advantage on that, uh.

03:41:53.000 --> 03:41:56.000
So, I mean, Selenium Automation site.

03:41:56.000 --> 03:42:03.000
Obviously, a lot, because, uh… like, Jennai can do a better automation as compared to Selenium, as per my knowledge.

03:42:03.000 --> 03:42:17.000
Okay. Yeah, okay, okay. Okay, uh, so, so, um, uh, here we'll cover all the, I mean, uh, tools and technologies. So, post that, we'll have, uh, some.

03:42:17.000 --> 03:42:26.000
Selenium-related, any, um, um, uh… what do you call it, demo kind of will happen, or should I…

03:42:26.000 --> 03:42:39.000
Um… no, I'm not going to… maybe you can bring some application and then show it to me, or maybe discuss, but specifically, I'm not going to talk about that, because in this way, uh, you know, I'll end up getting a lot of demand saying that.

03:42:39.000 --> 03:42:42.000
From cybersecurity do is, from blockchain, let's do it. I mean, you know, that will be endless journey.

03:42:42.000 --> 03:42:46.000
Okay. Okay, okay, okay, okay, yeah.

03:42:46.000 --> 03:42:52.000
Maria, don't worry, if you're bringing something, and if you're asking me to discuss, I don't have any issue. I will be happy to discuss.

03:42:52.000 --> 03:42:53.000
Yeah, yeah, yeah, yeah. Thank you so much, Seth. Yeah, thanks, Nick.

03:42:53.000 --> 03:42:58.000
Yeah, yeah, thanks, thanks. Thanks. Okay, next one, please.

03:42:58.000 --> 03:43:02.000
Anyone, guys, like, I think I can see only 4 hands now, yeah.

03:43:02.000 --> 03:43:03.000
Yeah, go ahead, Liz. Yupo-san, tell me.

03:43:03.000 --> 03:43:10.000
Hello, sir. So… So, basically, sir, I'm facing some issue when we run, you know.

03:43:10.000 --> 03:43:11.000
So, like, uh… God, we're just gonna check them, man.

03:43:11.000 --> 03:43:20.000
Apcos screenshot kamai? Okay, in between, so Arman, Naresh, Ranjit Udayh, anyone?

03:43:20.000 --> 03:43:24.000
What I would do is, every time when we use the embedding model, it will use a cosine similarity to search for the query and get the output.

03:43:24.000 --> 03:43:31.000
Mm-hmm. Poor searching, so no, not every time. So, depends.

03:43:31.000 --> 03:43:37.000
Uh, see, it's not depends upon the embedding. It depends upon the DB, actually.

03:43:37.000 --> 03:43:46.000
Embedding is simply converting… you're using a model, and then you're transferring the data, right? So, this similarity search will happen when? When there is a data in my database.

03:43:46.000 --> 03:43:58.000
And then you are sending a query, so basically to match two embeddings, I'm going to use some technique over there. So that is… Where, like, a cosine similarity, or this Manhattan distances, or Euclidean distances, all these things comes into a picture.

03:43:58.000 --> 03:44:02.000
This is the model will already know that what technique I had to use.

03:44:02.000 --> 03:44:12.000
No, Vimeo will be aware about it. There is no relationship with the model, right? Model only knows that how to convert your… Uh, textual information into a numerical information. That is the only role of model.

03:44:12.000 --> 03:44:14.000
Also, that we have to decide those very technical yet.

03:44:14.000 --> 03:44:25.000
Yeah, basically, yes. Like, Biomor will decide that how you're doing a similarity search, or how you are comparing the data. Model's responsibility is to take an input as your textual information.

03:44:25.000 --> 03:44:33.000
Give you output as a vector, that's it. Numerical. Got it right? Yeah.

03:44:33.000 --> 03:44:37.000
Okay, next one, please. Yeah, I think Prasadi, you're sharing your screen. Yeah, I can see her screen.

03:44:37.000 --> 03:44:43.000
This is said… Esme…

03:44:43.000 --> 03:44:44.000
And put together on it, so…

03:44:44.000 --> 03:44:54.000
It. The KS VIN access, like, eh?

03:44:54.000 --> 03:45:02.000
Oh, I don't think… No, I think, uh, screen access… I've looked at Biff's thank you, right?

03:45:02.000 --> 03:45:03.000
Jankie. Zoning. Okay.

03:45:03.000 --> 03:45:19.000
Actually, Abhiliva Jaina Nabuse. Juma app download karke. Aja thi ka, ishu ke aarai aapat data ka issue arawah hai aapar embedding mein data.

03:45:19.000 --> 03:45:20.000
Uh, Sudan, so authorization key. Star stars are there.

03:45:20.000 --> 03:45:26.000
Big one authorization? Ah, yes.

03:45:26.000 --> 03:45:29.000
See, they go up.

03:45:29.000 --> 03:45:34.000
Copy gather.

03:45:34.000 --> 03:45:35.000
Yeah, it's super precision, it kind of way.

03:45:35.000 --> 03:45:40.000
Just go back, go back to your API key.

03:45:40.000 --> 03:45:48.000
The Stargamatla will hide what you guys. How about an aggregate kind of… like, I have copied low.

03:45:48.000 --> 03:45:52.000
Cardi begins with Yahweh.

03:45:52.000 --> 03:46:00.000
Okay.

03:46:00.000 --> 03:46:14.000
Ki skull length…

03:46:14.000 --> 03:46:20.000
Hmm. I'll be…

03:46:20.000 --> 03:46:23.000
Oh, yeah.

03:46:23.000 --> 03:46:27.000
Or equalism from sentence ka, is meh bhi kuchara arata ratta.

03:46:27.000 --> 03:46:35.000
Hmm, hmm. Python kaversan 3.13 hai.

03:46:35.000 --> 03:46:49.000
Python government, 3.10 karoo, kias indes transformer johyne. 3.10 nachin. Nahayk environment.

03:46:49.000 --> 03:46:57.000
Create hyphen environment creation, Python environment.

03:46:57.000 --> 03:46:59.000
So you guys are looking aapko.

03:46:59.000 --> 03:47:04.000
Yeah, and uh… acha, environmental, lower kernel padega version.

03:47:04.000 --> 03:47:14.000
Well, nein questionay. Python environment, click on that. Python environment.

03:47:14.000 --> 03:47:27.000
Not quite near. Acha 3.11 hai, 3.1 triggers a base, huh?

03:47:27.000 --> 03:47:33.000
Huh?

03:47:33.000 --> 03:47:39.000
Now, if you change aapka Python 3.13 lagre.

03:47:39.000 --> 03:47:41.000
The answer to the environment test.

03:47:41.000 --> 03:47:55.000
Hmm, 3 points, select another kernel.

03:47:55.000 --> 03:48:01.000
To search this environment may surah karne keba jo code dik hai barbar usi jaike karna praga, right?

03:48:01.000 --> 03:48:10.000
Toho. Environment ka.

03:48:10.000 --> 03:48:12.000
Toh agar mahi environment sad under rang kariga jolikhai, right?

03:48:12.000 --> 03:48:25.000
Hmm. Hmm, hmm, hmm.

03:48:25.000 --> 03:48:26.000
Dominantly, man. Chika.

03:48:26.000 --> 03:48:45.000
Tigers. 526.

03:48:45.000 --> 03:48:46.000
Console? Hmm.

03:48:46.000 --> 03:48:56.000
Uska pajisa se ha ax. They say I'm XYZ, just have me 1,2 basically position, present, get 2D, making a point A is point k position, react successful nihin.

03:48:56.000 --> 03:48:57.000
Mm-hmm, mm.

03:48:57.000 --> 03:49:03.000
Length is ki 5067. So yeah, basically, uh, do you…

03:49:03.000 --> 03:49:14.000
Usko represent kar rahamatla uska. Numerical equivalent, semi-data here.

03:49:14.000 --> 03:49:21.000
My name is Rhanjo Kumar. My name is Rhanjo Kumar, but more numeric value available, eh?

03:49:21.000 --> 03:49:22.000
Um, medical side?

03:49:22.000 --> 03:49:30.000
3 computer code text so many… ajay aega. Wait. Just wait for a couple of more classes.

03:49:30.000 --> 03:49:38.000
Somebody got something like a fundamental mitika thee.

03:49:38.000 --> 03:49:44.000
Wo data hai. Ko sentence, basically. Sentence.

03:49:44.000 --> 03:49:47.000
I said, sentence the gap position and auscalicular exercise.

03:49:47.000 --> 03:49:54.000
Sentenced hi. My name is Rancho Kumar, huh?

03:49:54.000 --> 03:49:55.000
Uh, haha, huh?

03:49:55.000 --> 03:50:00.000
Right. Ap usko hum convert ki hain, numerical representation, majo aapko data mild numerical wala vector.

03:50:00.000 --> 03:50:01.000
Ha, actor me, huh?

03:50:01.000 --> 03:50:05.000
Digger. Vector, my name is Dansu Kumari today.

03:50:05.000 --> 03:50:07.000
Okay? Usweptak length hai.

03:50:07.000 --> 03:50:16.000
Hai. Covid 2D hua, 3D hua? I, na?

03:50:16.000 --> 03:50:21.000
Mm-hmm.

03:50:21.000 --> 03:50:28.000
3d coordinate hoa X coordinate jet coordinate. Right? But they represent the extrasa point quicker, right?

03:50:28.000 --> 03:50:32.000
Dig in, dig in. Right.

03:50:32.000 --> 03:50:54.000
My name is Anchu Kumar.

03:50:54.000 --> 03:50:55.000
Hmm.

03:50:55.000 --> 03:51:00.000
Take care. Mere hai. Basically.

03:51:00.000 --> 03:51:06.000
Number of text agar thas jar hai, or model output dera aapko e kajar.

03:51:06.000 --> 03:51:28.000
Usko aap. Finally, job embedding karne ho to uska dimension aap hajar meinhusko darsare ho.

03:51:28.000 --> 03:51:29.000
And then a number of words, eh. Uskop, alag the unit ma aap representative.

03:51:29.000 --> 03:51:34.000
Number of words nihu output therea.

03:51:34.000 --> 03:51:39.000
Bbb.

03:51:39.000 --> 03:51:43.000
Exactly wo haggya. Have you talked about that, I think.

03:51:43.000 --> 03:51:50.000
Pleasure. They basically will model the output, how to model ka uponk length hot and length of output hota, right? To move model ka wo output hai.

03:51:50.000 --> 03:52:00.000
But again, Apka sentence, ap jobhi sentenced daloge, burap kitaab dal doskandar, us ek unit kelee, unahi lendrakkega.

03:52:00.000 --> 03:52:11.000
Embedding size din hai ka. So, it will try to maintain that.

03:52:11.000 --> 03:52:15.000
Basically…

03:52:15.000 --> 03:52:21.000
Monologue output a woman.

03:52:21.000 --> 03:52:30.000
Kiji.

03:52:30.000 --> 03:52:34.000
I mean, technology.

03:52:34.000 --> 03:52:48.000
Take your time. I'm Abhini Boulder. But yes, bohot clear hojayaka bahu jada clear jab wahan saba start karo, kyo ki…

03:52:48.000 --> 03:52:54.000
Modules will be open.

03:52:54.000 --> 03:53:03.000
Just method and function use karke. But a class member chatting in nahi.

03:53:03.000 --> 03:53:09.000
Otherwise, jarvat bhi nita.

03:53:09.000 --> 03:53:11.000
Way out of the camera.

03:53:11.000 --> 03:53:18.000
Exactly, embedding call karte, Python function. Take it.

03:53:18.000 --> 03:53:19.000
Man, I would do all that so much, man. Thank you, sir. Take care.

03:53:19.000 --> 03:53:27.000
Yeah, Tiga, Tiga. Victor, I mentioned he has to look into… yes, Salfraz, that's true.

03:53:27.000 --> 03:53:28.000
Uh, with that, Apa question… huh, yeah, please go ahead, Nadice, yeah.

03:53:28.000 --> 03:53:37.000
Yeah, uh, hi, uh, Sudanj. Uh, yeah, so, uh, this vectors, how do you, uh, handle, uh, you know, multiple languages? Let's say.

03:53:37.000 --> 03:53:43.000
Uh, my vector is in Hindi language, and if user asks the English question.

03:53:43.000 --> 03:53:44.000
So…

03:53:44.000 --> 03:53:53.000
Hmm. That doesn't depend upon me, actually, that depends upon the model inferencing. So, let's suppose my model understands, my model has been trained on a multilingual data, right? For example, ChatGPT. So, obviously, it's a multilingual model.

03:53:53.000 --> 03:54:06.000
You can understand multiple languages, right? So, whenever you try to send any kind of a data, raw data, I would say, so first of all, you try to identify that, okay, fine, so which language it is. And that is possible with the help of a small.

03:54:06.000 --> 03:54:12.000
One word of NLP library. So, it will be able to identify the languages. Now, accordingly, it will try to switch.

03:54:12.000 --> 03:54:18.000
No, but in the case of embeddings and similarities such how it works across languages.

03:54:18.000 --> 03:54:24.000
Let's see, by default, let's suppose my model has been trained only on English, and I'm trying to pass something in Hindi.

03:54:24.000 --> 03:54:29.000
It will be able to give me embeddings. But it will be a bad one.

03:54:29.000 --> 03:54:30.000
Because, see, the best embedding means what? Best embedding doesn't mean that I'm just looking for a number.

03:54:30.000 --> 03:54:35.000
Okay, understood. So…

03:54:35.000 --> 03:54:41.000
Best embedding simply means that, that as per my dataset, I have to select a particular model.

03:54:41.000 --> 03:54:49.000
Which has been trained on similar data. So that it will be able to understand the syntactic semantic, grammatical, each and everything, right?

03:54:49.000 --> 03:54:57.000
Otherwise, see, if I don't have to even go and use an embedding to convert some data into a numerical space. I can use a TF IDF.

03:54:57.000 --> 03:54:58.000
Yeah.

03:54:58.000 --> 03:55:10.000
Right, I can use a TF IDF, no need to train the model, no need to use a transformer. But again, we know that that TF IDF never understands the semantic syntactic, right? It never understands the mathematical understanding, it just understands the frequency of occurrences.

03:55:10.000 --> 03:55:19.000
Into the data. So, the purpose of building an embedding is very simple, a kind of a, like a… I have to use a model.

03:55:19.000 --> 03:55:24.000
Which understands my type of the data in a better way.

03:55:24.000 --> 03:55:25.000
Otherwise, you will end up generating embeddings, and then in a search.

03:55:25.000 --> 03:55:28.000
Gotcha.

03:55:28.000 --> 03:55:29.000
It'll look less. Yeah. Okay, yeah.

03:55:29.000 --> 03:55:33.000
Yeah, gotcha, gotcha. Thanks. Thank you.

03:55:33.000 --> 03:55:48.000
Okay, next one, please. Uday, I think you have pinged me. Both come, hey, keyboard basis per search kartne elastic.

03:55:48.000 --> 03:55:55.000
Aapko sa option hai. Arman, you have a question? How there? Arman, yeah, Arman, please go ahead.

03:55:55.000 --> 03:56:06.000
Yes, sir. So, I'm getting that 3 point version error, I have created the environment also with the versions, okay, fix that, please.

03:56:06.000 --> 03:56:15.000
Yeah, sure, sure, share your screen, I'll check.

03:56:15.000 --> 03:56:23.000
Ganesh is telling me that, Answer, could you please add the last assignment on a resource section? Ganesh, it's already added last week itself. So, last week class.

03:56:23.000 --> 03:56:27.000
I've already added it, I remember. It's there.

03:56:27.000 --> 03:56:28.000
Sir, can I share now?

03:56:28.000 --> 03:56:39.000
Yeah, you can share, please.

03:56:39.000 --> 03:56:47.000
Uh, yes, it is visible. Let me take a screen control. Okay, so I have requested, if you can give me control.

03:56:47.000 --> 03:56:55.000
What is the issue that you're facing? So, running yourself with the… Okay, require IP on the kernel package, right? Execute it, and then it will…

03:56:55.000 --> 03:57:04.000
Install automatically. No Model N request, hmm. So, per install R-E-Q.

03:57:04.000 --> 03:57:10.000
Esd is execute.

03:57:10.000 --> 03:57:22.000
Uh, in PIP PIP, I-N-S-T-A-L-R-E-Q-U. Reque S…

03:57:22.000 --> 03:57:27.000
She goes, yeah, spelling is fine.

03:57:27.000 --> 03:57:49.000
You're using Python itself, right?

03:57:49.000 --> 03:57:56.000
Hmm, it's done.

03:57:56.000 --> 03:58:23.000
No Module M NumPy, okay.

03:58:23.000 --> 03:58:29.000
So this one is also done?

03:58:29.000 --> 03:58:39.000
Hmm, fine, everything is working? Now, you can check your embeddings.

03:58:39.000 --> 03:58:44.000
This is the embedding, right? For this data. So yeah, everything is working now.

03:58:44.000 --> 03:58:46.000
One more thing, a previous follow-up session that you just explained to… regarding the vector spaces.

03:58:46.000 --> 03:58:51.000
Mm-hmm.

03:58:51.000 --> 03:58:54.000
So, let's say I have a… I have push intentions. So, intense and sentence B. So, both of them will be represented as vector in that 153C dimension.

03:58:54.000 --> 03:59:02.000
Hmm.

03:59:02.000 --> 03:59:03.000
Hmm?

03:59:03.000 --> 03:59:09.000
And whenever the user sends the query. So, it will try to find a similarity between that two. The query will be also in that dimension only, right?

03:59:09.000 --> 03:59:14.000
Yeah, so first of all, whenever you're sending a query, so you have to use the same embedding models to convert.

03:59:14.000 --> 03:59:21.000
A query, because textual information to a numerical information, it will not be able to do a comparison.

03:59:21.000 --> 03:59:22.000
Right? So it should be available into numeric-to-numeric. So, query, put it into the embedding models.

03:59:22.000 --> 03:59:27.000
Right, right.

03:59:27.000 --> 03:59:29.000
You will get the vector, and then vector-to-vector match, you do it.

03:59:29.000 --> 03:59:35.000
Okay, so it has, like, the same concept that we use in linear regression, and we found the best fit line.

03:59:35.000 --> 03:59:36.000
So when… when… Most of concepts are so concept, where we draw a line, we have…

03:59:36.000 --> 03:59:48.000
Uh, no, linear regression is a different, uh, thing. No, no, no, not at all. So, in case of a linear integration, generally, we try to, like, backpropagate with M and value of C.

03:59:48.000 --> 03:59:53.000
By the way, it's not at all same, not even close.

03:59:53.000 --> 03:59:58.000
Also, can you go on the website where you copy the model for entry, something?

03:59:58.000 --> 03:59:59.000
I want to understand one thing.

03:59:59.000 --> 04:00:04.000
Okay. Hmm. Okay, so let me open up your…

04:00:04.000 --> 04:00:09.000
System itself. I will do it in your system.

04:00:09.000 --> 04:00:16.000
Hugging Face. Go here…

04:00:16.000 --> 04:00:21.000
And go to the model.

04:00:21.000 --> 04:00:23.000
Embed… QUAN3, right? Yeah, so here is the one. Tell me now.

04:00:23.000 --> 04:00:32.000
Yes, sir. So, this is about this, uh, parameters type.

04:00:32.000 --> 04:00:41.000
So you told that the context length is the… that can be input also, that what we are giving, and the length, what we are expecting from model.

04:00:41.000 --> 04:00:46.000
Do you sequence limbs that they had prescribed. It is for an output one, or the input one combined.

04:00:46.000 --> 04:00:59.000
So, output 1 in general, people try to give it to you. That's okay, so this will be able to do this much of influencing. Generally, for some of the models, they try to give you input context length, as well as output context length, both. That, okay, fine, so this is the maximum context.

04:00:59.000 --> 04:01:04.000
You can try to pass to my model, and this is the maximum which I will be able to give you as an output.

04:01:04.000 --> 04:01:08.000
Oh, so let's say if your model has a 4K context, right?

04:01:08.000 --> 04:01:12.000
And I passed a UK input length, so the model will convert that 8K into the 4K image of the day.

04:01:12.000 --> 04:01:16.000
No, it will break it down into chunks.

04:01:16.000 --> 04:01:20.000
But the length will be in the output, that will be related to 4K only.

04:01:20.000 --> 04:01:29.000
If our model output is 4K, then it will be 4K, but generally, when we try to deploy the model, so we try to follow the concept of streaming over there.

04:01:29.000 --> 04:01:33.000
Right. So, 4K and then 4K, and then 4K, so it will keep on generating the output, so in this way, we feel like.

04:01:33.000 --> 04:01:44.000
Model consciousness is high, but technically it's a 4K. Output which I'm getting.

04:01:44.000 --> 04:01:45.000
The streaming part is good, and fully worked on.

04:01:45.000 --> 04:01:51.000
Sorry? Okay, Steve, let's suppose you are looking for 8K context, right? But model is giving you 4K.

04:01:51.000 --> 04:01:59.000
So it will give you 4K toys, simple, but I'll try to configure that interface in such a way that you will feel like it's giving me 8K. But technically, it's giving me 4K.

04:01:59.000 --> 04:02:04.000
So, regardless of the Puerto Ripas. We get an extra deal with $40,000, sir.

04:02:04.000 --> 04:02:11.000
If you are configuring, it depends upon the configuration, but originally, model is giving you 4K.

04:02:11.000 --> 04:02:19.000
Oh, cool. As well, one more thing with higher BMB, uh, dimension, embedding dimension, higher the accuracy, is that something…

04:02:19.000 --> 04:02:44.000
No, not at all. So, yeah, you will be able to get a more space, but it depends upon a training of the model as well, that which model you are using, and what is, uh, you know… the total, like, data which is, uh, like, been used to train that particular model. It depends more on that side, less on higher dimension. Yes, dimension plays a role, so if I'm using more dimension, means I'm trying to.

04:02:44.000 --> 04:02:54.000
Do more explanation about the data. My data… I'm just trying to create the embeddings, but this embedding is more explanatory. If it is more explanatory, so when I'll try to do a similarity search.

04:02:54.000 --> 04:03:05.000
It will be able to work in a better way, right? It will be able to do a more accurate similarity search operation. That is true. But first, story starts from a model content, model data, by the way.

04:03:05.000 --> 04:03:16.000
In that way, like, how we train… how we practice ourselves, the more on the particular tech, then we get familiar with that. In that particular… way we can expect from the model also that the number of.

04:03:16.000 --> 04:03:21.000
But at times, we've trained the model. Then we can expect the particular output in the output.

04:03:21.000 --> 04:03:29.000
Exactly, exactly. Higher dimension plays an important role, plays a significant role, and it will give you the better output, higher and higher.

04:03:29.000 --> 04:03:37.000
If you're using a dimension, but if someone is going to ask you a question, immediately get into a… so instead of giving a direct answer, right?

04:03:37.000 --> 04:03:45.000
Like, you have asked me a question, that higher dimension will give me more accurate result in terms of similarity search. So instead of giving you the straight answer.

04:03:45.000 --> 04:03:50.000
I'll try to give answer in this way. So, same thing you should do in an interview.

04:03:50.000 --> 04:03:57.000
With the… with… with… So, depending on the embedding dimension.

04:03:57.000 --> 04:03:58.000
Hmm.

04:03:58.000 --> 04:04:07.000
So, let's say we have a huge amount of data. And we are… so how do we decide that we can choose higher dimension?

04:04:07.000 --> 04:04:08.000
My question is.

04:04:08.000 --> 04:04:14.000
No, no, so basically, see, I'll not decide based on the higher dimension, lower dimension, I'll decide based on the model knowledge, basically.

04:04:14.000 --> 04:04:15.000
Okay.

04:04:15.000 --> 04:04:27.000
My first criteria will always be a knowledge of the model, right? Not on a higher or lower dimension. I'll try to take… I'm okay with 1000 dimension, because that's more than enough to represent any data, right? And that, to one of our wider space.

04:04:27.000 --> 04:04:33.000
So, that's more than enough for me, but yeah, I'll go after the model.

04:04:33.000 --> 04:04:41.000
Yeah, if it is, like, a 2-year-old model, I will not use it, right? Because I know that the knowledge of the model will be very less.

04:04:41.000 --> 04:04:48.000
And point will be, like, the how my data is getting converted into vector accurately by model.

04:04:48.000 --> 04:04:49.000
Which is a perfect thing.

04:04:49.000 --> 04:04:56.000
Yes. See, every model will be able to convert your data, because every model can understand, but it's more about the relation.

04:04:56.000 --> 04:05:03.000
You are using transformer model to convert something into an embedding, but with relation. Understanding.

04:05:03.000 --> 04:05:07.000
Right? So, more knowledgeable model will be able to build a better relation.

04:05:07.000 --> 04:05:12.000
Unless hallucination, right? Otherwise, I can… I could have used a TF IDF itself, right?

04:05:12.000 --> 04:05:13.000
I could have used one hot encoding itself to convert my data into a numeric value.

04:05:13.000 --> 04:05:18.000
One of the…

04:05:18.000 --> 04:05:22.000
One hard encoding, TFIDF, all these approaches are already there, right?

04:05:22.000 --> 04:05:25.000
We try to study it from machine learning classes, or stats classes itself.

04:05:25.000 --> 04:05:36.000
That, okay, fine, so TFI, DFN, convert any kind of a textual information into a numerical information. But I know I can do it, but it is not going to work well for me, because I'm not capturing a relationship between the textual information.

04:05:36.000 --> 04:05:37.000
Yeah? What to vector again, what to vector is the next step? So, some Rakeshi saying, what to vector.

04:05:37.000 --> 04:05:42.000
Okay…

04:05:42.000 --> 04:05:49.000
What to vector is, again, a small neural network, right? At least better than TF, IDF, and other old approaches, by the way.

04:05:49.000 --> 04:05:54.000
But still, like, uh… I'm not going to give you a benefit.

04:05:54.000 --> 04:06:00.000
As you said, we can configure the model context lens as per our requirements, so…

04:06:00.000 --> 04:06:05.000
You can't configure the model context length once model has been trained. You can do it.

04:06:05.000 --> 04:06:08.000
Uh, while training the model, while creating a model architecture, uh.

04:06:08.000 --> 04:06:16.000
At that point of a time. After that. You can try to configure it as a streaming, that, okay, fine, so do one loop.

04:06:16.000 --> 04:06:20.000
Then another loop, then third loop. In this way, you can configure it.

04:06:20.000 --> 04:06:25.000
Instead of this score, then some of our teams, recruiting and distance are magnetic distance. When the distance, the two distance, located in 9 and 10.

04:06:25.000 --> 04:06:29.000
Sorry? Hmm.

04:06:29.000 --> 04:06:32.000
And the core one similarity, that is used to compare the data to…

04:06:32.000 --> 04:06:40.000
Cosine stability itself is a distance matrix, just at different distancing matrices in itself. It's not Euclidean, it's not a Manhattan. Both are different.

04:06:40.000 --> 04:06:48.000
None of the 3D distance that we have, the distance that we have, so deciding that which will.

04:06:48.000 --> 04:06:51.000
Give me the actual output when this comes in the picture.

04:06:51.000 --> 04:07:05.000
No, depends upon the databases. Some databases provide you all three options, so that whenever you're sending a query, you can pass that as a parameter, that use the statistical matrices. And some databases don't even provide you those, like, inputs. They just use it cosine, by the way, or maybe hybrid.

04:07:05.000 --> 04:07:07.000
Okay, sir. Thank you for anything.

04:07:07.000 --> 04:07:13.000
Yeah, so you have a control to pass as a parameter with some databases. So, with FAISS, so they are giving you that.

04:07:13.000 --> 04:07:16.000
But some will not give you. That is just a parameter.

04:07:16.000 --> 04:07:21.000
Okay, so that also will depend on model, like, when they have…

04:07:21.000 --> 04:07:26.000
I don't know why it will depend upon the model. Converse of the data only depends upon the model.

04:07:26.000 --> 04:07:31.000
Similarity comparison, how it is going to depend on the model? Never.

04:07:31.000 --> 04:07:38.000
I have two vectors, right? I have two coordinates. I'm doing the comparison, so how models come into a picture?

04:07:38.000 --> 04:07:45.000
Moder will come into picture only till creating this two-vector, two points.

04:07:45.000 --> 04:07:46.000
Through all sorts of people.

04:07:46.000 --> 04:07:54.000
In any dimension. Okay. Yeah, okay, fine, then I think we are done with all the questions. There is no question left for me.

04:07:54.000 --> 04:07:56.000
Uh, so with that, thank you so much, everyone. Take care, see you again, uh, tomorrow, same time, 9pm, sorry, 9am IST.

04:07:56.000 --> 04:08:02.000
So

